---
title: "PEM Sample Analysis"
author: "Kiri Daust"
date: "20/08/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(clhs)
library(sf)
library(raster)
library(gdistance)
library(foreach)
library(data.table)
library(fasterize)
library(reticulate)
library(here)
library(LearnGeom)
library(dplyr)
library(tidyr)
library(exactextractr)
require(ggthemes)
require(philentropy)
require(tictoc)
require(ggpubr)
require(flextable)

```

The below chunk creates some generic statistical functions

```{r source}
#source_python("./mTSP.py")

# require(doParallel)
# cl <- makePSOCKcluster(detectCores()-4)
# registerDoParallel(cl)
# library(doMC)
# registerDoMC(10)

source("./SampleAnalysis_functions.R")
```

Load data and setup covariates

```{r load data}
datLocGit <- here("InputData") ## Data
covLoc <- here("Covariates/Covariates_PemTSP") ## Too big for git data

### landscape levels covariates
covars <- paste(covLoc, c("25m_DAH_3Class.tif","25m_LandformClass_Default_Seive4.tif",
                          "25m_MRVBF_Classified_IS64Low6Up2.tif","becRaster.tif"), sep = "/")# ,"DEM_25m.tif"
layerNamesLL <- c("DAH","LFC","MRVBF","BEC", "cost") ##need to change this if you change the layers

ancDatLL <- raster::stack(covars)
proj4string(ancDatLL) <- "+init=epsg:3005"
becLayer <- raster(paste0(covLoc,"/becRaster.tif"))
ancDatLL <- mask(ancDatLL,becLayer)

SLcov <- c("twi.tif","valley_depth_2.tif","open_neg.tif","swi_twi.tif","convexity.tif", "rid_level.tif", "mrvbf2.tif")
covars <- paste(covLoc, SLcov, sep = "/")
ancDatSL <- raster::stack(covars)
proj4string(ancDatSL) <- "+init=epsg:3005"
layerNamesSl <- names(ancDatSL) ##need to change this if you change the layers
for(i in layerNamesSl){
  ancDatSL[[i]][ancDatSL[[i]] > quantile(ancDatSL[[i]],0.975, na.rm = T)] <- NA
}

ancVals <- as.data.table(getValues(ancDatSL))##table of SL data
ancVals <- na.omit(ancVals)

##setup univariate distribution
ancBin <- ancVals[,lapply(.SD, function(x){cut(x,breaks = seq(min(x),max(x),length.out = 9),labels = F,include.lowest = T)})]
ancBin <- copy(unite(ancBin, LHSVar, sep = "_"))
ancSmall <- ancBin[,.(Num = .N), by = .(LHSVar)] ##count occurences
ancSmall <- ancSmall[Num > 150,] ##remove uncommon
setorder(ancSmall,-Num)
ancSmall[,ID := seq_along(Num)]

## dem for transtion layer should be 25m?
alt <- raster(paste0(covLoc, "/dem.tif")) 
proj4string(alt) <- "+init=epsg:3005"
altLL <- raster(paste0(covLoc, "/dem25m.tif"))
proj4string(altLL) <- "+init=epsg:3005"
## template raster
allRast <- raster(paste0(covLoc,"/Road_Rast_Small.tif"))    
allRast[allRast == 255] <- NA
allRast <- trim(allRast)

###read in roads
rdsAll <- st_read(paste0(datLocGit,"/road_access_for_cost.gpkg"))
rdsAll <- rdsAll[,"DESCRIPTIO"]
colnames(rdsAll)[1] <- "road_surface"
rdsAll <- as.data.table(rdsAll) %>% st_as_sf()

##Smithers start location
start_sf <- st_read("SmithersStart.gpkg")
start <- as(start_sf, "Spatial")
```

This section creates the transition layer from the slope and clost surface using Tobler's hiking function

```{r create cost surface}
##load previously save cost rasters
acost <- raster( "./Results/acost.tif")
acost2<- raster ("./Results/acost2.tif")
acostLL <- raster("./Results/acostLL.tif")
ancDatSL_cost <- stack(ancDatSL,acost2)
ancDatLL_cost <- stack(ancDatLL,acostLL)

##generate new cost layer
##road speed
# rSpd <- fread("road_speed.csv")
# rSpd[,Values := speed]
# rdsAll <- merge(rdsAll, rSpd, by = "road_surface", all = F)
# rdsAll <- rdsAll[,"Values"] %>%
#   st_buffer(dist = 35, endCapStyle = "SQUARE", joinStyle = "MITRE") %>%
#   st_cast("MULTIPOLYGON")
# rdsRast <- fasterize(rdsAll, allRast, field = "Values")
# 
# alt2 <- projectRaster(alt,rdsRast,method = 'ngb')
# altAll <- merge(rdsRast, alt2)
# altSmall <- raster::aggregate(altAll, fact = 3L, fun = min)
# 
# ## trasition function: if > 500, elevation, so get diff, else speed to select min
# trFn <- function(x){
#   if(x[1] > 500 & x[2] > 500){
#     x[1]-x[2]
#   }else{
#     min(x[1],x[2])
#   }
# }
# 
# rdIdx <- which(values(altAll) < 100) ##which cells are for roads?
# slpIdx <- which(values(altAll) > 500)
# adj <- adjacent(altAll, cells = slpIdx, pairs = T, directions = 8)
# adj <- adj[!adj[,1] %in% rdIdx ,] ##remove roads inside area
# adj <- adj[!adj[,2] %in% rdIdx ,]
# 
# tr <- transition(altAll,trFn,directions = 8, symm = F) ##altDiff and speed (km/h)
# tr1 <- geoCorrection(tr) ##divided by 25 - slope and conductance (km/h/m)
# tr1[adj] <- (3/5)*(6*exp(-3.5*abs(tr1[adj] + 0.08))) ##tobler's hiking function * 3/5 - gives km/h
# tr1 <- tr1*1000 ##now roads are correct conductance (h/m), and walking in m/h
# tr2 <- geoCorrection(tr1) ##have to geocorrect this part again
# tr1[adj] <- tr2[adj] ##tr1 values are now all conductance in h/metre
# 
# acost <- accCost(tr1,start)
# plot(acost)
# 
# ##create smaller version for TSP
# rdIdx <- which(values(altSmall) < 100) ##which cells are for roads?
# slpIdx <- which(values(altSmall) > 500)
# adj <- adjacent(altSmall, cells = slpIdx, pairs = T, directions = 8)
# adj <- adj[!adj[,1] %in% rdIdx ,] ##remove roads inside area
# adj <- adj[!adj[,2] %in% rdIdx ,]
# 
# tr <- transition(altSmall,trFn,directions = 8, symm = F) ##altDiff and speed (km/h)
# trSmall <- geoCorrection(tr) ##divided by 25 - slope and conductance (km/h/m)
# trSmall[adj] <- (3/5)*(6*exp(-3.5*abs(tr1[adj] + 0.08))) ##tobler's hiking function * 3/5 - gives km/h
# trSmall <- trSmall*1000 ##now roads are correct conductance (h/m), and walking in m/h
# tr2 <- geoCorrection(trSmall) ##have to geocorrect this part again
# trSmall[adj] <- tr2[adj] ##tr1 values are now all conductance in h/metre
# 
# acost2 <- projectRaster(acost, ancDatSL)
# acost2 <- mask(acost2, alt)
# acostLL <- projectRaster(acost, ancDatLL)
# acostLL  <- mask(acostLL, altLL)
# ancDatSL_cost <- stack(ancDatSL,acost2)
# 
# plot(acost2)
# writeRaster(acost, "./Results/acost.tif", format = "GTiff", overwrite = TRUE)
# writeRaster(acost2, "./Results/acost2.tif", format = "GTiff", overwrite = TRUE)
# writeRaster(acostLL, "./Results/acostLL.tif", format = "GTiff", overwrite = TRUE)
# png('./outputs/acost2.png', height=nrow(acost2), width=ncol(acost2))
# plot(acost2, maxpixels=ncell(acost2))
# dev.off()

```
### Simulation 1: Single point sampling

For all the following simulations we're testing with random samples, LHS samples, CCLHS samples and sometimes TSP sampling (not for single points).

There are different ways of testing space filling: in the below code, we use KS statistics, a simple bin fill function

```{r setup distribtuion and test functions}
tempID <- ancDatSL$twi
tempID[] <- 1:(nrow(tempID)*ncol(tempID))
names(tempID) <- "CellID"
ancDatSL <- stack(ancDatSL,tempID)

###setup full LHS distribution for statistics
ancVals <- as.data.table(getValues(ancDatSL))##table of SL data
ancVals <- na.omit(ancVals)
idVals <- ancVals$CellID
ancVals[,CellID := NULL]
```



```{r sample functions single point}
##single points clhs sampled
allSLDat <- as.data.table(getValues(ancDatSL_cost))##table of SL data
allSLDat <- na.omit(allSLDat)

##functions to create sampling setups
createRandom <- function(X,n){
  small <- sample(1:nrow(X),size = n, replace = F)
  return(X[small,])
}

createLHS <- function(X,n){
  temp <- clhs(X,size = n,use.cpp = T, iter = 5000, simple = F)
  small <- as.matrix(temp$sampled_data)
  return(small)
}

createCCLHS <- function(X,n){
  temp <- clhs(X,size = n,cost = ncol(X),use.cpp = T, iter = 5000,simple = F)
  small <- as.matrix(temp$sampled_data)
  return(small)
}
#X=ancVals; sampleFun=createLHS; nums = nums; testFun1 = KL_stat_clhs; testFun2 = KS_stat_clhs; n=1000
#collecting stats with different space filling tests
#                     .packages = c("philentropy","foreach","clhs","data.table","dplyr","tidyr")
collectStats <- function(X,sampleFun,nums,testFun1 = KL_stat_clhs){ ##KS_prob_clhs
  test_res <- foreach(n = nums, .combine = rbind) %do% {
                      cat(".")
                      smallSet <- sampleFun(X,n)
                      res1 <- testFun1(sample = smallSet,ancSmall)
                    data.table(Num = n,KL = res1)
                    }
  return(test_res)
}


```

```{r run and plot simulations}
### basically shows that there is no difference in sample method on KL statistic
#nums <- round(seq(10, 5000, length.out = 10))
nums <- c(500, 1000, 1500, 2000, 2500, 3000)
nums <- rep(nums, each = 5)
statsRand <- collectStats(ancVals,createRandom,nums)
statsRand$SType <- "Random"
statsLHS <- collectStats(ancVals,createLHS, nums)
statsLHS$SType <- "LHS"
statsCCLHS <- collectStats(allSLDat,createCCLHS, nums)
statsCCLHS$SType <- "CCLHS"

statsAll <- rbind(statsRand,statsLHS,statsCCLHS)%>% filter(Num < 3200) ###filter to remove some categories after the fact for grtaph
statsAll$Num <- round(statsAll$Num, -2) %>% as.factor()
statsAll$SType <- as.factor(statsAll$SType) %>% factor(levels = c("Random", "LHS", "CCLHS"))

###Graphic of number of points needed to match variable space
statsAll <- statsAll 
plotsim <- ggplot(statsAll, aes(x = Num, y = KL, fill = SType))+
  geom_boxplot()+
   scale_y_continuous(name = "Kullback-Leibler Divergence", limits=c(0, 3),breaks=seq(0,8,1))+
  #labs(x = "Number of Sample Points") +
   scale_fill_discrete(name='Sample Method', labels = c('random', 'cLHS', 'cost cLHS' ))+
  expand_limits(x = 0, y = 0)+
  geom_hline(yintercept = 1,  linetype = "dashed", color = "red")+
  #xlim(0,3000)+
  theme_pem()
plotsim
  finalise_plot (plotsim, "./figures/nosamplesbymethodKLis1.png",
                          width_pixels=640,
                          height_pixels=450)
```

```{r num single points}
###this needs to be adjusted to reflect a set KL distance
statsAll <- as.data.table(statsAll)
temp <- statsAll[,.(y = median(KL)), by = .(SType,Num)]
#temp[,`:=`(y = (y-min(y))/(max(y)-min(y)), Num = as.numeric(as.character(Num)))]
temp[,`:=`(Num = as.numeric(as.character(Num)))]
numPoints <- temp[,.(nPoints = predict(loess(Num ~ y,data = .SD), data.frame(y = 1))), by = .(SType)]
numPoints$nPoints <- round(numPoints$nPoints, -1)
# write.table(numPoints, file = "./Results/Number of points required to match covariate distribution.csv", sep = ",", quote = FALSE, row.names = F)
# knitr::kable(numPoints, digits = -1, caption = "Number of points for KL divergence = 1")

NumSamples_flex <- flextable(numPoints) %>% align(align = "center", part = "all") %>% set_table_properties(width = .5, layout = "autofit")
print(NumSamples_flex, preview = "docx")
save_as_docx(NumSamples_flex , path = "./figures/numPointsforKLis1.docx")
dev.off()

```

# Create examples of summary distributions for manuscript
#
```{r univariate distributions}
s1 <- createLHS(ancVals,100)
s2 <- createLHS(ancVals,500)
s3 <- createLHS(ancVals,2500)
s4 <- createLHS(ancVals,5000)

temp <- plotUniDists(s1)
t1 <- temp$data
t1[,NumSamples := 100]
t1[,Label := paste0("100 Samples\nKL = ",round(temp$KL, digits = 3))]
temp <- plotUniDists(s2)
t2 <- temp$data
t2[,NumSamples := 500]
t2[,Label := paste0("500 Samples\nKL = ",round(temp$KL, digits = 3))]
temp <- plotUniDists(s3)
t3 <- temp$data
t3[,NumSamples := 2500]
t3[,Label := paste0(" 2500 Samples\nKL = ",round(temp$KL, digits = 3))]
temp <- plotUniDists(s4)
t4 <- temp$data
t4[,NumSamples := 5000]
t4[,Label := paste0(" 5000 Samples\nKL = ",round(temp$KL, digits = 3))]
dat <- rbind(t1,t2,t3,t4)
dat[,Label := as.factor(Label)]
dat[,Label := reorder(Label,NumSamples)]

dat2 <- melt(dat, id.vars = c("Label","ID"), measure.vars = c("Num","SampNum"))
source("./_functions/doc_theme_pem.R")
fillspace <- ggplot(dat2, aes(x = ID, y = value, colour = variable, size = variable)) +
  geom_line() +
  facet_wrap(.~Label)+
  ##theme_pem_facet()+
  scale_color_manual(name = "Source", values = c("black","grey"), labels = c("Map Area","cLHS Sample")) +
  scale_size_manual(name = "Source",values = c(3.2,0.8),labels = c("Actual Distribution","cLHS Sample")) +
  scale_alpha(range = c(0.4, 0.8))+
  labs(x = "Multivariate Bins", y = "Bin ratio of total map area")
fillspace
  finalise_plot (fillspace, "./figures/samplestofillbins.png",
                          width_pixels=640,
                          height_pixels=450)
dev.off()
```

## Test filling of real data. Add by slice

```{r}
# s1Dat <- fread("./InputData/DeceptionFinalAllPoints_cleaned.csv")
# s1Dat <- s1Dat[,.(X,Y,ID,Site_Series_Map_Unit)]
# setnames(s1Dat, old = "Site_Series_Map_Unit", new = "Unit")
# s1Points <- st_as_sf(s1Dat,coords = c("X","Y"), crs = 3005)

s1Dat <- fread("./InputData/s1_clean_pts_all_fnf.csv")
s1Dat <- s1Dat[,.(x,y,ID,mapunit1, slice)]
setnames(s1Dat, old = "mapunit1", new = "Unit")
s1Points <- st_as_sf(s1Dat,coords = c("x","y"), crs = 3005)

##All points
sample <- ancDatVL$extract_points(s1Points)
sample <- sample[,1:7]
sFull1 <- plotUniDists(sample)

## Single slice
slices = c("1")
sample1 <- s1Points %>% filter(slice %in% slices)
sample1 <- ancDatVL$extract_points(sample1)
sample1 <- sample1[,1:7]
sSlice1 <- plotUniDists(sample1)

## 2 slice
slices = c("1", "2")
sample1 <- s1Points %>% filter(slice %in% slices)
sample1 <- ancDatVL$extract_points(sample1)
sample1 <- sample1[,1:7]
sSlice2 <- plotUniDists(sample1)
## 3 slice
slices = c("1", "2", "3")
sample1 <- s1Points %>% filter(slice %in% slices)
sample1 <- ancDatVL$extract_points(sample1)
sample1 <- sample1[,1:7]
sSlice3 <- plotUniDists(sample1)
## 4 slice
slices = c("1", "2", "3", "4")
sample1 <- s1Points %>% filter(slice %in% slices)
sample1 <- ancDatVL$extract_points(sample1)
sample1 <- sample1[,1:7]
sSlice4 <- plotUniDists(sample1)

t1 <- sFull1$data
t1[,Label := paste0("All Data\nKL = ",round(sFull1$KL,digits = 3))]

s1 <- sSlice1$data
s1[,Label := paste0("1 Slice\nKL = ",round(sSlice1$KL,digits = 3))]

s2 <- sSlice2$data
s2[,Label := paste0("2 Slice\nKL = ",round(sSlice2$KL,digits = 3))] 
dat <- rbind(t1,s1,s2)


s3 <- sSlice3$data
s3[,Label := paste0("3 Slice\nKL = ",round(sSlice3$KL,digits = 3))]

s4 <- sSlice4$data
s4[,Label := paste0("4 Slice\nKL = ",round(sSlice4$KL,digits = 3))] 

dat <- rbind(t1,s1,s2,s3, s4)

dat2 <- melt(dat, id.vars = c("Label","ID"), measure.vars = c("Num","SampNum"))

samplespace <- ggplot(dat2, aes(x = ID, y = value, colour = variable, size = variable)) +
  geom_line() +
  facet_wrap(.~Label)+
  theme_pem_facet()+
  scale_color_manual(name = "Source", values = c("black","grey"), labels = c("Map Area","cLHS Sample")) +
  scale_size_manual(name = "Source",values = c(3.2,0.8),labels = c("Actual Distribution","cLHS Sample")) +
  scale_alpha(range = c(0.4, 0.8))+
  labs(x = "Multivariate Bins", y = "Bin ratio of total map area")
samplespace
  finalise_plot (samplespace, "./figures/slicestofillbins.png",
                          width_pixels=640,
                          height_pixels=450)
#dev.off()
```


### Simulation 2: Transect Sampling

Now instead of choosing single points, we use the cclhs or lhs to pick centre points, and then create a triangular transect around it, which we then sample from (either using a second clhs or just regular spacing). The below chunk simulates 3 transect scenarios: non cost-constrained clhs, using clhs to sample from transects, and cost-constrained clhs using both clhs and regular spacing for transect sampling. Note that this chunk takes a while to run and will use all your CPU resources. The time limiting part is not the clhs function, but rather extracting data from the raster stack once each transect has been created (even using Velox this takes a while).

```{r set up transect simulations}
ancDatLL_cost <- stack(ancDatLL,acostLL)
fullSet <- sampleRegular(ancDatLL, size = 200000,sp = T) 
X <- st_as_sf(fullSet) ##This line takes a while, but I'm not sure of an alternative
X <- X[!is.na(X$X25m_DAH_3Class),]
# 
# fullSL <- sampleRegular(ancDatSL, size = 100000,GDAL = T)
# fullSL <- fullSL[complete.cases(fullSL),]

fullSet <- sampleRegular(ancDatLL_cost, size = 200000,sp = T)
Xcost <- st_as_sf(fullSet)
Xcost <- Xcost[!is.na(Xcost$X25m_DAH_3Class),]
Xcost <- Xcost[!is.infinite(Xcost$acostLL),]
```



```{r sampling functions}
##create cclhs transects, sample using lhs
createCCLHS_transect <- function(X,nSlices){
  xMat <- st_drop_geometry(X)
  currpoints <- NULL
  pointsPerSlice <- 5
  datOut <- foreach(snum = 1:nSlices,.combine = rbind) %do% {
    cat(".")
    temp <- clhs(xMat,size = pointsPerSlice * snum,must.include = currpoints,
                 cost = ncol(xMat),iter = 5000,use.cpp = T,simple = F)
    currpoints <- temp$index_samples
    dat <- X[temp$index_samples,]
    small <- foreach(j = 1:nrow(dat),.combine = rbind) %do% {
      tri <- Tri_build(id = j, x = st_coordinates(dat[j,])[1],y = st_coordinates(dat[j,])[2])
      tri <- st_buffer(tri, dist = 5)
      st_crs(tri) <- 3005
      tempDat <- exact_extract(ancDatSL,tri)[[1]]
      tempDat[,-c(8,9)]
    }
    data.table(SliceNum = snum, KL = suppressMessages(KL_stat_clhs(sample = small, ancSmall = ancSmall)))
  }
  
  return(datOut)
}

createCCLHS_transect_paired <- function(X,nSlices){
  xMat <- st_drop_geometry(X)
  currpoints <- NULL
  pointsPerSlice <- 5
  datOut <- foreach(snum = 1:nSlices,.combine = rbind) %do% {
    cat(".")
    temp <- clhs(xMat,size = pointsPerSlice * snum,must.include = currpoints,
                 cost = ncol(xMat),iter = 5000,use.cpp = T,simple = F)
    currpoints <- temp$index_samples
    dat <- X[temp$index_samples,]
    small <- foreach(j = 1:nrow(dat),.combine = rbind) %do% {
      tri <- Tri_build(id = j, x = st_coordinates(dat[j,])[1],y = st_coordinates(dat[j,])[2])
      tri <- st_buffer(tri, dist = 5)
      st_crs(tri) <- 3005
      tri2 <- rotFeature(tri,dat$geometry[j],45)
      tri2$geometry <- tri2$geometry + matrix(data = c(0, 400), ncol = 2)
      st_crs(tri2$geometry) <- 3005
      tri <- st_union(tri$g,tri2$geometry)
      tempDat <- exact_extract(ancDatSL,tri)[[1]]
      tempDat[,-c(8,9)]
    }
    data.table(SliceNum = snum, KL = suppressMessages(KL_stat_clhs(sample = small, ancSmall = ancSmall)))
  }
  
  return(datOut)
}

transectSim_single <- foreach(i = 1:5, .combine = rbind) %do% {
  temp <- createCCLHS_transect(Xcost,12)
  temp[,SimNum := i]
  temp
}

ggplot(transectSim_single,aes(x = SliceNum,y = KL,group = SliceNum)) + 
  geom_boxplot()

transectSim_paired <- foreach(i = 1:5, .combine = rbind) %do% {
  temp <- createCCLHS_transect_paired(Xcost,12)
  temp[,SimNum := i]
  temp
}

ggplot(transectSim_paired,aes(x = SliceNum,y = KL,group = SliceNum)) + 
  geom_boxplot()


```

```{r run simulations transect}
nums <- round(seq(6, 100, length.out = 10), -1) ##set sequence of transect number
nums <- rep(nums, each = 5) ##replicates of each number (should be at least 5)

# statsLHS_trans <- collectStats(X,createLHS,nums)
# statsLHS_trans$SType = "LHS"

tic()
statsCCLHS_trans <- collectStats(Xcost,createCCLHS,nums)
statsCCLHS_trans$SType <- "cLHS_resample"
toc()


statsCCLHS_30m <- collectStats(Xcost, createCCLHS_30m, nums)
statsCCLHS_30m$SType <- "All_points"

##plot
statsAll2 <- rbind(statsCCLHS_trans, statsCCLHS_30m) #statsLHS_trans, 
statsAll2$Num <- as.factor(statsAll2$Num)
statsAll2$SType <- as.factor(statsAll2$SType)%>% factor(levels = c("All_points", "cLHS_resample"))
# ggplot(statsAll2, aes(x = Num, y = KL, fill = SType))+
#   geom_boxplot()
ggplot(statsAll2, aes(x = Num, y = KL, fill = SType))+
  geom_boxplot()

###Graphic of number of transects needed to match variable space
ggplot(statsAll2, aes(x = Num, y = KL, fill = SType))+
  geom_boxplot()+
  scale_y_continuous(name = "Kullback-Leibler Divergence(KD)", limits=c(0, 8),breaks=seq(0,8,1))+
  labs(x = "Number of Transects") +
  scale_fill_discrete(name='Sample Method', labels = c('all points', 'cLHS subsampled'))+
  expand_limits(x = 0, y = 0)+
  geom_hline(yintercept = 1, linetype = "dashed", color = "red")+
   geom_smooth(method = "loess", se=FALSE, aes(group=SType), color = "black", size = .5)+
  theme_light()

ggsave("./Results/KL_Divergence_transect_subsampling.jpeg", device = 'jpeg', width = 6, height=4, units = "in")
#fwrite(statsAll,"statsAll_save.csv")

# KS_transectPlot <- ggplot(statsAll, aes(x = Num, y = KS, fill = SType))+
#   geom_boxplot() +
#   labs(x = "Number of Transects", y= "Kolmogorov-Smirnov statistic (0 = identical)")+
#   scale_fill_discrete(guide = guide_legend(reverse=FALSE), name='Sample Method', labels = c('cLHS- cLHS resampled', 'cost cLHS - systematic resampled', 'cost cLHS - cLHS resampled'))+
#   theme_few()
# plot(KS_transectPlot)
#ggsave("./Results/KS_transect_plot.jpeg", device = 'jpeg')
# 
# Bfill_transectPlot <- ggplot(statsAll, aes(x = Num, y = fillQual, fill = SType))+
#   geom_boxplot() +
#   labs(x = "Number of Transects", y= "Bins remaining to be filled")+
#   scale_fill_discrete(guide = guide_legend(reverse=FALSE), name='Sample Method', labels = c('cLHS- cLHS resampled', 'cost cLHS - systematic resampled', 'cost cLHS - cLHS resampled'))+
#   theme_few()
# plot(Bfill_transectPlot)
#ggsave("./Results/KS_transectPlot.jpeg", device = 'jpeg')

# smallSet <- as.matrix(smallSet)
# test <- cramer.test(x = fullSL,y = smallSet, just.statistic = T)
```

## Calculate number of sites to fill

```{r num points}
statsAll2 <- as.data.table(statsAll2)
temp <- statsAll2[,.(y = median(KL)), by = .(SType,Num)]
#temp[,`:=`(y = (y-min(y))/(max(y)-min(y)), Num = as.numeric(as.character(Num)))]
temp[,`:=`(Num = as.numeric(as.character(Num)))]
numTrans <- temp[,.(nTrans = predict(loess(Num ~ y,data = .SD), data.frame(y = 1))), by = .(SType)]

knitr::kable(numTrans, digits = 2, caption = "Number of points for KL divergence = 1")

# calcNumTrans <- function(tempDat){
#   temp <- tempDat[,.(y = median(fillQual)), by = .(Num)]
#   temp[,`:=`(y = (y-min(y))/(max(y)-min(y)), Num = as.numeric(as.character(Num)))]
#   l2 <- loess(Num ~ y, data = temp)
#   numTrans <- predict(l2,data.frame(y = 0.15))
#   return(numTrans)
# }
# 
# statsAll <- as.data.table(statsAll)
# numTrans <- statsAll[,.(Num90 = calcNumTrans(.SD)), by = .(SType)]
```

## Number of TSP iterations needed

Not much benefit after 15-20 iterations

```{r}
calcCost <- function(Xcost,n,nTry){
  pkgs <- c("Rcpp","sf","data.table","raster","dplyr","reticulate","gdistance","scales","clhs")
  tspTime <- foreach(i = 1:nTry,.combine = rbind, .packages = pkgs,
                     .export = c("start_sf","trSmall")) %dopar% {
    reticulate::source_python("mTSP.py")
    xMat <- st_drop_geometry(Xcost)
    temp <- clhs(xMat,size = n,cost = ncol(xMat),iter = 5000,use.cpp = T,simple = F)
    pnts <- Xcost[temp$index_samples,]
    pnts <- pnts[,1]
    colnames(pnts) <- c("name","geometry")
    st_geometry(pnts) <- "geometry"
    startPnts <- st_as_sf(data.frame(name = "Start",geometry = start_sf))
    pnts <- rbind(pnts, startPnts)
    pnts2 <- as(pnts, "Spatial")
    ## create distance matrix between sample points
    test <- costDistance(trSmall,pnts2,pnts2)
    dMat2 <- as.matrix(test)
    dMat2 <- dMat2*60
    dMat2[is.infinite(dMat2)] <- 1000
    
    ##penalty based on quality of points
    objVals <- max(temp$final_obj_continuous) - temp$final_obj_continuous
    maxTime <- 8L ##hours
    ## time per transect
    plotTime <- 50L ##mins
    temp <- dMat2[1:n,1:n]
    maxDist <- sum(temp[upper.tri(temp)])
    minPen <- maxDist
    maxPen <- maxDist * 4
    objVals <- scales::rescale(objVals, to = c(minPen,maxPen))
    objVals <- as.integer(objVals)
    
    ndays <- as.integer(ceiling(n/5)+1)
    pen = objVals
    
    indStart <- as.integer(rep(n,ndays))
    ##run vehicle routing problem from python script
    ## GCS is global span cost coefficient
    vrp <- py_mTSP(dat = dMat2,num_days = ndays, start = indStart, end = indStart, 
                   max_cost = maxTime*60L, plot_time = plotTime, penalty =  pen, arbDepot = F, GSC = 5L)
    time <- vrp[[2]]
    totTime <- sum(as.numeric(unlist(time)))/480
    num = length(as.numeric(unlist(vrp[[1]])))-2*length(vrp[[1]])
    data.table(It = i, Cost = totTime, Num = num)
  }
  return(tspTime)
}

# nums <- seq(2,40,by = 5)
# tspTest <- foreach(numRep = nums, .combine = rbind) %do% {
#   dat <- calcCost(Xcost, n = 30, nTry = numRep)
#   dat[,NumRep := numRep]
#   dat
# }
# 
# tspSum <- tspTest[,.(MinTime = min(Cost)), by = .(NumRep)]
# knitr::kable(tspSum)
```

## Simulation 3: Sampling Time

The previous chunks investigated how variable space filling changed with different sampling strategies. We now apply the same process, but collect results on the amount of time each plan would take (as determined by the TSP). We use similar sampling plans as in the above chunks: cost-constrained clhs, regular clhs, and TSP (cost-constrained clhs optimised using the TSP). First, we run the simulations as above, with  increasing numbers of transects and multiple replications at each level. We then simulate the cost to fill the variable space to KL = 1 as calculated above, using each of these methods, and present a sample TSP route.

```{r time cost}
##create TSP to calculate time
calcCost_clhs <- function(pnts,objVals,plotTime = 50L, minPerDay = 3L){
  n = nrow(pnts)
  p2 <- st_as_sf(pnts)
  pnts <- pnts[,1]
  colnames(pnts) <- c("name","geometry")
  st_geometry(pnts) <- "geometry"
  startPnts <- st_as_sf(data.frame(name = "Start",geometry = start_sf))
  pnts <- rbind(pnts, startPnts)
  pnts2 <- as(pnts, "Spatial")
  
  ## create distance matrix between sample points
  test <- costDistance(trSmall,pnts2,pnts2)
  dMat2 <- as.matrix(test)
  dMat2 <- dMat2*60
  dMat2[is.infinite(dMat2)] <- 1000
  
  ##penalty based on quality of points
  objVals <- max(objVals) - objVals
  
  maxTime <- 8L ##hours
  ## time per transect
  temp <- dMat2[1:n,1:n]
  maxDist <- sum(temp[upper.tri(temp)])
  minPen <- maxDist
  maxPen <- maxDist * 4
  objVals <- scales::rescale(objVals, to = c(minPen,maxPen))
  objVals <- as.integer(objVals)

  ndays <- as.integer(ceiling(n/minPerDay)+1)
  pen = objVals

  indStart <- as.integer(rep(n,ndays))
  ##run vehicle routing problem from python script
  ## GCS is global span cost coefficient
  vrp <- py_mTSP(dat = dMat2,num_days = ndays, start = indStart, end = indStart, 
                 max_cost = maxTime*60L, plot_time = plotTime, penalty =  pen, arbDepot = F, GSC = 5L)
  time <- vrp[[2]]
  totTime <- sum(as.numeric(unlist(time)))
  return(totTime)
}

## regular clhs
timeLHS <- function(X,n){
  xMat <- st_drop_geometry(X)
  temp <- clhs(xMat,size = n,cost = NULL,iter = 5000,use.cpp = T,simple = F)
  return(temp)
}

## cost_constrained clhs
timeCCLHS <- function(X,n){
  xMat <- st_drop_geometry(X)
  temp <- clhs(xMat,size = n,cost = ncol(xMat),iter = 5000,use.cpp = T,simple = F)
  return(temp)
}

##function to run simulations
collectTime <- function(X,sampleFun, num){
  pkgs <- c("Rcpp","LaplacesDemon", "foreach","sf",
          "raster","LearnGeom","dplyr","reticulate","gdistance","scales","clhs")
  test_res <- foreach(n = num, .combine = rbind, .packages = pkgs,
                    .export = c("start_sf","trSmall", "calcCost_clhs")) %dopar% { #
                      reticulate::source_python("mTSP.py")
                      templhs <- sampleFun(X,n)
                      dat <- X[templhs$index_samples,]
                      if(nrow(dat) > 1){
                        time <- calcCost_clhs(dat,templhs$final_obj_continuous)
                        data.frame(Num = n, Cost = time)
                      }else{
                        NULL
                      }
                      
                    }
  return(test_res)
}

trans <- as.integer(c(5, 10, 15, 20, 25, 30, 35, 40, 45, 50)) ##numbers of transects
trans <- rep(trans,each = 5) ##replications at each level

LHSTime <- collectTime(X,timeLHS,num = trans)
LHSTime$Type = "LHS"
CCLHSTime <- collectTime(Xcost,timeCCLHS,num = trans)
CCLHSTime$Type = "CCLHS"

##function to choose route of least costly point set
calcCost_tsp <- function(Xcost,n,nTry){
  pkgs <- c("Rcpp","sf","data.table","raster","dplyr","reticulate","gdistance","scales","clhs")
  tspTime <- foreach(i = 1:nTry,.combine = rbind, .packages = pkgs,
                     .export = c("start_sf","trSmall")) %dopar% {
    reticulate::source_python("mTSP.py")
    xMat <- st_drop_geometry(Xcost)
    temp <- clhs(xMat,size = n,cost = ncol(xMat),iter = 5000,use.cpp = T,simple = F)
    pnts <- Xcost[temp$index_samples,]
    pnts <- pnts[,1]
    colnames(pnts) <- c("name","geometry")
    st_geometry(pnts) <- "geometry"
    startPnts <- st_as_sf(data.frame(name = "Start",geometry = start_sf))
    pnts <- rbind(pnts, startPnts)
    pnts2 <- as(pnts, "Spatial")
    ## create distance matrix between sample points
    test <- costDistance(trSmall,pnts2,pnts2)
    dMat2 <- as.matrix(test)
    dMat2 <- dMat2*60
    dMat2[is.infinite(dMat2)] <- 1000
    
    ##penalty based on quality of points
    objVals <- max(temp$final_obj_continuous) - temp$final_obj_continuous
    maxTime <- 8L ##hours
    ## time per transect
    plotTime <- 50L ##mins
    temp <- dMat2[1:n,1:n]
    maxDist <- sum(temp[upper.tri(temp)])
    minPen <- maxDist
    maxPen <- maxDist * 4
    objVals <- scales::rescale(objVals, to = c(minPen,maxPen))
    objVals <- as.integer(objVals)
    
    ndays <- as.integer(ceiling(n/5)+1)
    pen = objVals
    
    indStart <- as.integer(rep(n,ndays))
    ##run vehicle routing problem from python script
    ## GCS is global span cost coefficient
    vrp <- py_mTSP(dat = dMat2,num_days = ndays, start = indStart, end = indStart, 
                   max_cost = maxTime*60L, plot_time = plotTime, penalty =  pen, arbDepot = F, GSC = 5L)
    time <- vrp[[2]]
    totTime <- sum(as.numeric(unlist(time)))
    num = length(as.numeric(unlist(vrp[[1]])))-2*length(vrp[[1]])
    data.table(It = i, Cost = totTime, Num = num)
  }
  return(min(tspTime$Cost))
}

# trans <- as.integer(c(5, 10, 15, 20, 25, 30, 35, 40, 45, 50)) ##numbers of transects
# trans <- rep(trans,each = 5) ##replications at each level

#this part takes a while
TSPTime <- foreach(n = trans,.combine = rbind) %do% {
  time <- calcCost_tsp(Xcost,n,8)
  data.frame(Num = n, Cost = time)
}
TSPTime$Type <- "TSP"
timeAll <- rbind(LHSTime,CCLHSTime,TSPTime)
timeAll$Cost <- timeAll$Cost/480 ##covert from minutes to 8-hour days

timeAll$Num <- as.factor(timeAll$Num)
timeAll$Type <- as.factor(timeAll$Type) %>% factor(levels = c( "LHS", "CCLHS", "TSP"))

ggplot(timeAll, aes(x = Num, y = Cost, fill = Type))+
  geom_boxplot()+
  scale_y_continuous(name = "Number of 8-hour days", limits=c(0, 15),breaks=seq(0,15,1))+
  labs(x = "Number of Sample Sites in Sample Plan") +
  expand_limits(x = 0, y = 0)+
  scale_fill_discrete (name= 'Method', labels = c('cLHS', 'cost-constrained cLHS',  'Travelling Salesman Optimized')) +
  theme_light()
ggsave("./Results/CostsbyTransectMethod.jpeg", device = 'jpeg')

# Time_plot <- ggplot(timeAll, aes(x = Num, y = Cost, fill = Type)) +
#   geom_boxplot() +
#   labs(x = "Number of Transects", y = "Time Cost to Sample Sufficient Samples") +
#   scale_fill_discrete(
#     guide = guide_legend(reverse = FALSE),
#     name = 'Sample Method',
#     labels = c(
#       'cLHS- cLHS resampled',
#       'cost cLHS - cLHS resampled',
#       'cost cLHS systematic resampled'
#     )
#   ) +
#   theme_few()
# plot(Bfill_transectPlot2)
```

## Example TSP Route

```{r tsp_example}
n = 25

xMat <- st_drop_geometry(Xcost)
temp <- clhs(xMat,size = n,cost = ncol(xMat),iter = 10000,use.cpp = T,simple = F)
pnts <- Xcost[temp$index_samples,]
pnts <- pnts[,1]
colnames(pnts) <- c("name","geometry")
st_geometry(pnts) <- "geometry"
startPnts <- st_as_sf(data.frame(name = "Start",geometry = start_sf))
pnts <- rbind(pnts, startPnts)
pnts2 <- as(pnts, "Spatial")
## create distance matrix between sample points
test <- costDistance(trSmall,pnts2,pnts2)
dMat2 <- as.matrix(test)
dMat2 <- dMat2*60
dMat2[is.infinite(dMat2)] <- 1000

##penalty based on quality of points
objVals <- max(temp$final_obj_continuous) - temp$final_obj_continuous
maxTime <- 8L ##hours
## time per transect
plotTime <- 50L ##mins
temp <- dMat2[1:n,1:n]
maxDist <- sum(temp[upper.tri(temp)])
minPen <- maxDist
maxPen <- maxDist * 4
objVals <- scales::rescale(objVals, to = c(minPen,maxPen))
objVals <- as.integer(objVals)

ndays <- as.integer(ceiling(n/5)+1)
pen = objVals

indStart <- as.integer(rep(n,ndays))
##run vehicle routing problem from python script
## GCS is global span cost coefficient
vrp <- py_mTSP(dat = dMat2,num_days = ndays, start = indStart, end = indStart, 
               max_cost = maxTime*60L, plot_time = plotTime, penalty =  pen, arbDepot = F, GSC = 100L)

result <- vrp[[1]]

## create spatial paths
paths <- foreach(j = 0:(length(result)-1), .combine = rbind) %do% {
  if(length(result[[as.character(j)]]) > 2){
    cat("Drop site",j,"...\n")
    p1 <- result[[as.character(j)]]+1
    out <- foreach(i = 1:(length(p1)-1), .combine = rbind) %do% {
      temp1 <- pnts[p1[i],]
      temp2 <- pnts[p1[i+1],]
      temp3 <- shortestPath(trSmall,st_coordinates(temp1),
                            st_coordinates(temp2),output = "SpatialLines") %>% st_as_sf()
      temp3$Segment = i
      temp3
    }
    out$DropSite = j
    out
  }
  
}

paths <- st_transform(paths, 3005)
#st_write(paths, dsn = "RoadTSP.gpkg", layer = "Paths", append = T, driver = "GPKG")  

plot(acost2)
plot(pnts,col = "black", add = T)
plot(paths,col = as.factor(paths$DropSite), add = T)

## label points
p2 <- pnts
p2$PID <- seq_along(p2$name)
p2 <- p2[,"PID"]
p2$DropLoc <- NA
p2$Order <- NA
for(i in 0:(length(result)-1)){
  p1 <- result[[as.character(i)]]+1
  p1 <- p1[-1]
  p2$DropLoc[p1] <- i
  p2$Order[p1] <- 1:length(p1)
}
p2 <- st_transform(p2, 3005)


```

## Time to fill variable space

Instead of just calculating the time cost for different numbers of transects, we can use the TSP to estimate the cost of using each sampling method to fill the variable space. Thus, here we use the number of transects needed to fill the variable space as calculated above, and use the TSP to estimate the cost of each method.

```{r num}
nSims <- 20
nRep <- 3

collectTime2 <- function(X,sampleFun,num, nReps, plotTime = 50L, minNum = 3L){
  pkgs <- c("Rcpp","LaplacesDemon", "foreach","sf",
          "raster","LearnGeom","dplyr","reticulate","gdistance","scales","clhs")
  test_res <- foreach(n = 1:nReps, .combine = rbind, .packages = pkgs,
                    .export = c("start_sf","trSmall", "calcCost_clhs")) %dopar% { #
                      reticulate::source_python("mTSP.py")
                      templhs <- sampleFun(X,num)
                      dat <- X[templhs$index_samples,]
                      if(nrow(dat) > 1){
                        time <- calcCost_clhs(dat,templhs$final_obj_continuous,plotTime,minNum)
                        data.frame(Rep = n, Cost = time)
                      }else{
                        NULL
                      }
                      
                    }
  return(test_res)
}

##single points clhs sampled
allSLDat <- as.data.frame(getValues(ancDatSL_cost))##table of SL data
#allSLDat[,CellID := seq_along(allSLDat$twi)]
allSLDat <- na.omit(allSLDat)
allSLDat <- allSLDat[!is.infinite(allSLDat$layer),]
numNeeded <- numPoints[SType == "CCLHS",as.integer(nPoints)]

pkgs <- c("Rcpp","LaplacesDemon", "foreach","sf",
        "raster","LearnGeom","dplyr","reticulate","gdistance","scales","clhs")

## note this part takes about 10 mins (should be nSims*5)
SPoint <- foreach(n = 1:(nSims*nRep), .combine = rbind, .packages = pkgs,
                  .export = c("start_sf","trSmall", "calcCost_clhs","allSLDat")) %dopar% { #
                    reticulate::source_python("mTSP.py")
                    templhs <- clhs(allSLDat,size = as.integer(numNeeded/4),cost = ncol(allSLDat),use.cpp = T, iter = 5000,simple = F)#6000
                    tempdat <- templhs$sampled_data
                    cellNums <- as.numeric(rownames(tempdat))
                    dat <- raster::xyFromCell(ancDatSL_cost,cell = cellNums, sp = T)
                    dat <- st_as_sf(dat)
                    dat <- cbind(1:nrow(dat),dat)
                    if(nrow(dat) > 1){
                      time <- calcCost_clhs(dat,templhs$final_obj_continuous,plotTime = 2L,minPerDay = 8L)
                      data.frame(Rep = n, Cost = time*4)
                    }else{
                      NULL
                    }
                    
                  }

SPoint <- as.data.table(SPoint)
SPoint[,TSPNum := rep(c(1:nRep),each = nSims)]
TSPpoint <- SPoint[,.(Cost = min(Cost)), by = .(TSPNum)]
TSPpoint[,SType := "Single Point TSP"]
setnames(TSPpoint, old = "TSPNum", new = "Rep")
SPoint[,TSPNum := NULL]
SPoint[,SType := "Single Point CLHS"]

##clhs resampled clhs
# numNeeded <- numTrans[SType == "CCLHS",as.integer(nTrans)]
# clhsTime <- collectTime2(Xcost,timeCCLHS,numNeeded,nSims)
# clhsTime$SType <- "CLHS_Resample"

##clhs regular resampling
numNeeded <- numTrans[SType == "cLHS_resample",as.integer(nTrans)]
clhsregTime <- collectTime2(Xcost,timeCCLHS,numNeeded,nSims)
clhsregTime$SType <- "CLHS_Regular"

##clhs optimised using TSP
nSims <- 5
numNeeded <- numTrans[SType == "cLHS_resample",as.integer(nTrans)]
tspTime <- foreach(n = 1:(nSims),.combine = rbind) %do% {
  time <- calcCost_tsp(Xcost,numNeeded,15)
  data.frame(Rep = n, Cost = time)
}
tspTime$SType <- "Transects_TSP"

timeAll <- rbind(TSPpoint,tspTime)#clhsTime,SPoint,clhsregTime,
timeAll <- as.data.table(timeAll)
timeAll[,Cost := Cost/480]
timeAll[,SType := as.factor(SType)]
timeAll[,SType := reorder(SType,Cost, FUN = function(x){-mean(x)})]

ggplot(timeAll, aes(x = SType, y = Cost))+
  geom_boxplot()+
    scale_y_continuous(name = "Number of 8-hour days")+
  labs(x = "Number of Sample Sites in Sample Plan") +
  scale_fill_discrete (name= 'Method', labels = c('cLHS', 'cost-constrained cLHS',  'Travelling Salesman Optimized')) +
  theme_light()
ggsave("./Results/CostsTransectvsPointsMethod.jpeg", device = 'jpeg', width = 4, height = 4)
```

## Compare SS transect data to covariate space with DataExplorer
```{r}
# SL <- fread("./InputData/Deception_Transect_SSxcovar_5m.csv")
# LL <- fread("./InputData/Deception_Transect_SSxcovar_LL.csv")
# LL <- LL %>% dplyr::select(mapunit1, mapunit2, '25m_MRVBF_', '25m_Landfo', '25m_DAH_3C')
# 
# DataExplorer::create_report(LL, y = "mapunit1")
```


