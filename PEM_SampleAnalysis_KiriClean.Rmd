---
title: "PEM Sample Analysis"
author: "Kiri Daust"
date: "20/08/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(clhs)
library(sf)
library(raster)
library(gdistance)
library(foreach)
library(data.table)
library(fasterize)
library(reticulate)
library(here)
library(LearnGeom)
library(dplyr)
library(tidyr)
library(exactextractr)
require(ggthemes)
require(philentropy)
require(tictoc)
require(ggpubr)
require(flextable)
require(doMC)

```

The below chunk creates some generic statistical functions

```{r source}
source_python("./_functions/mTSP.py")

require(doParallel)
cl <- makePSOCKcluster(detectCores()-2)
registerDoParallel(cl)

#the following line will create a local 4-node snow cluster
workers = makeCluster(6, type="SOCK")
registerDoParallel(workers)

# library(doMC)
# registerDoMC(10)

source("./_functions/SampleAnalysis_functions.R")
source("./_functions/doc_theme_pem.R")
```
```{r setup, echo = FALSE,  warning=FALSE, message = FALSE, results = FALSE}
knitr::opts_chunk$set(echo = FALSE, warning=FALSE, message = FALSE, results = FALSE, 
                      knitr.table.format = "html", fig.width=8, fig.asp = 0.8, out.width = "80%", dev = 'png', dpi = 600,
                      fig.path = 'output/figures/')

```
```{r graphics defaults,  results = 'hide'}
set_flextable_defaults(
  font.family = "Helvetica", 
  font.size = 9,
  font.color = "black",
  table.layout = "fixed",
  digits = 0,
  text.align ="centre"#,
  #theme_fun = "theme_vanilla"
  )

ggplot2::theme_set(theme_pem())
##GUI for chosing colourspace palettes
# choose_palette()
# hclwizard()

colorspace::sequential_hcl(palette = "Grays", n = 4, h = 0, c = c(0, NA, NA), l = c(20, 80), power = 1.3, rev = TRUE, fixup = FALSE, register = "Gray4Class")

# flex.default <- set_flextable_defaults(
#   font.family = "Helvetica",
#   font.size = 10,
#   font.color = NULL,
#   text.align = NULL,
#   padding = NULL,
#   padding.bottom = NULL,
#   padding.top = NULL,
#   padding.left = NULL,
#   padding.right = NULL,
#   border.color = NULL,
#   background.color = NULL,
#   line_spacing = NULL,
#   table.layout = NULL,
#   cs.family = NULL,
#   eastasia.family = NULL,
#   hansi.family = NULL,
#   decimal.mark = NULL,
#   big.mark = NULL,
#   digits = NULL,
#   na_str = NULL,
#   nan_str = NULL,
#   fmt_date = NULL,
#   fmt_datetime = NULL,
#   extra_css = NULL,
#   fonts_ignore = NULL,
#   theme_fun = NULL,
#   post_process_pdf = NULL,
#   post_process_docx = NULL,
#   post_process_html = NULL,
#   post_process_pptx = NULL
# )


```


Load data and setup covariates

```{r load data}
datLocGit <- here("InputData") ## Data
covLoc <- here("Covariates") ## Too big for git data
### landscape levels covariates
covars <- paste(covLoc, c("25m_DAH_3Class.tif","25m_LandformClass_Default_Seive4.tif",
                          "25m_MRVBF_Classified_IS64Low6Up2.tif","becRaster.tif"), sep = "/")# ,"DEM_25m.tif"
layerNamesLL <- c("DAH","LFC","MRVBF","BEC") ##need to change this if you change the layers

ancDatLL <- raster::stack(covars)
proj4string(ancDatLL) <- "+init=epsg:3005"
becLayer <- raster(paste0(covLoc,"/becRaster.tif"))
ancDatLL <- mask(ancDatLL,becLayer)

SLcov <- c("twi.tif","valley_depth_2.tif","open_neg.tif","swi_twi.tif","convexity.tif", "rid_level.tif", "mrvbf2.tif")
covars <- paste(covLoc, SLcov, sep = "/")
ancDatSL <- raster::stack(covars)
proj4string(ancDatSL) <- "+init=epsg:3005"
layerNamesSl <- names(ancDatSL) ##need to change this if you change the layers
for(i in layerNamesSl){
  ancDatSL[[i]][ancDatSL[[i]] > quantile(ancDatSL[[i]],0.975, na.rm = T)] <- NA
}

ancVals <- as.data.table(getValues(ancDatSL))##table of SL data
ancVals <- na.omit(ancVals)

##setup univariate distribution
ancBin <- ancVals[,lapply(.SD, function(x){cut(x,breaks = seq(min(x),max(x),length.out = 9),labels = F,include.lowest = T)})]
ancBin <- copy(unite(ancBin, LHSVar, sep = "_"))
ancSmall <- ancBin[,.(Num = .N), by = .(LHSVar)] ##count occurences
ancSmall <- ancSmall[Num > 150,] ##remove uncommon
setorder(ancSmall,-Num)
ancSmall[,ID := seq_along(Num)]

## dem for transtion layer should be 25m?
alt <- raster(paste0(covLoc, "/dem.tif")) 
proj4string(alt) <- "+init=epsg:3005"
altLL <- raster(paste0(covLoc, "/dem25m.tif"))
proj4string(altLL) <- "+init=epsg:3005"
## template raster
allRast <- raster(paste0(covLoc,"/Road_Rast_Small.tif"))    
allRast[allRast == 255] <- NA
allRast <- trim(allRast)

###read in roads
rdsAll <- st_read(paste0(datLocGit,"/road_access_for_cost.gpkg"))
rdsAll <- rdsAll[,"DESCRIPTIO"]
colnames(rdsAll)[1] <- "road_surface"
rdsAll <- as.data.table(rdsAll) %>% st_as_sf()

##Smithers start location
start_sf <- st_read("SmithersStart.gpkg")
start <- as(start_sf, "Spatial")
```

This section creates the transition layer from the slope and clost surface using Tobler's hiking function

```{r create cost surface}
##load previously save cost rasters
acost <- raster( "./Results/acost.tif")
acost2<- raster ("./Results/acost2.tif")
acostLL <- raster("./Results/acostLL.tif")
ancDatSL_cost <- stack(ancDatSL,acost2)
ancDatLL_cost <- stack(ancDatLL,acostLL)

#generate new cost layer
#road speed
rSpd <- fread("road_speed.csv")
rSpd[,Values := speed]
rdsAll <- merge(rdsAll, rSpd, by = "road_surface", all = F)
rdsAll <- rdsAll[,"Values"] %>%
  st_buffer(dist = 35, endCapStyle = "SQUARE", joinStyle = "MITRE") %>%
  st_cast("MULTIPOLYGON")
rdsRast <- fasterize(rdsAll, allRast, field = "Values")

alt2 <- projectRaster(alt,rdsRast,method = 'ngb')
altAll <- merge(rdsRast, alt2)
altSmall <- raster::aggregate(altAll, fact = 3L, fun = min)

## trasition function: if > 500, elevation, so get diff, else speed to select min
trFn <- function(x){
  if(x[1] > 500 & x[2] > 500){
    x[1]-x[2]
  }else{
    min(x[1],x[2])
  }
}

# rdIdx <- which(values(altAll) < 100) ##which cells are for roads?
# slpIdx <- which(values(altAll) > 500)
# adj <- adjacent(altAll, cells = slpIdx, pairs = T, directions = 8)
# adj <- adj[!adj[,1] %in% rdIdx ,] ##remove roads inside area
# adj <- adj[!adj[,2] %in% rdIdx ,]
# 
# tr <- transition(altAll,trFn,directions = 8, symm = F) ##altDiff and speed (km/h)
# tr1 <- geoCorrection(tr) ##divided by 25 - slope and conductance (km/h/m)
# tr1[adj] <- (3/5)*(6*exp(-3.5*abs(tr1[adj] + 0.08))) ##tobler's hiking function * 3/5 - gives km/h
# tr1 <- tr1*1000 ##now roads are correct conductance (h/m), and walking in m/h
# tr2 <- geoCorrection(tr1) ##have to geocorrect this part again
# tr1[adj] <- tr2[adj] ##tr1 values are now all conductance in h/metre
# 
# acost <- accCost(tr1,start)
# plot(acost)

##create smaller version for TSP
rdIdx <- which(values(altSmall) < 100) ##which cells are for roads?
slpIdx <- which(values(altSmall) > 500)
adj <- adjacent(altSmall, cells = slpIdx, pairs = T, directions = 8)
adj <- adj[!adj[,1] %in% rdIdx ,] ##remove roads inside area
adj <- adj[!adj[,2] %in% rdIdx ,]

tr <- transition(altSmall,trFn,directions = 8, symm = F) ##altDiff and speed (km/h) - need fro TSP
trSmall <- geoCorrection(tr) ##divided by 25 - slope and conductance (km/h/m)
trSmall[adj] <- (3/5)*(6*exp(-3.5*abs(trSmall[adj] + 0.08))) ##tobler's hiking function * 3/5 - gives km/h
trSmall <- trSmall*1000 ##now roads are correct conductance (h/m), and walking in m/h
tr2 <- geoCorrection(trSmall) ##have to geocorrect this part again
trSmall[adj] <- tr2[adj] ##tr1 values are now all conductance in h/metre

# acost2 <- projectRaster(acost, ancDatSL)
# acost2 <- mask(acost2, alt)
# acostLL <- projectRaster(acost, ancDatLL)
# acostLL  <- mask(acostLL, altLL)
# ancDatSL_cost <- stack(ancDatSL,acost2)
# 
# plot(acost2)
# writeRaster(acost, "./Results/acost.tif", format = "GTiff", overwrite = TRUE)
# writeRaster(acost2, "./Results/acost2.tif", format = "GTiff", overwrite = TRUE)
# writeRaster(acostLL, "./Results/acostLL.tif", format = "GTiff", overwrite = TRUE)
# png('./outputs/acost2.png', height=nrow(acost2), width=ncol(acost2))
# plot(acost2, maxpixels=ncell(acost2))
# dev.off()

```
### Simulation 1: Single point sampling

For all the following simulations we're testing with random samples, LHS samples, CCLHS samples and sometimes TSP sampling (not for single points).

There are different ways of testing space filling: in the below code, we use KS statistics, a simple bin fill function

```{r setup distribtuion and test functions}
tempID <- ancDatSL$twi
tempID[] <- 1:(nrow(tempID)*ncol(tempID))
names(tempID) <- "CellID"
ancDatSL <- stack(ancDatSL,tempID)

###setup full LHS distribution for statistics
ancVals <- as.data.table(getValues(ancDatSL))##table of SL data
ancVals <- na.omit(ancVals)
idVals <- ancVals$CellID
ancVals[,CellID := NULL]
ancValsSm <- ancVals[sample(.N,5e5)]

##single points clhs sampled
allSLDat <- as.data.table(getValues(ancDatSL_cost))##table of SL data
allSLDat <- na.omit(allSLDat)
slDatSm <- allSLDat[sample(.N,5e5)]
```



```{r sample functions single point}
##functions to create sampling setups
# createRandom <- function(X,n){
#   small <- sample(1:nrow(X),size = n, replace = F)
#   return(X[small,])
# }
# 
createLHS <- function(X,n){
  temp <- clhs(X,size = n,use.cpp = T, iter = 5000, simple = F)
  small <- as.matrix(temp$sampled_data)
  return(small)
}

createCCLHS <- function(X,n){
  temp <- clhs(X,size = n,cost = ncol(X),use.cpp = T, iter = 5000,simple = F)
  small <- as.matrix(temp$sampled_data)
  return(small)
}
#X=ancVals; sampleFun=createLHS; nums = nums; testFun1 = KL_stat_clhs; testFun2 = KS_stat_clhs; n=1000
#collecting stats with different space filling tests
#                     .packages = c("philentropy","foreach","clhs","data.table","dplyr","tidyr")
collectStats <- function(X,sampleFun,nums,testFun1 = KL_stat_clhs){ ##KS_prob_clhs
  test_res <- foreach(n = nums, .combine = rbind) %do% {
                      cat(".")
                      smallSet <- sampleFun(X,n)
                      res1 <- testFun1(sample = smallSet,ancSmall)
                    data.table(Num = n,KL = res1)
                    }
  return(test_res)
}

nums <- c(500, 1000, 1500, 2000, 2500, 3000,3500)
nums <- rep(nums, each = 5)
statsCCLHS <- collectStats(slDatSm,createCCLHS,nums)
statsCCLHS$SType <- "CCLHS"
```

```{r num single points}
KLCutoff <- 1.25
###this needs to be adjusted to reflect a set KL distance
statsAll <- as.data.table(statsCCLHS)
temp <- statsAll[,.(y = median(KL)), by = .(Num)]
#temp[,`:=`(y = (y-min(y))/(max(y)-min(y)), Num = as.numeric(as.character(Num)))]
temp[,`:=`(Num = as.numeric(as.character(Num)))]
numPoints <- temp[,.(nPoints = predict(loess(Num ~ y,data = .SD), data.frame(y = KLCutoff)))]
numSinglePts <- numPoints$nPoints[1] %>% round(0)

```

# Create examples of summary distributions for manuscript
#
```{r univariate distributions}
s1 <- createLHS(ancVals,100)
s2 <- createLHS(ancVals,500)
s3 <- createLHS(ancVals,1000)
s4 <- createLHS(ancVals,2000)
s5 <- createLHS(ancVals,3000)
s6 <- createLHS(ancVals,4000)
temp <- plotUniDists(s1)
t1 <- temp$data
t1[,NumSamples := 100]
t1[,Label := paste0("100 Samples\nKL = ",round(temp$KL, digits = 3))]
temp <- plotUniDists(s2)
t2 <- temp$data
t2[,NumSamples := 500]
t2[,Label := paste0("500 Samples\nKL = ",round(temp$KL, digits = 3))]
temp <- plotUniDists(s3)
t3 <- temp$data
t3[,NumSamples := 1000]
t3[,Label := paste0("1000 Samples\nKL = ",round(temp$KL, digits = 3))]
temp <- plotUniDists(s4)
t4 <- temp$data
t4[,NumSamples := 2000]
t4[,Label := paste0("2000 Samples\nKL = ",round(temp$KL, digits = 3))]
temp <- plotUniDists(s6)
t5 <- temp$data
t5[,NumSamples := 3000]
t5[,Label := paste0("3000 Samples\nKL = ",round(temp$KL, digits = 3))]

temp <- plotUniDists(s6)
t6 <- temp$data
t6[,NumSamples := 4000]
t6[,Label := paste0("4000 Samples\nKL = ",round(temp$KL, digits = 3))]

dat <- rbind(t1,t2,t3,t4,t5,t6)
dat[,Label := as.factor(Label)]
dat[,Label := reorder(Label,NumSamples)]

dat2 <- melt(dat, id.vars = c("Label","ID"), measure.vars = c("Num","SampNum"))
source("./_functions/doc_theme_pem.R")
fillspace <- ggplot(dat2, aes(x = ID, y = value, colour = variable, size = variable)) +
  geom_line() +
  facet_wrap(.~Label)+
  theme_pem_facet()+
  scale_color_manual(name = "Source", values = c("black","grey"), labels = c("Map Area","cLHS Sample")) +
  scale_size_manual(name = "Source",values = c(3.2,0.8),labels = c("Actual Distribution","cLHS Sample")) +
  scale_alpha(range = c(0.4, 0.8))+
  labs(x = "Multivariate Bins", y = "Bin ratio of total map area")
fillspace
  finalise_plot (fillspace, "D:/GitHub/PEM_Methods_DevX/PEM_standards_manuscripts/outputs/ptsamplestofillbins.png",
                          width_pixels=640,
                          height_pixels=450)
#dev.off()
  # require(CDFt)
  # test <- dat %>% dplyr::filter(Label == "50000 Samples\nKL = 0.048") %>% select(ID,Num, SampNum)
  #  res <- ks.test(test$Num,test$SampNum)
  #  res$p.value
  # res <- CramerVonMisesTwoSamples(test$Num, test$SampNum)
  # p_value = 1/6*exp(-res) %>% as.integer
  # p_value
```


### Simulation 2: Transect Sampling

Now instead of choosing single points, we use the cclhs or lhs to pick centre points, and then create a triangular transect around it, which we then sample from (either using a second clhs or just regular spacing). The below chunk simulates 3 transect scenarios: non cost-constrained clhs, using clhs to sample from transects, and cost-constrained clhs using both clhs and regular spacing for transect sampling. Note that this chunk takes a while to run and will use all your CPU resources. The time limiting part is not the clhs function, but rather extracting data from the raster stack once each transect has been created (even using Velox this takes a while).

```{r set up transect simulations}
ancDatLL_cost <- stack(ancDatLL,acostLL)
fullSet <- sampleRegular(ancDatLL, size = 200000,sp = T) 
X <- st_as_sf(fullSet) ##This line takes a while, but I'm not sure of an alternative
X <- X[!is.na(X$X25m_DAH_3Class),]
# 
# fullSL <- sampleRegular(ancDatSL, size = 100000,GDAL = T)
# fullSL <- fullSL[complete.cases(fullSL),]

fullSet <- sampleRegular(ancDatLL_cost, size = 200000,sp = T)
Xcost <- st_as_sf(fullSet)
Xcost <- Xcost[!is.na(Xcost$X25m_DAH_3Class),]
Xcost <- Xcost[!is.infinite(Xcost$acostLL),]

##stand level
fullSet <- sampleRegular(ancDatSL_cost, size = 500000,sp = T)
Xcost_SL <- st_as_sf(fullSet)
Xcost_SL <- Xcost_SL[!is.na(Xcost_SL$twi),]
```



```{r sampling functions}
##create cclhs transects, sample using lhs
transectsPerSlice <- 5

createCCLHS_transect <- function(X,nSlices){
  xMat <- sf::st_drop_geometry(X)
  currpoints <- NULL
  pointsPerSlice <- transectsPerSlice
  datOut <- foreach(snum = 1:nSlices,.combine = rbind) %do% {
    cat(".")
    temp <- clhs(xMat,size = pointsPerSlice * snum,must.include = currpoints,
                 cost = ncol(xMat),iter = 5000,use.cpp = T,simple = F)
    currpoints <- temp$index_samples
    dat <- X[temp$index_samples,]
    small <- foreach(j = 1:nrow(dat),.combine = rbind) %do% {
      tri <- Tri_build(id = j, x = st_coordinates(dat[j,])[1],y = st_coordinates(dat[j,])[2])
      tri <- st_buffer(tri, dist = 5)
      st_crs(tri) <- 3005
      tempDat <- exact_extract(ancDatSL,tri)[[1]]
      tempDat[,-c(8,9)]
    }
    data.table(SliceNum = snum, KL = suppressMessages(KL_stat_noextra(sample = small, ancSmall = ancSmall)),
               BinFill = binfill_clhs(sample = small, ancSmall = ancSmall))
  }
  
  return(datOut)
}

createCCLHS_transect_paired <- function(X,nSlices){
  xMat <- st_drop_geometry(X)
  currpoints <- NULL
  pointsPerSlice <- transectsPerSlice
  datOut <- foreach(snum = 1:nSlices,.combine = rbind) %do% {
    cat(".")
    temp <- clhs(xMat,size = pointsPerSlice * snum,must.include = currpoints,
                 cost = ncol(xMat),iter = 5000,use.cpp = T,simple = F)
    currpoints <- temp$index_samples
    dat <- X[temp$index_samples,]
    small <- foreach(j = 1:nrow(dat),.combine = rbind) %do% {
      tri <- Tri_build(id = j, x = st_coordinates(dat[j,])[1],y = st_coordinates(dat[j,])[2])
      tri <- st_buffer(tri, dist = 5)
      st_crs(tri) <- 3005
      tri2 <- rotFeature(tri,dat$geometry[j],45)
      tri2$geometry <- tri2$geometry + matrix(data = c(0, 400), ncol = 2)
      st_crs(tri2$geometry) <- 3005
      tri <- st_union(tri$g,tri2$geometry)
      tempDat <- exact_extract(ancDatSL,tri)[[1]]
      tempDat[,-c(8,9)]
    }
    data.table(SliceNum = snum, KL = suppressMessages(KL_stat_noextra(sample = small, ancSmall = ancSmall)),
               BinFill = binfill_clhs(sample = small, ancSmall = ancSmall))
  }
  
  return(datOut)
}

maxSlices <- 15
transectSim_single <- foreach(i = 1:10, .combine = rbind) %do% {
  temp <- createCCLHS_transect(Xcost,maxSlices)
  temp[,SimNum := i]
  temp
}
transectSim_single[,Type := "Single Transect"]

transectSim_paired <- foreach(i = 1:10, .combine = rbind) %do% {
  temp <- createCCLHS_transect_paired(Xcost,maxSlices)
  temp[,SimNum := i]
  temp
}
transectSim_paired[,Type := "Paired Transect"]

allTrans <- rbind(transectSim_single,transectSim_paired)
allTrans[,NumTransects := SliceNum * transectsPerSlice]
allTrans[,Type := as.factor(Type)]
allTrans[,NumTransects := as.factor(NumTransects)]

num.transects <- ggplot(allTrans, aes(x = NumTransects, y = KL, fill = Type)) +
  geom_boxplot()+
  geom_hline(yintercept=1.3, linetype="dashed", color = "black") +
  scale_fill_grey(start = 0.5)+
  theme_pem()
num.transects

finalise_plot (num.transects, "D:/GitHub/PEM_Methods_DevX/PEM_standards_manuscripts/outputs/num.transects.png",
                          width_pixels=640,
                          height_pixels=450)
```

## Calculate number of sites to fill

```{r num points}
KLCutoff <- 1.25
statsAll2 <- as.data.table(allTrans)
temp <- statsAll2[,.(y = median(KL)), by = .(Type,SliceNum)]
temp[,`:=`(SliceNum = as.numeric(as.character(SliceNum)))]
numTrans <- temp[,.(nSlice = predict(loess(SliceNum ~ y,data = .SD), data.frame(y = KLCutoff))), by = .(Type)] 
numTrans$nSlice <-   round(numTrans$nSlice, 0)

#knitr::kable(numTrans, digits = 2, caption = paste0("Number of slices for KL divergence = ",KLCutoff))

transSingleNum <- numTrans$nSlice[1] %>% round(0)
transPairedNum <- numTrans$nSlice[2] %>% round(0)
numSinglePts$Type <- "Single Points" 
numSinglePts <- numSinglePts %>% as.data.frame %>% rename(nSlice = 1) %>% select(Type, nSlice)
MinSample <- rbind(numSinglePts, numTrans ) %>% rename("Sample_Type" = Type, "Min._Sample_Units" = nSlice)
MinSample$Min._Sample_Units <- round(MinSample$Min._Sample_Units,0)

MinSample_flex <- flextable(MinSample) %>% autofit()# %>%  align(align = "center", part = "all") %>% set_table_properties(width = .5, layout = "autofit")
MinSample_flex
save_as_docx(MinSample_flex , path = "D:/GitHub/PEM_Methods_DevX/PEM_standards_manuscripts/outputs/MinimumSampleNumber_KL125.docx")

```


## Simulations Time to fill variable space

Instead of just calculating the time cost for different numbers of transects, we can use the TSP to estimate the cost of using each sampling method to fill the variable space. Thus, here we use the number of transects needed to fill the variable space as calculated above, and use the TSP to estimate the cost of each method.

```{r num}
numSims <- 10

## cost_constrained clhs
timeCCLHS <- function(X,n){
  xMat <- st_drop_geometry(X)
  temp <- clhs(xMat,size = n,cost = ncol(xMat),iter = 5000,use.cpp = T,simple = F)
  return(temp)
}

X <- Xcost
num <- 20
templhs <- timeCCLHS(X,num)
dat <- X[templhs$index_samples,]
temp <- calcCost_clhs(dat,templhs$final_obj_continuous,45L,3L)

num <- 100
templhs <- timeCCLHS(X,num)
dat <- X[templhs$index_samples,]
calcCost_clhs(dat,templhs$final_obj_continuous,2L,20L)

collectTime2 <- function(X,num, nReps = numSims, plotTime = 45L, minNum = 3L){
  pkgs <- c("Rcpp","LaplacesDemon", "foreach","sf",
          "raster","LearnGeom","dplyr","reticulate","gdistance","scales","clhs")
  # .packages = pkgs,
  #                   .export = c("start_sf","trSmall", "calcCost_clhs")
  test_res <- foreach(n = 1:nReps, .combine = rbind) %do% { #
                      #reticulate::source_python("mTSP.py")
                      templhs <- timeCCLHS(X,num)
                      dat <- X[templhs$index_samples,]
                      if(nrow(dat) > 1){
                        time <- calcCost_clhs(dat,templhs$final_obj_continuous,plotTime,minNum)
                        data.table(Rep = n, Cost = time$totTime, NumDays = time$nDays)
                      }else{
                        NULL
                      }
                      
                    }
  return(test_res)
}

#transSingleNum <- 10 ## ASSIGNED ABOVE
#transPairedNum <- 5## ASSIGNED ABOVE
transTime <- collectTime2(Xcost,as.integer((transSingleNum * 5)))
transTime$SType <- "Single Transect"
transTime2 <- transTime %>% mutate(NumDays = NumDays/2)
pairedTime <- collectTime2(Xcost,as.integer((transPairedNum * 5)))
pairedTime$SType <- "Paired Transect"
################################################################
##single points 
#numSinglePts <- 1700 ##from script above
numNeeded <- 100L
numTimes <- numSinglePts/numNeeded
singleTime <- collectTime2(Xcost_SL,num = numNeeded,nReps = numSims, plotTime = 2L,minNum = 10L)
singleTime[,NumDays := NumDays*numTimes]
singleTime$SType <- "Single Points"
singleTime2 <- singleTime %>% mutate(NumDays = NumDays/2)

allStats <- rbind(singleTime2,transTime2,pairedTime)#
allStats[,SType := as.factor(SType)]
allStats[,SType := reorder(SType,NumDays, FUN = function(x){-mean(x)})]

samplecost <- ggplot(allStats,aes(x = SType, y = NumDays)) +
  geom_boxplot()+
  theme_pem()+
  ylab("Number of crew-days")+
  xlab("Sample method")
samplecost
  
finalise_plot (samplecost, "./Results/coststofillhypercube.png",
                          width_pixels=640,
                          height_pixels=450)
## flextable of costs
temp2 <- allStats[,.(y = mean(NumDays)), by = .(SType)]
#temp2[,`:=`(SliceNum = as.numeric(as.character(SliceNum)))]

Min.sample.costs <- cbind(MinSample, temp2) %>% select(-3) %>% rename(mean_crew_days = 3) 
Min.sample.costs$mean_crew_days <- round(Min.sample.costs$mean_crew_days, 0)
Min.sample.costs_flex <- flextable(Min.sample.costs) %>% align(align = "center", part = "all") %>% set_table_properties(width = .5, layout = "autofit")
Min.sample.costs_flex
save_as_docx(Min.sample.costs_flex , path = "D:/GitHub/PEM_Methods_DevX/PEM_standards_manuscripts/outputs/Costs_MinimumSampleNumber_KL125.docx")

#mean.days <- 

# SPoint <- as.data.table(SPoint)
# SPoint[,TSPNum := rep(c(1:nRep),each = nSims)]
# TSPpoint <- SPoint[,.(Cost = min(Cost)), by = .(TSPNum)]
# TSPpoint[,SType := "Single Point TSP"]
# setnames(TSPpoint, old = "TSPNum", new = "Rep")
# SPoint[,TSPNum := NULL]
# SPoint[,SType := "Single Point CLHS"]

##clhs resampled clhs
# numNeeded <- numTrans[SType == "CCLHS",as.integer(nTrans)]
# clhsTime <- collectTime2(Xcost,timeCCLHS,numNeeded,nSims)
# clhsTime$SType <- "CLHS_Resample"

#

##clhs optimised using TSP
# nSims <- 5
# numNeeded <- numTrans[SType == "cLHS_resample",as.integer(nTrans)]
# tspTime <- foreach(n = 1:(nSims),.combine = rbind) %do% {
#   time <- calcCost_tsp(Xcost,numNeeded,15)
#   data.frame(Rep = n, Cost = time)
# }
# tspTime$SType <- "Transects_TSP"
# 
# timeAll <- rbind(TSPpoint,tspTime)#clhsTime,SPoint,clhsregTime,
# timeAll <- as.data.table(timeAll)
# timeAll[,Cost := Cost/480]

# 
# ggplot(timeAll, aes(x = SType, y = Cost))+
#   geom_boxplot()+
#     scale_y_continuous(name = "Number of 8-hour days")+
#   labs(x = "Number of Sample Sites in Sample Plan") +
#   scale_fill_discrete (name= 'Method', labels = c('cLHS', 'cost-constrained cLHS',  'Travelling Salesman Optimized')) +
#   theme_light()
# ggsave("./Results/CostsTransectvsPointsMethod.jpeg", device = 'jpeg', width = 4, height = 4)
```

## Test filling of real sample data from Deception Lake Study area. Add by slice

```{r}
library(velox)
ancDatVL <- velox(ancDatSL)

s1Dat <- fread("./InputData/s1_clean_pts_all_fnf.csv")
numTrans <- unique(s1Dat[,.(slice,transect_id)])
numTrans <- numTrans[,.(Num = .N), by = .(slice)]
s1Dat <- s1Dat[,.(x,y,ID,mapunit1, slice)]
setnames(s1Dat, old = "mapunit1", new = "Unit")
s1Points <- st_as_sf(s1Dat,coords = c("x","y"), crs = 3005)
totalSlices <- max(s1Points$slice)

##all slice combinations
datAll <- foreach(nslice = 1:totalSlices, .combine = rbind) %do% {
  cat(".")
  allComb <- combn(1:totalSlices,m = nslice)
  out <- foreach(i = 1:ncol(allComb), .combine = rbind) %do% {
    sample <- s1Points[s1Points$slice %in% allComb[,i],]
    samp2 <- as.data.table(ancDatVL$extract_points(sample)[,1:7])
    data.table(SliceNum = nslice,Iter = i, KL = suppressMessages(KL_stat_noextra(sample = samp2, ancSmall = ancSmall)), BinFill = binfill_clhs(sample = samp2, ancSmall = ancSmall))
  }
  out
}

datAll$SliceNum <- as.factor(datAll$SliceNum)
fieldslicecovariate <- ggplot(datAll,aes(x = SliceNum, y = KL, group = SliceNum)) +
  geom_boxplot()+
  geom_hline(yintercept=1.25, linetype="dashed", color = "black")+
  theme_pem() +
  ylab("Kullback-Leibler Distance")+
  scale_x_discrete("Number of paired transect slices", labels = c("1" = "3","2" = "6", "3" = "9","4" = "12","5" = "15", "6" = "18"))

fieldslicecovariate 

  finalise_plot (fieldslicecovariate, "D:/GitHub/PEM_Methods_DevX/PEM_standards_manuscripts/outputs/fieldslicestofillbins.jpg",
                          width_pixels=450,
                          height_pixels=450)
#dev.off()
```

## Map and example TSP Route

```{r tsp_example}
n = 25

xMat <- st_drop_geometry(Xcost)
temp <- clhs(xMat,size = n,cost = ncol(xMat),iter = 10000,use.cpp = T,simple = F)
pnts <- Xcost[temp$index_samples,]
pnts <- pnts[,1]
colnames(pnts) <- c("name","geometry")
st_geometry(pnts) <- "geometry"
startPnts <- st_as_sf(data.frame(name = "Start",geometry = start_sf))
pnts <- rbind(pnts, startPnts)
pnts2 <- as(pnts, "Spatial")
## create distance matrix between sample points
test <- costDistance(trSmall,pnts2,pnts2)
dMat2 <- as.matrix(test)
dMat2 <- dMat2*60
dMat2[is.infinite(dMat2)] <- 1000

##penalty based on quality of points
objVals <- max(temp$final_obj_continuous) - temp$final_obj_continuous
maxTime <- 8L ##hours
## time per transect
plotTime <- 50L ##mins
temp <- dMat2[1:n,1:n]
maxDist <- sum(temp[upper.tri(temp)])
minPen <- maxDist
maxPen <- maxDist * 4
objVals <- scales::rescale(objVals, to = c(minPen,maxPen))
objVals <- as.integer(objVals)

ndays <- as.integer(ceiling(n/5)+1)
pen = objVals

indStart <- as.integer(rep(n,ndays))
##run vehicle routing problem from python script
## GCS is global span cost coefficient
vrp <- py_mTSP(dat = dMat2,num_days = ndays, start = indStart, end = indStart, 
               max_cost = maxTime*60L, plot_time = plotTime, penalty =  pen, arbDepot = F, GSC = 100L)

result <- vrp[[1]]

## create spatial paths
paths <- foreach(j = 0:(length(result)-1), .combine = rbind) %do% {
  if(length(result[[as.character(j)]]) > 2){
    cat("Drop site",j,"...\n")
    p1 <- result[[as.character(j)]]+1
    out <- foreach(i = 1:(length(p1)-1), .combine = rbind) %do% {
      temp1 <- pnts[p1[i],]
      temp2 <- pnts[p1[i+1],]
      temp3 <- shortestPath(trSmall,st_coordinates(temp1),
                            st_coordinates(temp2),output = "SpatialLines") %>% st_as_sf()
      temp3$Segment = i
      temp3
    }
    out$DropSite = j
    out
  }
  
}

paths <- st_transform(paths, 3005)
#st_write(paths, dsn = "RoadTSP.gpkg", layer = "Paths", append = T, driver = "GPKG")  
jpeg(file="D:/GitHub/PEM_Methods_DevX/PEM_standards_manuscripts/outputs/VRP_example_map.jpg", width=600, height=480)
plot(acost2)
plot(pnts,col = "black", add = T)
plot(paths,col = as.factor(paths$DropSite), add = T)

## label points
p2 <- pnts
p2$PID <- seq_along(p2$name)
p2 <- p2[,"PID"]
p2$DropLoc <- NA
p2$Order <- NA
for(i in 0:(length(result)-1)){
  p1 <- result[[as.character(i)]]+1
  p1 <- p1[-1]
  p2$DropLoc[p1] <- i
  p2$Order[p1] <- 1:length(p1)
}
p2 <- st_transform(p2, 3005)

p2
dev.off()


```

## Compare SS transect data to covariate space with DataExplorer
```{r}
# SL <- fread("./InputData/Deception_Transect_SSxcovar_5m.csv")
# LL <- fread("./InputData/Deception_Transect_SSxcovar_LL.csv")
# LL <- LL %>% dplyr::select(mapunit1, mapunit2, '25m_MRVBF_', '25m_Landfo', '25m_DAH_3C')
# 
# DataExplorer::create_report(LL, y = "mapunit1")
```


