---
title: "PEM Sample Analysis"
author: "Kiri Daust"
date: "20/08/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(clhs)
library(sf)
library(raster)
library(gdistance)
library(foreach)
library(data.table)
library(fasterize)
library(reticulate)
library(here)
library(LearnGeom)
library(dplyr)
library(tidyr)
library(exactextractr)
require(ggthemes)
require(philentropy)
require(tictoc)
require(ggpubr)
require(flextable)

```

The below chunk creates some generic statistical functions

```{r source}
source_python("./mTSP.py")

# require(doParallel)
# cl <- makePSOCKcluster(detectCores()-4)
# registerDoParallel(cl)
library(doMC)
registerDoMC(10)

source("./SampleAnalysis_functions.R")
source("./_functions/doc_theme_pem.R")
```

Load data and setup covariates

```{r load data}
datLocGit <- here("InputData") ## Data
covLoc <- here("Covariates/Covariates_PemTSP") ## Too big for git data
### landscape levels covariates
covars <- paste(covLoc, c("25m_DAH_3Class.tif","25m_LandformClass_Default_Seive4.tif",
                          "25m_MRVBF_Classified_IS64Low6Up2.tif","becRaster.tif"), sep = "/")# ,"DEM_25m.tif"
layerNamesLL <- c("DAH","LFC","MRVBF","BEC") ##need to change this if you change the layers

ancDatLL <- raster::stack(covars)
proj4string(ancDatLL) <- "+init=epsg:3005"
becLayer <- raster(paste0(covLoc,"/becRaster.tif"))
ancDatLL <- mask(ancDatLL,becLayer)

SLcov <- c("twi.tif","valley_depth_2.tif","open_neg.tif","swi_twi.tif","convexity.tif", "rid_level.tif", "mrvbf2.tif")
covars <- paste(covLoc, SLcov, sep = "/")
ancDatSL <- raster::stack(covars)
proj4string(ancDatSL) <- "+init=epsg:3005"
layerNamesSl <- names(ancDatSL) ##need to change this if you change the layers
for(i in layerNamesSl){
  ancDatSL[[i]][ancDatSL[[i]] > quantile(ancDatSL[[i]],0.975, na.rm = T)] <- NA
}

ancVals <- as.data.table(getValues(ancDatSL))##table of SL data
ancVals <- na.omit(ancVals)

##setup univariate distribution
ancBin <- ancVals[,lapply(.SD, function(x){cut(x,breaks = seq(min(x),max(x),length.out = 9),labels = F,include.lowest = T)})]
ancBin <- copy(unite(ancBin, LHSVar, sep = "_"))
ancSmall <- ancBin[,.(Num = .N), by = .(LHSVar)] ##count occurences
ancSmall <- ancSmall[Num > 150,] ##remove uncommon
setorder(ancSmall,-Num)
ancSmall[,ID := seq_along(Num)]

## dem for transtion layer should be 25m?
alt <- raster(paste0(covLoc, "/dem.tif")) 
proj4string(alt) <- "+init=epsg:3005"
altLL <- raster(paste0(covLoc, "/dem25m.tif"))
proj4string(altLL) <- "+init=epsg:3005"
## template raster
allRast <- raster(paste0(covLoc,"/Road_Rast_Small.tif"))    
allRast[allRast == 255] <- NA
allRast <- trim(allRast)

###read in roads
rdsAll <- st_read(paste0(datLocGit,"/road_access_for_cost.gpkg"))
rdsAll <- rdsAll[,"DESCRIPTIO"]
colnames(rdsAll)[1] <- "road_surface"
rdsAll <- as.data.table(rdsAll) %>% st_as_sf()

##Smithers start location
start_sf <- st_read("SmithersStart.gpkg")
start <- as(start_sf, "Spatial")
```

This section creates the transition layer from the slope and clost surface using Tobler's hiking function

```{r create cost surface}
##load previously save cost rasters
acost <- raster( "./Results/acost.tif")
acost2<- raster ("./Results/acost2.tif")
acostLL <- raster("./Results/acostLL.tif")
ancDatSL_cost <- stack(ancDatSL,acost2)
ancDatLL_cost <- stack(ancDatLL,acostLL)

#generate new cost layer
#road speed
rSpd <- fread("road_speed.csv")
rSpd[,Values := speed]
rdsAll <- merge(rdsAll, rSpd, by = "road_surface", all = F)
rdsAll <- rdsAll[,"Values"] %>%
  st_buffer(dist = 35, endCapStyle = "SQUARE", joinStyle = "MITRE") %>%
  st_cast("MULTIPOLYGON")
rdsRast <- fasterize(rdsAll, allRast, field = "Values")

alt2 <- projectRaster(alt,rdsRast,method = 'ngb')
altAll <- merge(rdsRast, alt2)
altSmall <- raster::aggregate(altAll, fact = 3L, fun = min)

## trasition function: if > 500, elevation, so get diff, else speed to select min
trFn <- function(x){
  if(x[1] > 500 & x[2] > 500){
    x[1]-x[2]
  }else{
    min(x[1],x[2])
  }
}

# rdIdx <- which(values(altAll) < 100) ##which cells are for roads?
# slpIdx <- which(values(altAll) > 500)
# adj <- adjacent(altAll, cells = slpIdx, pairs = T, directions = 8)
# adj <- adj[!adj[,1] %in% rdIdx ,] ##remove roads inside area
# adj <- adj[!adj[,2] %in% rdIdx ,]
# 
# tr <- transition(altAll,trFn,directions = 8, symm = F) ##altDiff and speed (km/h)
# tr1 <- geoCorrection(tr) ##divided by 25 - slope and conductance (km/h/m)
# tr1[adj] <- (3/5)*(6*exp(-3.5*abs(tr1[adj] + 0.08))) ##tobler's hiking function * 3/5 - gives km/h
# tr1 <- tr1*1000 ##now roads are correct conductance (h/m), and walking in m/h
# tr2 <- geoCorrection(tr1) ##have to geocorrect this part again
# tr1[adj] <- tr2[adj] ##tr1 values are now all conductance in h/metre
# 
# acost <- accCost(tr1,start)
# plot(acost)

##create smaller version for TSP
rdIdx <- which(values(altSmall) < 100) ##which cells are for roads?
slpIdx <- which(values(altSmall) > 500)
adj <- adjacent(altSmall, cells = slpIdx, pairs = T, directions = 8)
adj <- adj[!adj[,1] %in% rdIdx ,] ##remove roads inside area
adj <- adj[!adj[,2] %in% rdIdx ,]

tr <- transition(altSmall,trFn,directions = 8, symm = F) ##altDiff and speed (km/h) - need fro TSP
trSmall <- geoCorrection(tr) ##divided by 25 - slope and conductance (km/h/m)
trSmall[adj] <- (3/5)*(6*exp(-3.5*abs(trSmall[adj] + 0.08))) ##tobler's hiking function * 3/5 - gives km/h
trSmall <- trSmall*1000 ##now roads are correct conductance (h/m), and walking in m/h
tr2 <- geoCorrection(trSmall) ##have to geocorrect this part again
trSmall[adj] <- tr2[adj] ##tr1 values are now all conductance in h/metre

# acost2 <- projectRaster(acost, ancDatSL)
# acost2 <- mask(acost2, alt)
# acostLL <- projectRaster(acost, ancDatLL)
# acostLL  <- mask(acostLL, altLL)
# ancDatSL_cost <- stack(ancDatSL,acost2)
# 
# plot(acost2)
# writeRaster(acost, "./Results/acost.tif", format = "GTiff", overwrite = TRUE)
# writeRaster(acost2, "./Results/acost2.tif", format = "GTiff", overwrite = TRUE)
# writeRaster(acostLL, "./Results/acostLL.tif", format = "GTiff", overwrite = TRUE)
# png('./outputs/acost2.png', height=nrow(acost2), width=ncol(acost2))
# plot(acost2, maxpixels=ncell(acost2))
# dev.off()

```
### Simulation 1: Single point sampling

For all the following simulations we're testing with random samples, LHS samples, CCLHS samples and sometimes TSP sampling (not for single points).

There are different ways of testing space filling: in the below code, we use KS statistics, a simple bin fill function

```{r setup distribtuion and test functions}
tempID <- ancDatSL$twi
tempID[] <- 1:(nrow(tempID)*ncol(tempID))
names(tempID) <- "CellID"
ancDatSL <- stack(ancDatSL,tempID)

###setup full LHS distribution for statistics
ancVals <- as.data.table(getValues(ancDatSL))##table of SL data
ancVals <- na.omit(ancVals)
idVals <- ancVals$CellID
ancVals[,CellID := NULL]
ancValsSm <- ancVals[sample(.N,5e5)]

##single points clhs sampled
allSLDat <- as.data.table(getValues(ancDatSL_cost))##table of SL data
allSLDat <- na.omit(allSLDat)
slDatSm <- allSLDat[sample(.N,5e5)]
```



```{r sample functions single point}
##functions to create sampling setups
# createRandom <- function(X,n){
#   small <- sample(1:nrow(X),size = n, replace = F)
#   return(X[small,])
# }
# 
# createLHS <- function(X,n){
#   temp <- clhs(X,size = n,use.cpp = T, iter = 10000, simple = F)
#   small <- as.matrix(temp$sampled_data)
#   return(small)
# }

createCCLHS <- function(X,n){
  temp <- clhs(X,size = n,cost = ncol(X),use.cpp = T, iter = 5000,simple = F)
  small <- as.matrix(temp$sampled_data)
  return(small)
}
#X=ancVals; sampleFun=createLHS; nums = nums; testFun1 = KL_stat_clhs; testFun2 = KS_stat_clhs; n=1000
#collecting stats with different space filling tests
#                     .packages = c("philentropy","foreach","clhs","data.table","dplyr","tidyr")
collectStats <- function(X,sampleFun,nums,testFun1 = KL_stat_clhs){ ##KS_prob_clhs
  test_res <- foreach(n = nums, .combine = rbind) %do% {
                      cat(".")
                      smallSet <- sampleFun(X,n)
                      res1 <- testFun1(sample = smallSet,ancSmall)
                    data.table(Num = n,KL = res1)
                    }
  return(test_res)
}

nums <- c(500, 1000, 1500, 2000, 2500, 3000,3500)
nums <- rep(nums, each = 5)
statsCCLHS <- collectStats(slDatSm,createCCLHS,nums)
statsCCLHS$SType <- "CCLHS"
```

```{r num single points}
KLCutoff <- 1.25
###this needs to be adjusted to reflect a set KL distance
statsAll <- as.data.table(statsCCLHS)
temp <- statsAll[,.(y = median(KL)), by = .(Num)]
#temp[,`:=`(y = (y-min(y))/(max(y)-min(y)), Num = as.numeric(as.character(Num)))]
temp[,`:=`(Num = as.numeric(as.character(Num)))]
numPoints <- temp[,.(nPoints = predict(loess(Num ~ y,data = .SD), data.frame(y = KLCutoff)))]
numSinglePts <- numPoints$nPoints[1]

```

# Create examples of summary distributions for manuscript
#
```{r univariate distributions}
s1 <- createLHS(ancVals,100)
s2 <- createLHS(ancVals,500)
s3 <- createLHS(ancVals,2500)
s4 <- createLHS(ancVals,5000)

temp <- plotUniDists(s1)
t1 <- temp$data
t1[,NumSamples := 100]
t1[,Label := paste0("100 Samples\nKL = ",round(temp$KL, digits = 3))]
temp <- plotUniDists(s2)
t2 <- temp$data
t2[,NumSamples := 500]
t2[,Label := paste0("500 Samples\nKL = ",round(temp$KL, digits = 3))]
temp <- plotUniDists(s3)
t3 <- temp$data
t3[,NumSamples := 2500]
t3[,Label := paste0(" 2500 Samples\nKL = ",round(temp$KL, digits = 3))]
temp <- plotUniDists(s4)
t4 <- temp$data
t4[,NumSamples := 5000]
t4[,Label := paste0(" 5000 Samples\nKL = ",round(temp$KL, digits = 3))]
dat <- rbind(t1,t2,t3,t4)
dat[,Label := as.factor(Label)]
dat[,Label := reorder(Label,NumSamples)]

dat2 <- melt(dat, id.vars = c("Label","ID"), measure.vars = c("Num","SampNum"))
source("./_functions/doc_theme_pem.R")
fillspace <- ggplot(dat2, aes(x = ID, y = value, colour = variable, size = variable)) +
  geom_line() +
  facet_wrap(.~Label)+
  ##theme_pem_facet()+
  scale_color_manual(name = "Source", values = c("black","grey"), labels = c("Map Area","cLHS Sample")) +
  scale_size_manual(name = "Source",values = c(3.2,0.8),labels = c("Actual Distribution","cLHS Sample")) +
  scale_alpha(range = c(0.4, 0.8))+
  labs(x = "Multivariate Bins", y = "Bin ratio of total map area")
fillspace
  finalise_plot (fillspace, "./figures/samplestofillbins.png",
                          width_pixels=640,
                          height_pixels=450)
#dev.off()
```


### Simulation 2: Transect Sampling

Now instead of choosing single points, we use the cclhs or lhs to pick centre points, and then create a triangular transect around it, which we then sample from (either using a second clhs or just regular spacing). The below chunk simulates 3 transect scenarios: non cost-constrained clhs, using clhs to sample from transects, and cost-constrained clhs using both clhs and regular spacing for transect sampling. Note that this chunk takes a while to run and will use all your CPU resources. The time limiting part is not the clhs function, but rather extracting data from the raster stack once each transect has been created (even using Velox this takes a while).

```{r set up transect simulations}
ancDatLL_cost <- stack(ancDatLL,acostLL)
fullSet <- sampleRegular(ancDatLL, size = 200000,sp = T) 
X <- st_as_sf(fullSet) ##This line takes a while, but I'm not sure of an alternative
X <- X[!is.na(X$X25m_DAH_3Class),]
# 
# fullSL <- sampleRegular(ancDatSL, size = 100000,GDAL = T)
# fullSL <- fullSL[complete.cases(fullSL),]

fullSet <- sampleRegular(ancDatLL_cost, size = 200000,sp = T)
Xcost <- st_as_sf(fullSet)
Xcost <- Xcost[!is.na(Xcost$X25m_DAH_3Class),]
Xcost <- Xcost[!is.infinite(Xcost$acostLL),]

##stand level
fullSet <- sampleRegular(ancDatSL_cost, size = 500000,sp = T)
Xcost_SL <- st_as_sf(fullSet)
Xcost_SL <- Xcost_SL[!is.na(Xcost_SL$twi),]
```



```{r sampling functions}
##create cclhs transects, sample using lhs
transectsPerSlice <- 5

createCCLHS_transect <- function(X,nSlices){
  xMat <- st_drop_geometry(X)
  currpoints <- NULL
  pointsPerSlice <- transectsPerSlice
  datOut <- foreach(snum = 1:nSlices,.combine = rbind) %do% {
    cat(".")
    temp <- clhs(xMat,size = pointsPerSlice * snum,must.include = currpoints,
                 cost = ncol(xMat),iter = 5000,use.cpp = T,simple = F)
    currpoints <- temp$index_samples
    dat <- X[temp$index_samples,]
    small <- foreach(j = 1:nrow(dat),.combine = rbind) %do% {
      tri <- Tri_build(id = j, x = st_coordinates(dat[j,])[1],y = st_coordinates(dat[j,])[2])
      tri <- st_buffer(tri, dist = 5)
      st_crs(tri) <- 3005
      tempDat <- exact_extract(ancDatSL,tri)[[1]]
      tempDat[,-c(8,9)]
    }
    data.table(SliceNum = snum, KL = suppressMessages(KL_stat_noextra(sample = small, ancSmall = ancSmall)),
               BinFill = binfill_clhs(sample = small, ancSmall = ancSmall))
  }
  
  return(datOut)
}

createCCLHS_transect_paired <- function(X,nSlices){
  xMat <- st_drop_geometry(X)
  currpoints <- NULL
  pointsPerSlice <- transectsPerSlice
  datOut <- foreach(snum = 1:nSlices,.combine = rbind) %do% {
    cat(".")
    temp <- clhs(xMat,size = pointsPerSlice * snum,must.include = currpoints,
                 cost = ncol(xMat),iter = 5000,use.cpp = T,simple = F)
    currpoints <- temp$index_samples
    dat <- X[temp$index_samples,]
    small <- foreach(j = 1:nrow(dat),.combine = rbind) %do% {
      tri <- Tri_build(id = j, x = st_coordinates(dat[j,])[1],y = st_coordinates(dat[j,])[2])
      tri <- st_buffer(tri, dist = 5)
      st_crs(tri) <- 3005
      tri2 <- rotFeature(tri,dat$geometry[j],45)
      tri2$geometry <- tri2$geometry + matrix(data = c(0, 400), ncol = 2)
      st_crs(tri2$geometry) <- 3005
      tri <- st_union(tri$g,tri2$geometry)
      tempDat <- exact_extract(ancDatSL,tri)[[1]]
      tempDat[,-c(8,9)]
    }
    data.table(SliceNum = snum, KL = suppressMessages(KL_stat_noextra(sample = small, ancSmall = ancSmall)),
               BinFill = binfill_clhs(sample = small, ancSmall = ancSmall))
  }
  
  return(datOut)
}

maxSlices <- 12
transectSim_single <- foreach(i = 1:10, .combine = rbind) %dopar% {
  temp <- createCCLHS_transect(Xcost,maxSlices)
  temp[,SimNum := i]
  temp
}
transectSim_single[,Type := "Single Transect"]

transectSim_paired <- foreach(i = 1:10, .combine = rbind) %dopar% {
  temp <- createCCLHS_transect_paired(Xcost,maxSlices)
  temp[,SimNum := i]
  temp
}
transectSim_paired[,Type := "Paired Transect"]

allTrans <- rbind(transectSim_single,transectSim_paired)
allTrans[,NumTransects := SliceNum * transectsPerSlice]
allTrans[,Type := as.factor(Type)]
allTrans[,NumTransects := as.factor(NumTransects)]

ggplot(allTrans, aes(x = NumTransects, y = KL, fill = Type)) +
  geom_boxplot()

```

## Calculate number of sites to fill

```{r num points}
KLCutoff <- 1.3
statsAll2 <- as.data.table(allTrans)
temp <- statsAll2[,.(y = median(KL)), by = .(Type,SliceNum)]
temp[,`:=`(SliceNum = as.numeric(as.character(SliceNum)))]
numTrans <- temp[,.(nSlice = predict(loess(SliceNum ~ y,data = .SD), data.frame(y = KLCutoff))), by = .(Type)]

knitr::kable(numTrans, digits = 2, caption = paste0("Number of slices for KL divergence = ",KLCutoff))

transSingleNum <- 10
transPairedNum <- 6
```

## Test filling of real data. Add by slice

```{r}
library(velox)
ancDatVL <- velox(ancDatSL)

s1Dat <- fread("./InputData/s1_clean_pts_all_fnf.csv")
numTrans <- unique(s1Dat[,.(slice,transect_id)])
numTrans <- numTrans[,.(Num = .N), by = .(slice)]
s1Dat <- s1Dat[,.(x,y,ID,mapunit1, slice)]
setnames(s1Dat, old = "mapunit1", new = "Unit")
s1Points <- st_as_sf(s1Dat,coords = c("x","y"), crs = 3005)
totalSlices <- max(s1Points$slice)

##all slice combinations
datAll <- foreach(nslice = 1:totalSlices, .combine = rbind) %do% {
  cat(".")
  allComb <- combn(1:totalSlices,m = nslice)
  out <- foreach(i = 1:ncol(allComb), .combine = rbind) %do% {
    sample <- s1Points[s1Points$slice %in% allComb[,i],]
    samp2 <- as.data.table(ancDatVL$extract_points(sample)[,1:7])
    data.table(SliceNum = nslice,Iter = i, KL = suppressMessages(KL_stat_noextra(sample = samp2, ancSmall = ancSmall)), BinFill = binfill_clhs(sample = samp2, ancSmall = ancSmall))
  }
  out
}

ggplot(datAll,aes(x = SliceNum, y = KL, group = SliceNum)) +
  geom_boxplot()
#dev.off()
```

## Time to fill variable space

Instead of just calculating the time cost for different numbers of transects, we can use the TSP to estimate the cost of using each sampling method to fill the variable space. Thus, here we use the number of transects needed to fill the variable space as calculated above, and use the TSP to estimate the cost of each method.

```{r num}
numSims <- 5

## cost_constrained clhs
timeCCLHS <- function(X,n){
  xMat <- st_drop_geometry(X)
  temp <- clhs(xMat,size = n,cost = ncol(xMat),iter = 5000,use.cpp = T,simple = F)
  return(temp)
}

X <- Xcost
num <- 20
templhs <- timeCCLHS(X,num)
dat <- X[templhs$index_samples,]
temp <- calcCost_clhs(dat,templhs$final_obj_continuous,45L,3L)

num <- 100
templhs <- timeCCLHS(X,num)
dat <- X[templhs$index_samples,]
calcCost_clhs(dat,templhs$final_obj_continuous,2L,20L)

collectTime2 <- function(X,num, nReps = numSims, plotTime = 45L, minNum = 3L){
  pkgs <- c("Rcpp","LaplacesDemon", "foreach","sf",
          "raster","LearnGeom","dplyr","reticulate","gdistance","scales","clhs")
  # .packages = pkgs,
  #                   .export = c("start_sf","trSmall", "calcCost_clhs")
  test_res <- foreach(n = 1:nReps, .combine = rbind) %dopar% { #
                      #reticulate::source_python("mTSP.py")
                      templhs <- timeCCLHS(X,num)
                      dat <- X[templhs$index_samples,]
                      if(nrow(dat) > 1){
                        time <- calcCost_clhs(dat,templhs$final_obj_continuous,plotTime,minNum)
                        data.table(Rep = n, Cost = time$totTime, NumDays = time$nDays)
                      }else{
                        NULL
                      }
                      
                    }
  return(test_res)
}

transSingleNum <- 10
transPairedNum <- 6
transTime <- collectTime2(Xcost,as.integer((transSingleNum * 5)))
transTime$SType <- "Single Transect"
pairedTime <- collectTime2(Xcost,as.integer((transPairedNum * 5)))
pairedTime$SType <- "Paired Transect"
################################################################
##single points
numSinglePts <- 1700
numNeeded <- 100L
numTimes <- numSinglePts/numNeeded
singleTime <- collectTime2(Xcost_SL,num = numNeeded,nReps = numSims, plotTime = 2L,minNum = 10L)
singleTime[,NumDays := NumDays*numTimes]
singleTime$SType <- "Single Points"

allStats <- rbind(singleTime,transTime,pairedTime)
allStats[,SType := as.factor(SType)]
allStats[,SType := reorder(SType,NumDays, FUN = function(x){-mean(x)})]

ggplot(allStats,aes(x = SType, y = NumDays)) +
  geom_boxplot()

# SPoint <- as.data.table(SPoint)
# SPoint[,TSPNum := rep(c(1:nRep),each = nSims)]
# TSPpoint <- SPoint[,.(Cost = min(Cost)), by = .(TSPNum)]
# TSPpoint[,SType := "Single Point TSP"]
# setnames(TSPpoint, old = "TSPNum", new = "Rep")
# SPoint[,TSPNum := NULL]
# SPoint[,SType := "Single Point CLHS"]

##clhs resampled clhs
# numNeeded <- numTrans[SType == "CCLHS",as.integer(nTrans)]
# clhsTime <- collectTime2(Xcost,timeCCLHS,numNeeded,nSims)
# clhsTime$SType <- "CLHS_Resample"

#

##clhs optimised using TSP
# nSims <- 5
# numNeeded <- numTrans[SType == "cLHS_resample",as.integer(nTrans)]
# tspTime <- foreach(n = 1:(nSims),.combine = rbind) %do% {
#   time <- calcCost_tsp(Xcost,numNeeded,15)
#   data.frame(Rep = n, Cost = time)
# }
# tspTime$SType <- "Transects_TSP"
# 
# timeAll <- rbind(TSPpoint,tspTime)#clhsTime,SPoint,clhsregTime,
# timeAll <- as.data.table(timeAll)
# timeAll[,Cost := Cost/480]

# 
# ggplot(timeAll, aes(x = SType, y = Cost))+
#   geom_boxplot()+
#     scale_y_continuous(name = "Number of 8-hour days")+
#   labs(x = "Number of Sample Sites in Sample Plan") +
#   scale_fill_discrete (name= 'Method', labels = c('cLHS', 'cost-constrained cLHS',  'Travelling Salesman Optimized')) +
#   theme_light()
# ggsave("./Results/CostsTransectvsPointsMethod.jpeg", device = 'jpeg', width = 4, height = 4)
```


## Simulation 3: Sampling Time

The previous chunks investigated how variable space filling changed with different sampling strategies. We now apply the same process, but collect results on the amount of time each plan would take (as determined by the TSP). We use similar sampling plans as in the above chunks: cost-constrained clhs, regular clhs, and TSP (cost-constrained clhs optimised using the TSP). First, we run the simulations as above, with  increasing numbers of transects and multiple replications at each level. We then simulate the cost to fill the variable space to KL = 1 as calculated above, using each of these methods, and present a sample TSP route.

```{r time cost}

collectTime <- function(X,sampleFun, num){
  pkgs <- c("Rcpp","LaplacesDemon", "foreach","sf",
          "raster","LearnGeom","dplyr","reticulate","gdistance","scales","clhs")
  test_res <- foreach(n = num, .combine = rbind, .packages = pkgs,
                    .export = c("start_sf","trSmall", "calcCost_clhs")) %dopar% { #
                      reticulate::source_python("mTSP.py")
                      templhs <- sampleFun(X,n)
                      dat <- X[templhs$index_samples,]
                      if(nrow(dat) > 1){
                        time <- calcCost_clhs(dat,templhs$final_obj_continuous)
                        data.frame(Num = n, Cost = time)
                      }else{
                        NULL
                      }
                      
                    }
  return(test_res)
}

trans <- as.integer(c(5, 10, 15, 20, 25, 30, 35, 40, 45, 50)) ##numbers of transects
trans <- rep(trans,each = 5) ##replications at each level

LHSTime <- collectTime(X,timeLHS,num = trans)
LHSTime$Type = "LHS"
CCLHSTime <- collectTime(Xcost,timeCCLHS,num = trans)
CCLHSTime$Type = "CCLHS"

##function to choose route of least costly point set
calcCost_tsp <- function(Xcost,n,nTry){
  pkgs <- c("Rcpp","sf","data.table","raster","dplyr","reticulate","gdistance","scales","clhs")
  tspTime <- foreach(i = 1:nTry,.combine = rbind, .packages = pkgs,
                     .export = c("start_sf","trSmall")) %dopar% {
    reticulate::source_python("mTSP.py")
    xMat <- st_drop_geometry(Xcost)
    temp <- clhs(xMat,size = n,cost = ncol(xMat),iter = 5000,use.cpp = T,simple = F)
    pnts <- Xcost[temp$index_samples,]
    pnts <- pnts[,1]
    colnames(pnts) <- c("name","geometry")
    st_geometry(pnts) <- "geometry"
    startPnts <- st_as_sf(data.frame(name = "Start",geometry = start_sf))
    pnts <- rbind(pnts, startPnts)
    pnts2 <- as(pnts, "Spatial")
    ## create distance matrix between sample points
    test <- costDistance(trSmall,pnts2,pnts2)
    dMat2 <- as.matrix(test)
    dMat2 <- dMat2*60
    dMat2[is.infinite(dMat2)] <- 1000
    
    ##penalty based on quality of points
    objVals <- max(temp$final_obj_continuous) - temp$final_obj_continuous
    maxTime <- 8L ##hours
    ## time per transect
    plotTime <- 50L ##mins
    temp <- dMat2[1:n,1:n]
    maxDist <- sum(temp[upper.tri(temp)])
    minPen <- maxDist
    maxPen <- maxDist * 4
    objVals <- scales::rescale(objVals, to = c(minPen,maxPen))
    objVals <- as.integer(objVals)
    
    ndays <- as.integer(ceiling(n/5)+1)
    pen = objVals
    
    indStart <- as.integer(rep(n,ndays))
    ##run vehicle routing problem from python script
    ## GCS is global span cost coefficient
    vrp <- py_mTSP(dat = dMat2,num_days = ndays, start = indStart, end = indStart, 
                   max_cost = maxTime*60L, plot_time = plotTime, penalty =  pen, arbDepot = F, GSC = 5L)
    time <- vrp[[2]]
    totTime <- sum(as.numeric(unlist(time)))
    num = length(as.numeric(unlist(vrp[[1]])))-2*length(vrp[[1]])
    data.table(It = i, Cost = totTime, Num = num)
  }
  return(min(tspTime$Cost))
}

# trans <- as.integer(c(5, 10, 15, 20, 25, 30, 35, 40, 45, 50)) ##numbers of transects
# trans <- rep(trans,each = 5) ##replications at each level

#this part takes a while
TSPTime <- foreach(n = trans,.combine = rbind) %do% {
  time <- calcCost_tsp(Xcost,n,8)
  data.frame(Num = n, Cost = time)
}
TSPTime$Type <- "TSP"
timeAll <- rbind(LHSTime,CCLHSTime,TSPTime)
timeAll$Cost <- timeAll$Cost/480 ##covert from minutes to 8-hour days

timeAll$Num <- as.factor(timeAll$Num)
timeAll$Type <- as.factor(timeAll$Type) %>% factor(levels = c( "LHS", "CCLHS", "TSP"))

ggplot(timeAll, aes(x = Num, y = Cost, fill = Type))+
  geom_boxplot()+
  scale_y_continuous(name = "Number of 8-hour days", limits=c(0, 15),breaks=seq(0,15,1))+
  labs(x = "Number of Sample Sites in Sample Plan") +
  expand_limits(x = 0, y = 0)+
  scale_fill_discrete (name= 'Method', labels = c('cLHS', 'cost-constrained cLHS',  'Travelling Salesman Optimized')) +
  theme_light()
ggsave("./Results/CostsbyTransectMethod.jpeg", device = 'jpeg')

# Time_plot <- ggplot(timeAll, aes(x = Num, y = Cost, fill = Type)) +
#   geom_boxplot() +
#   labs(x = "Number of Transects", y = "Time Cost to Sample Sufficient Samples") +
#   scale_fill_discrete(
#     guide = guide_legend(reverse = FALSE),
#     name = 'Sample Method',
#     labels = c(
#       'cLHS- cLHS resampled',
#       'cost cLHS - cLHS resampled',
#       'cost cLHS systematic resampled'
#     )
#   ) +
#   theme_few()
# plot(Bfill_transectPlot2)
```

## Example TSP Route

```{r tsp_example}
n = 25

xMat <- st_drop_geometry(Xcost)
temp <- clhs(xMat,size = n,cost = ncol(xMat),iter = 10000,use.cpp = T,simple = F)
pnts <- Xcost[temp$index_samples,]
pnts <- pnts[,1]
colnames(pnts) <- c("name","geometry")
st_geometry(pnts) <- "geometry"
startPnts <- st_as_sf(data.frame(name = "Start",geometry = start_sf))
pnts <- rbind(pnts, startPnts)
pnts2 <- as(pnts, "Spatial")
## create distance matrix between sample points
test <- costDistance(trSmall,pnts2,pnts2)
dMat2 <- as.matrix(test)
dMat2 <- dMat2*60
dMat2[is.infinite(dMat2)] <- 1000

##penalty based on quality of points
objVals <- max(temp$final_obj_continuous) - temp$final_obj_continuous
maxTime <- 8L ##hours
## time per transect
plotTime <- 50L ##mins
temp <- dMat2[1:n,1:n]
maxDist <- sum(temp[upper.tri(temp)])
minPen <- maxDist
maxPen <- maxDist * 4
objVals <- scales::rescale(objVals, to = c(minPen,maxPen))
objVals <- as.integer(objVals)

ndays <- as.integer(ceiling(n/5)+1)
pen = objVals

indStart <- as.integer(rep(n,ndays))
##run vehicle routing problem from python script
## GCS is global span cost coefficient
vrp <- py_mTSP(dat = dMat2,num_days = ndays, start = indStart, end = indStart, 
               max_cost = maxTime*60L, plot_time = plotTime, penalty =  pen, arbDepot = F, GSC = 100L)

result <- vrp[[1]]

## create spatial paths
paths <- foreach(j = 0:(length(result)-1), .combine = rbind) %do% {
  if(length(result[[as.character(j)]]) > 2){
    cat("Drop site",j,"...\n")
    p1 <- result[[as.character(j)]]+1
    out <- foreach(i = 1:(length(p1)-1), .combine = rbind) %do% {
      temp1 <- pnts[p1[i],]
      temp2 <- pnts[p1[i+1],]
      temp3 <- shortestPath(trSmall,st_coordinates(temp1),
                            st_coordinates(temp2),output = "SpatialLines") %>% st_as_sf()
      temp3$Segment = i
      temp3
    }
    out$DropSite = j
    out
  }
  
}

paths <- st_transform(paths, 3005)
#st_write(paths, dsn = "RoadTSP.gpkg", layer = "Paths", append = T, driver = "GPKG")  

plot(acost2)
plot(pnts,col = "black", add = T)
plot(paths,col = as.factor(paths$DropSite), add = T)

## label points
p2 <- pnts
p2$PID <- seq_along(p2$name)
p2 <- p2[,"PID"]
p2$DropLoc <- NA
p2$Order <- NA
for(i in 0:(length(result)-1)){
  p1 <- result[[as.character(i)]]+1
  p1 <- p1[-1]
  p2$DropLoc[p1] <- i
  p2$Order[p1] <- 1:length(p1)
}
p2 <- st_transform(p2, 3005)


```

## Compare SS transect data to covariate space with DataExplorer
```{r}
# SL <- fread("./InputData/Deception_Transect_SSxcovar_5m.csv")
# LL <- fread("./InputData/Deception_Transect_SSxcovar_LL.csv")
# LL <- LL %>% dplyr::select(mapunit1, mapunit2, '25m_MRVBF_', '25m_Landfo', '25m_DAH_3C')
# 
# DataExplorer::create_report(LL, y = "mapunit1")
```


