---
title: "PEM Sample Analysis"
author: "Kiri Daust"
date: "20/08/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(clhs)
library(sf)
library(raster)
library(sp)
library(gdistance)
library(foreach)
library(data.table)
library(fasterize)
library(reticulate)
library(here)
library(LearnGeom)
library(tidyverse)
library(goftest)
library(velox)
require(ggthemes)

```

The below chunk creates some generic statistical functions

```{r source}
source_python("./mTSP.py")

require(doParallel)
cl <- makePSOCKcluster(detectCores()-2)
registerDoParallel(cl)

fillTest <- function(full, small){
  strata <- apply(
    full, 
    2, 
    function(x) {
      quantile(x, probs = seq(0, 1, 0.01), na.rm = TRUE)
    }
  )
  fillQual <- foreach(var = 1:ncol(full),.combine = cbind) %do% {
    .Call(graphics:::C_BinCount, small[,var], strata[,var], TRUE,TRUE)
  }
  
  return(length(fillQual[fillQual == 0])) 
}

fillTest_Transect <- function(full, small){
  strata <- lapply(
    full$rasterbands, 
    function(x) {
      quantile(x, probs = seq(0, 1, 0.01), na.rm = TRUE)
    }
  )
  fillQual <- foreach(var = 1:ncol(small),.combine = cbind) %do% {
    .Call(graphics:::C_BinCount, small[,var], strata[[var]], TRUE,TRUE)
  }
  
  return(length(fillQual[fillQual == 0])) 
}

x=0; y=0
Tri_build <- function(id, x, y){
  tris <- CreateRegularPolygon(3, c(x,y), 145) # number of sides, center pt and length of sides
  tris <- tris[c(1:3,1),]
  lines <- st_linestring(tris)
  
  g = st_sfc(lines)
  out <- st_sf(g,ID = id)
  #st_set_crs(newproj)
  return(out)
} 
```

Load data and setup covariates

```{r load data}
datLocGit <- here("InputData") ## Data
covLoc <- here("Covariates") ## Too big for git data

### landscape levels covariates
covars <- paste(covLoc, c("25m_DAH_3Class.tif","25m_LandformClass_Default_Seive4.tif",
                          "25m_MRVBF_Classified_IS64Low6Up2.tif","becRaster.tif"), sep = "/")# ,"DEM_25m.tif"
layerNamesLL <- c("DAH","LFC","MRVBF","BEC", "cost") ##need to change this if you change the layers

ancDatLL <- raster::stack(covars)
proj4string(ancDatLL) <- "+init=epsg:3005"
becLayer <- raster(paste0(covLoc,"/becRaster.tif"))
ancDatLL <- mask(ancDatLL,becLayer)


# area <- 5
# if(area == "all"){
#   becLayer[!is.na(becLayer)] <- 1
# }else{
#   becLayer[becLayer != area] <- NA
# }
# ancDatLL <- mask(ancDatLL,becLayer)

# LL_corr <- layerStats(ancDatLL, 'pearson', na.rm=T)
# corr_matrixLL <- LL_corr$'pearson correlation coefficient'

## stand level covariates (about 11 million r)
SLcov <- c("twi.tif","valley_depth_2.tif","tca2.tif","swi_area_mod.tif","tpi.tif")
covars <- paste(covLoc, SLcov, sep = "/")
layerNamesSl <- c("twi","valley","tca2","swi","tpi") ##need to change this if you change the layers
ancDatSL <- raster::stack(covars)
proj4string(ancDatSL) <- "+init=epsg:3005"

# ancDatSL <- mask(ancDatSL,becLayer)

# SL_corr <- layerStats(ancDatSL, 'pearson', na.rm=T)
# corr_matrixSL <- SL_corr$'pearson correlation coefficient'

##combine layers to compare correlation
# ancDatLL2 <- projectRaster(ancDatLL, ancDatSL, method = 'ngb')
# ancDatALL <- stack(ancDatSL, ancDatLL2)
# All_corr <- layerStats(ancDatALL, 'pearson', na.rm=T)
# corr_matrixAll <- All_corr$'pearson correlation coefficient'

## dem for transtion layer
alt <- raster(paste0(covLoc, "/dem.tif"))
proj4string(alt) <- "+init=epsg:3005"

## template raster
allRast <- raster(paste0(covLoc,"/Road_Rast_Small.tif"))    
allRast[allRast == 255] <- NA
allRast <- trim(allRast)

###read in roads
rdsAll <- st_read(paste0(datLocGit,"/road_access_for_cost.gpkg"))
rdsAll <- rdsAll[,"DESCRIPTIO"]
colnames(rdsAll)[1] <- "road_surface"
rdsAll <- as.data.table(rdsAll) %>% st_as_sf()

##Smithers start location
start_sf <- st_read("SmithersStart.gpkg")
start <- as(start_sf, "Spatial")
```

This section creates the transition layer from the slope and clost surface using Tobler's hiking function

```{r create cost surface}
##road speed
rSpd <- fread("road_speed.csv")
rSpd[,Values := speed]
rdsAll <- merge(rdsAll, rSpd, by = "road_surface", all = F)
rdsAll <- rdsAll[,"Values"] %>%
  st_buffer(dist = 35, endCapStyle = "SQUARE", joinStyle = "MITRE") %>%
  st_cast("MULTIPOLYGON")
rdsRast <- fasterize(rdsAll, allRast, field = "Values")

alt2 <- projectRaster(alt,rdsRast,method = 'ngb')
altAll <- merge(rdsRast, alt2)
altSmall <- raster::aggregate(altAll, fact = 3L, fun = min)

## trasition function: if > 500, elevation, so get diff, else speed to select min
trFn <- function(x){
  if(x[1] > 500 & x[2] > 500){
    x[1]-x[2]
  }else{
    min(x[1],x[2])
  }
}

rdIdx <- which(values(altAll) < 100) ##which cells are for roads?
slpIdx <- which(values(altAll) > 500)
adj <- adjacent(altAll, cells = slpIdx, pairs = T, directions = 8)
adj <- adj[!adj[,1] %in% rdIdx ,] ##remove roads inside area
adj <- adj[!adj[,2] %in% rdIdx ,]

tr <- transition(altAll,trFn,directions = 8, symm = F) ##altDiff and speed (km/h)
tr1 <- geoCorrection(tr) ##divided by 25 - slope and conductance (km/h/m)
tr1[adj] <- (3/5)*(6*exp(-3.5*abs(tr1[adj] + 0.08))) ##tobler's hiking function * 3/5 - gives km/h
tr1 <- tr1*1000 ##now roads are correct conductance (h/m), and walking in m/h
tr2 <- geoCorrection(tr1) ##have to geocorrect this part again
tr1[adj] <- tr2[adj] ##tr1 values are now all conductance in h/metre

acost <- accCost(tr1,start)
plot(acost)

##create smaller version for TSP
rdIdx <- which(values(altSmall) < 100) ##which cells are for roads?
slpIdx <- which(values(altSmall) > 500)
adj <- adjacent(altSmall, cells = slpIdx, pairs = T, directions = 8)
adj <- adj[!adj[,1] %in% rdIdx ,] ##remove roads inside area
adj <- adj[!adj[,2] %in% rdIdx ,]

tr <- transition(altSmall,trFn,directions = 8, symm = F) ##altDiff and speed (km/h)
trSmall <- geoCorrection(tr) ##divided by 25 - slope and conductance (km/h/m)
trSmall[adj] <- (3/5)*(6*exp(-3.5*abs(tr1[adj] + 0.08))) ##tobler's hiking function * 3/5 - gives km/h
trSmall <- trSmall*1000 ##now roads are correct conductance (h/m), and walking in m/h
tr2 <- geoCorrection(trSmall) ##have to geocorrect this part again
trSmall[adj] <- tr2[adj] ##tr1 values are now all conductance in h/metre

acost2 <- projectRaster(acost, ancDatSL)
acost2 <- mask(acost2, alt)
#acost2 <- mask(acost2, becLayer)
ancDatSL_cost <- stack(ancDatSL,acost2)
```
### Simulation 1: Single point sampling

For all the following simulations we're testing with random samples, LHS samples, CCLHS samples and sometimes TSP sampling (not for single points).

There are different ways of testing space filling: in the below code, we use KS statistics, a simple bin fill function

```{r setup distribtuion and test functions}
ancDatSL <- raster::stack(covars)
tempID <- ancDatSL$twi
tempID[] <- 1:(nrow(tempID)*ncol(tempID))
names(tempID) <- "CellID"
ancDatSL <- stack(ancDatSL,tempID)
ancDatVL <- velox(ancDatSL) ##create velox object

###setup full LHS distribution for statistics
ancVals <- as.data.table(getValues(ancDatSL))##table of SL data
ancVals <- na.omit(ancVals)
idVals <- ancVals$CellID
ancVals[,CellID := NULL]
#ancBin <- ancVals[,lapply(.SD, function(x){cut(x,breaks = seq(min(x),max(x),length.out = 50),labels = F,include.lowest = T)})] ##put into 10 bins by quantile
 ##save quantiles of full set

##function to test KS statistic of LHS distribution
ks_stat_clhs <- function(sample){
  sampDat <- as.data.table(sample)
  sampDat <- na.omit(sampDat)
  nsamp <- nrow(sampDat)
  
    ancBin <- ancVals[,lapply(.SD, function(x){cut(x,breaks = quantile(x, probs = seq(0,1,length.out = 9)),labels = F,include.lowest = T)})]
    ancBin <- copy(unite(ancBin, LHSVar, sep = "_"))
    ancSmall <- ancBin[,.(Num = .N), by = .(LHSVar)] ##count occurences
    ancSmall <- ancSmall[Num > 99,] ##remove uncommon
    setorder(ancSmall,-Num)
    ancSmall[,ID := seq_along(Num)]
    ancBin[ancSmall, ID := i.ID, on = "LHSVar"]
    ancBin <- na.omit(ancBin)
    fullHist <- hist(ancBin$ID, breaks = as.integer(nsamp/5))
    fullHistBr <- fullHist$breaks
    fullDens <- fullHist$counts/sum(fullHist$counts)
    #plot(ancSmall$Num, type = "l")
    fullBreaks <- ancVals[,lapply(.SD,function(x){quantile(x, probs = seq(0,1,length.out = 9))})]
    ##split into same bins as full data set
  for(i in 1:ncol(sampDat)){
    sampDat[[i]] <- cut(sampDat[[i]],breaks = fullBreaks[[i]], labels = F, include.lowest = T)
  }
  sampBin <- copy(unite(sampDat,sampLHS,sep = "_"))
  sampBin[ancSmall, ID := i.ID, on = c(sampLHS = "LHSVar")]
  sampBin <- na.omit(sampBin)
  smallHist <- hist(sampBin$ID, breaks = fullHistBr, plot = F)
  smallDens <- smallHist$counts/sum(smallHist$counts)
  tempDat <- data.table(ID = 1:length(smallDens), Sample = smallDens, Full = fullDens)
  tempDat <- melt(tempDat, id.vars = "ID")
  tempDat[,variable := as.factor(variable)]
  # print(ggplot(tempDat, aes(x = ID, y = value, colour = variable))+
  #   geom_bar(stat = "identity", position = "dodge"))
  # tempMat <- rbind(fullDens,smallDens)
  # return(KL(tempMat))
  diff <- abs(cumsum(fullDens) - cumsum(smallDens))
  return(max(diff))
  # sampBin <- sampBin[,.(SampNum = .N), by = .(sampLHS)]##count
  # #merge to compare
  # fullDat <- copy(ancSmall)
  # fullDat[sampBin, SampNum := i.SampNum, on = c(LHSVar = "sampLHS")]
  # fullDat[is.na(SampNum), SampNum := 0]
  # fullDat[,SampNum := SampNum/sum(SampNum)]
  # fullDat[,Num := Num/sum(Num)]
  # # fullDat[,MAvg := frollmean(SampNum, n = 15, align = "left")]
  # # fullDat <- na.omit(fullDat)
  # # fullDat[,MAvg := MAvg/sum(MAvg)]
  # tempMat <- rbind(fullDat$Num,fullDat$SampNum)
  # fullDat <- melt(fullDat, id.vars = c("LHSVar","ID"))
  # print(ggplot(fullDat, aes(x = ID, y = value, col = variable))+
  #   geom_line()+
  #   ggtitle(type))
  # kl2 <- KL(tempMat)
  # return(sum(kl2))
}

{
  sampDat <- as.data.table(rand)
  ##split into same bins as full data set
  for(i in 1:ncol(sampDat)){
    sampDat[[i]] <- cut(sampDat[[i]],breaks = ancQuants[[i]], labels = F, include.lowest = T)
  }
  sampBin <- copy(unite(sampDat,sampLHS,sep = ""))
  sampBin[,sampLHS := as.numeric(sampLHS)]
  sampBin[ancSmall, ID := i.ID, on = c(sampLHS = "LHSVar")]
  sampBin <- na.omit(sampBin)
  smallHist <- hist(sampBin$ID, breaks = fullBreaks)
  smallDens <- smallHist$counts/sum(smallHist$counts)
  sum(abs(fullDens - smallDens))
  
  sampBin <- sampBin[,.(SampNum = .N), by = .(sampLHS)]##count
  ##merge to compare
  fullDat <- copy(ancSmall)
  fullDat[sampBin, SampNum := i.SampNum, on = c(LHSVar = "sampLHS")]
  fullDat[is.na(SampNum), SampNum := 0]
  fullDat[,SampNum := SampNum/sum(SampNum)]
  fullDat[,Num := Num/sum(Num)]
  fullDat[,MAvg := frollmean(SampNum, n = 15, align = "left")]
  fullDat <- na.omit(fullDat)
  fullDat[,MAvg := MAvg/sum(MAvg)]
  tempMat <- rbind(fullDat$Num,fullDat$MAvg)
  kl2 <- KL(tempMat)
  fullDat[,SampNum := NULL]
  fullDat[,LHSID := seq_along(Num)]
  fullDat <- melt(fullDat, id.vars = c("LHSID","LHSVar"))
  ggplot(fullDat, aes(x = LHSID, y = value, col = variable))+
    geom_line()

}

##similar to above but slightly different implimentation
ks_stat_clhs_v2 <- function(sample,fullDat,fullBreaks){
  sampDat <- as.data.table(sample)
  ##split into same bins as full data set
  for(i in 1:ncol(sampDat)){
    sampDat[[i]] <- cut(sampDat[[i]],breaks = fullBreaks[[i]], labels = F, include.lowest = T)
  }
  sampBin <- unite(sampDat,sampLHS,sep = "")
  sampBin[,sampLHS := as.numeric(sampLHS)]
  sampBin <- sampBin[,.(SampNum = .N), by = .(sampLHS)]##count
  ##merge to compare
  fullDat[sampBin, SampNum := i.SampNum, on = c(LHSVar = "sampLHS")]
  fullDat[is.na(SampNum), SampNum := 0]
  fullDat[,SampNum := SampNum/sum(SampNum)]
  fullDat[,Num := Num/sum(Num)]
  fullDat[,LHSID := seq_along(Num)]
  fullDat[,Diff := Num - SampNum]
  return(sum(abs(fullDat$Diff)))
}

```


```{r single point}
##single points clhs sampled
allSLDat <- as.data.frame(getValues(ancDatSL_cost))##table of SL data
#allSLDat[,CellID := seq_along(allSLDat$twi)]
allSLDat <- na.omit(allSLDat)
allSLDat <- allSLDat[!is.infinite(allSLDat$layer),]


##functions to create sampling setups
createRandom <- function(X,n){
  small <- sample(1:nrow(X),size = n, replace = F)
  return(X[small,])
}

createLHS <- function(X,n){
  temp <- clhs(X,size = n, cost = NULL, use.cpp = T, iter = 10000, simple = F)
  small <- as.matrix(temp$sampled_data)
  return(small)
}

createCCLHS <- function(X,n){
  temp <- clhs(X,size = n,cost = ncol(X),use.cpp = T, iter = 10000,simple = F)
  small <- as.matrix(temp$sampled_data)
  return(small)
}

##collecting stats with different space filling tests
collectStats <- function(X,sampleFun,nums,testFun1 = ks_stat_clhs,testFun2 = fillTest){
  test_res <- foreach(n = nums, .combine = rbind, 
                    .packages = c("Rcpp","philentropy","foreach","clhs","data.table","dplyr","tidyr"),
                    .export = c("ancVals")) %dopar% {
                      smallSet <- sampleFun(as.data.frame(X),n)
                      res1 <- testFun1(sample = smallSet)
                      res2 <- testFun2(X[,1:5], smallSet)
                      data.frame(Num = n,KS = res1, fill = res2)
                    }
  return(test_res)
}

nums <- round(seq(10, 500, length.out = 8))
nums <- rep(nums, each = 8)
statsRand <- collectStats(allSLDat[,-ncol(allSLDat)],createRandom,nums)
statsRand$SType <- "Random"
statsLHS <- collectStats(allSLDat[,-ncol(allSLDat)],createLHS, nums)
statsLHS$SType <- "LHS"
statsCCLHS <- collectStats(allSLDat,createCCLHS, nums)
statsCCLHS$SType <- "CCLHS"

n = 200
out <- foreach(i = 1:5, .combine = rbind) %do% {
  sample <- createRandom(ancVals,n)
  randOut <- ks_stat_clhs(sample,type = "random")
  sample <- createLHS(ancVals,n)
  lhsOut <- ks_stat_clhs(sample, type = "LHS")
  data.table(it = i, Random = randOut, LHS = lhsOut)
}

statsAll <- rbind(statsRand,statsLHS)
statsAll$Num <- as.factor(statsAll$Num)
statsAll$SType <- as.factor(statsAll$SType)

ggplot(statsAll, aes(x = Num, y = KS, fill = SType))+
  geom_boxplot()
ggplot(statsAll, aes(x = Num, y = fill, fill = SType))+
  geom_boxplot()

# statsAll <- as.data.table(statsAll)
# statsAll[, Norm := (KS-min(KS))/(max(KS)-min(KS))]
# statsAll[,Norm := 1-Norm]
# KSplot <- ggplot(statsAll, aes(x = Num, y = Norm, fill = SType))+
#   geom_boxplot() +
#   labs(x = "Minimum Number of Training Points", y= "Kolmogorov-Smirnov statistic")+ 
#   scale_fill_discrete(guide = guide_legend(reverse=TRUE), name='Sample Method', labels = c('cost cLHS', 'cLHS', 'random'))+
#   theme_few()
# plot(KSplot)
# ggsave("./Results/K-S Plot.jpeg", device = 'jpeg')
# 
# binfillplot <- ggplot(statsAll, aes(x = Num, y = fill, fill = SType))+
#   geom_boxplot() +
#   labs(x = "Minimum Number of Training Points", y= "Bins remaining to be filled")+ 
#   scale_fill_discrete(guide = guide_legend(reverse=TRUE), name='Sample Method', labels = c('cost cLHS', 'cLHS', 'random'))+
#   theme_few()
# plot(binfillplot)
# ggsave("./Results/Bin fill Plot.jpeg", device = 'jpeg')
```

```{r num single points}
statsAll <- as.data.table(statsAll)
temp <- statsAll[,.(y = median(KS)), by = .(SType,Num)]
temp[,`:=`(y = (y-min(y))/(max(y)-min(y)), Num = as.numeric(as.character(Num)))]
numPoints <- temp[,.(nPoints = predict(loess(Num ~ y,data = .SD), data.frame(y = 0.1))), by = .(SType)]

knitr::kable(numPoints, digits = 2, caption = "Number of points need for 90% of maximum space filling")
```

### Simulation 2: Transect Sampling

Now instead of choosing single points, we use the cclhs or lhs to pick centre points, and then create a triangular transect around it, which we then sample from (either using a second clhs or just regular spacing). The below chunk simulates 3 transect scenarios: non cost-constrained clhs, using clhs to sample from transects, and cost-constrained clhs using both clhs and regular spacing for transect sampling. Note that this chunk takes a while to run and will use all your CPU resources. The time limiting part is not the clhs function, but rather extracting data from the raster stack once each transect has been created (even using Velox this takes a while).

```{r set up transect simulations}
ancDatLL_cost <- stack(ancDatLL,acost2)
fullSet <- sampleRegular(ancDatLL, size = 100000,sp = T) 
X <- st_as_sf(fullSet) ##This line takes a while, but I'm not sure of an alternative
X <- X[!is.na(X$X25m_DAH_3Class),]
# 
# fullSL <- sampleRegular(ancDatSL, size = 100000,GDAL = T)
# fullSL <- fullSL[complete.cases(fullSL),]

fullSet <- sampleRegular(ancDatLL_cost, size = 100000,sp = T)
Xcost <- st_as_sf(fullSet)
Xcost <- Xcost[!is.na(Xcost$X25m_DAH_3Class),]
Xcost <- Xcost[!is.infinite(Xcost$layer),]
```

```{r sampling functions}
###create LHS transect sample, use clhs to sample transects
createLHS <- function(X,n,ancDat){
  xMat <- st_drop_geometry(X)
  temp <- clhs(xMat,size = n,cost = NULL,iter = 10000,use.cpp = T,simple = F)
  dat <- X[temp$index_samples,]
  small <- foreach(j = 1:nrow(dat),.combine = c) %do% {
    tri <- Tri_build(id = j, x = st_coordinates(dat[j,])[1],y = st_coordinates(dat[j,])[2])
    tempDat <- ancDat$extract(tri,df = T)
    tempDat[,ncol(tempDat)]
  }
  idxs <- which(idVals %in% small)
  sl_lhs <- clhs(ancVals, size = 15*n,possible.sample = idxs,iter = 10000,use.cpp = T,simple = F)
  temp <- sl_lhs$sampled_data
  return(temp)
}

##create cclhs sample, extract points @ 50m intervals
createCCLHS_30m <- function(X,n,ancDat){
  xMat <- st_drop_geometry(X)
  temp <- clhs(xMat,size = n,cost = ncol(xMat),iter = 10000,use.cpp = T,simple = F)
  dat <- X[temp$index_samples,]
  small <- foreach(j = 1:nrow(dat),.combine = rbind) %do% {
    tri <- Tri_build(id = j, x = st_coordinates(dat[j,])[1],y = st_coordinates(dat[j,])[2])
    tPs <- st_sample(tri, size = 15, type = "regular")
    temp <- suppressWarnings(st_sf(data.frame(id = j, geometry = tPs)) %>%
       st_cast("POINT"))
    temp
    
  }
  small <- st_as_sf(small)
  temp <- ancDat$extract_points(small)
  temp <- temp[!is.na(temp[,1]),]
  temp <- temp[,-ncol(temp)]
  return(temp)
}

##create cclhs transects, sample using lhs
createCCLHS <- function(X,n,ancDat){
  xMat <- st_drop_geometry(X)
  temp <- clhs(xMat,size = n,cost = ncol(xMat),iter = 10000,use.cpp = T,simple = F)
  dat <- X[temp$index_samples,]
  small <- foreach(j = 1:nrow(dat),.combine = c) %do% {
    tri <- Tri_build(id = j, x = st_coordinates(dat[j,])[1],y = st_coordinates(dat[j,])[2])
    tempDat <- ancDat$extract(tri,df = T)
    tempDat[,ncol(tempDat)]
  }
  idxs <- which(idVals %in% small)
  sl_lhs <- clhs(ancVals, size = 15*n,possible.sample = idxs,iter = 10000,use.cpp = T,simple = F)
  temp <- sl_lhs$sampled_data
  return(temp)
}

##function to run simulations
collectStats <- function(X,sampleFun, nums,ancDat, testFun1 = ks_stat_clhs_v2,testFun2 = fillTest_Transect){
  pkgs <- c("Rcpp","philentropy", "foreach","sf",
          "raster","LearnGeom","dplyr","reticulate","gdistance","scales","velox","clhs","data.table","tidyr")
  test_res <- foreach(n = nums, .combine = rbind, .packages = pkgs,
                    .export = c("Tri_build","ancQuants","ancSmall","idVals","ancVals")) %dopar% { #
                      #reticulate::source_python("mTSP.py")
                      smallSet <- sampleFun(X,n,ancDat)
                      smallSet <- smallSet[complete.cases(smallSet),]
                      if(nrow(smallSet) > 1){
                        res1 <- testFun1(sample = smallSet,fullDat = ancSmall,fullBreaks = ancQuants)
                        res2 <- testFun2(ancDat,smallSet)
                        data.frame(Num = n,KS = res1, fillQual = res2)
                      }else{
                        NULL
                      }
                      
                    }
  return(test_res)
}
```

```{r run simulations}
nums <- round(seq(5, 70, length.out = 8)) ##set sequence of transect number
nums <- rep(nums, each = 8) ##replicates of each number (should be at least 5)

statsLHS_trans <- collectStats(X,createLHS,nums,ancDat = ancDatVL)
statsLHS_trans$SType = "LHS"
#fwrite(statsLHS_trans, "LHSNumSave.csv")
statsCCLHS_trans <- collectStats(Xcost,createCCLHS,nums, ancDat = ancDatVL)
statsCCLHS_trans$SType <- "CCLHS"
#fwrite(statsCCLHS_trans, "CCLHSNumSave.csv")

statsCCLHS_30m <- collectStats(Xcost, createCCLHS_30m, nums, ancDat = ancDatVL)
statsCCLHS_30m$SType <- "CCLHSReg"

##plot
statsAll <- rbind(statsLHS_trans, statsCCLHS_trans,statsCCLHS_30m)
statsAll$Num <- as.factor(statsAll$Num)
statsAll$SType <- as.factor(statsAll$SType)
ggplot(statsAll, aes(x = Num, y = KS, fill = SType))+
  geom_boxplot()
ggplot(statsAll, aes(x = Num, y = fillQual, fill = SType))+
  geom_boxplot()

fwrite(statsAll,"statsAll_save.csv")

# KS_transectPlot <- ggplot(statsAll, aes(x = Num, y = KS, fill = SType))+
#   geom_boxplot() +
#   labs(x = "Number of Transects", y= "Kolmogorov-Smirnov statistic (0 = identical)")+
#   scale_fill_discrete(guide = guide_legend(reverse=FALSE), name='Sample Method', labels = c('cLHS- cLHS resampled', 'cost cLHS - systematic resampled', 'cost cLHS - cLHS resampled'))+
#   theme_few()
# plot(KS_transectPlot)
# #ggsave("./Results/KS_transect_plot.jpeg", device = 'jpeg')
# 
# Bfill_transectPlot <- ggplot(statsAll, aes(x = Num, y = fillQual, fill = SType))+
#   geom_boxplot() +
#   labs(x = "Number of Transects", y= "Bins remaining to be filled")+
#   scale_fill_discrete(guide = guide_legend(reverse=FALSE), name='Sample Method', labels = c('cLHS- cLHS resampled', 'cost cLHS - systematic resampled', 'cost cLHS - cLHS resampled'))+
#   theme_few()
# plot(Bfill_transectPlot)
#ggsave("./Results/KS_transectPlot.jpeg", device = 'jpeg')

# smallSet <- as.matrix(smallSet)
# test <- cramer.test(x = fullSL,y = smallSet, just.statistic = T)
```

## Calculate number of sites to fill

```{r num points}
statsAll <- as.data.table(statsAll)
temp <- statsAll[,.(y = median(KS)), by = .(SType,Num)]
temp[,`:=`(y = (y-min(y))/(max(y)-min(y)), Num = as.numeric(as.character(Num)))]
numTrans <- temp[,.(nTrans = predict(loess(Num ~ y,data = .SD), data.frame(y = 0.1))), by = .(SType)]

knitr::kable(numTrans, digits = 2, caption = "Number of transects need for 90% of maximum space filling")

# calcNumTrans <- function(tempDat){
#   temp <- tempDat[,.(y = median(fillQual)), by = .(Num)]
#   temp[,`:=`(y = (y-min(y))/(max(y)-min(y)), Num = as.numeric(as.character(Num)))]
#   l2 <- loess(Num ~ y, data = temp)
#   numTrans <- predict(l2,data.frame(y = 0.15))
#   return(numTrans)
# }
# 
# statsAll <- as.data.table(statsAll)
# numTrans <- statsAll[,.(Num90 = calcNumTrans(.SD)), by = .(SType)]
```

## Number of TSP iterations needed

```{r}
calcCost <- function(Xcost,n,nTry){
  pkgs <- c("Rcpp","sf","data.table","raster","dplyr","reticulate","gdistance","scales","clhs")
  tspTime <- foreach(i = 1:nTry,.combine = rbind, .packages = pkgs,
                     .export = c("start_sf","trSmall")) %dopar% {
    reticulate::source_python("mTSP.py")
    xMat <- st_drop_geometry(Xcost)
    temp <- clhs(xMat,size = n,cost = ncol(xMat),iter = 5000,use.cpp = T,simple = F)
    pnts <- Xcost[temp$index_samples,]
    pnts <- pnts[,1]
    colnames(pnts) <- c("name","geometry")
    st_geometry(pnts) <- "geometry"
    startPnts <- st_as_sf(data.frame(name = "Start",geometry = start_sf))
    pnts <- rbind(pnts, startPnts)
    pnts2 <- as(pnts, "Spatial")
    ## create distance matrix between sample points
    test <- costDistance(trSmall,pnts2,pnts2)
    dMat2 <- as.matrix(test)
    dMat2 <- dMat2*60
    dMat2[is.infinite(dMat2)] <- 1000
    
    ##penalty based on quality of points
    objVals <- max(temp$final_obj_continuous) - temp$final_obj_continuous
    maxTime <- 8L ##hours
    ## time per transect
    plotTime <- 50L ##mins
    temp <- dMat2[1:n,1:n]
    maxDist <- sum(temp[upper.tri(temp)])
    minPen <- maxDist
    maxPen <- maxDist * 4
    objVals <- scales::rescale(objVals, to = c(minPen,maxPen))
    objVals <- as.integer(objVals)
    
    ndays <- as.integer(ceiling(n/5)+1)
    pen = objVals
    
    indStart <- as.integer(rep(n,ndays))
    ##run vehicle routing problem from python script
    ## GCS is global span cost coefficient
    vrp <- py_mTSP(dat = dMat2,num_days = ndays, start = indStart, end = indStart, 
                   max_cost = maxTime*60L, plot_time = plotTime, penalty =  pen, arbDepot = F, GSC = 5L)
    time <- vrp[[2]]
    totTime <- sum(as.numeric(unlist(time)))/480
    num = length(as.numeric(unlist(vrp[[1]])))-2*length(vrp[[1]])
    data.table(It = i, Cost = totTime, Num = num)
  }
  return(tspTime)
}

nums <- seq(5,40,by = 5)
tspTest <- foreach(numRep = nums, .combine = rbind) %do% {
  dat <- calcCost(Xcost, n = 30, nTry = numRep)
  dat[,NumRep := numRep]
  dat
}

tspSum <- tspTest[,.(MinTime = min(Cost)), by = .(NumRep)]
```

## Simulation 3: Sampling Time

The previous chunks investigated how variable space filling changed with different sampling strategies. We now apply the same process, but collect results on the amount of time each plan would take (as determined by the TSP). We use similar sampling plans as in the above chunks: cost-constrained clhs, regular clhs, and TSP (same as cost-constrained clhs, but allowing dropped sites). As above, the simulation runs each scenario with increasing numbers of transects and multiple replications at each level.

```{r time cost}
##create TSP to calculate time
calcCost_clhs <- function(pnts,objVals,plotTime = 50L, minPerDay = 3L){
  n = nrow(pnts)
  p2 <- st_as_sf(pnts)
  pnts <- pnts[,1]
  colnames(pnts) <- c("name","geometry")
  st_geometry(pnts) <- "geometry"
  startPnts <- st_as_sf(data.frame(name = "Start",geometry = start_sf))
  pnts <- rbind(pnts, startPnts)
  pnts2 <- as(pnts, "Spatial")
  
  ## create distance matrix between sample points
  test <- costDistance(trSmall,pnts2,pnts2)
  dMat2 <- as.matrix(test)
  dMat2 <- dMat2*60
  dMat2[is.infinite(dMat2)] <- 1000
  
  ##penalty based on quality of points
  objVals <- max(objVals) - objVals
  
  maxTime <- 8L ##hours
  ## time per transect
  temp <- dMat2[1:n,1:n]
  maxDist <- sum(temp[upper.tri(temp)])
  minPen <- maxDist
  maxPen <- maxDist * 4
  objVals <- scales::rescale(objVals, to = c(minPen,maxPen))
  objVals <- as.integer(objVals)

  ndays <- as.integer(ceiling(n/minPerDay)+1)
  pen = objVals

  indStart <- as.integer(rep(n,ndays))
  ##run vehicle routing problem from python script
  ## GCS is global span cost coefficient
  vrp <- py_mTSP(dat = dMat2,num_days = ndays, start = indStart, end = indStart, 
                 max_cost = maxTime*60L, plot_time = plotTime, penalty =  pen, arbDepot = F, GSC = 5L)
  time <- vrp[[2]]
  totTime <- sum(as.numeric(unlist(time)))
  return(totTime)
}

## regular clhs
timeLHS <- function(X,n){
  xMat <- st_drop_geometry(X)
  temp <- clhs(xMat,size = n,cost = NULL,iter = 5000,use.cpp = T,simple = F)
  return(temp)
}

## cost_constrained clhs
timeCCLHS <- function(X,n){
  xMat <- st_drop_geometry(X)
  temp <- clhs(xMat,size = n,cost = ncol(xMat),iter = 5000,use.cpp = T,simple = F)
  return(temp)
}

##function to run simulations
collectTime <- function(X,sampleFun, num){
  pkgs <- c("Rcpp","LaplacesDemon", "foreach","sf",
          "raster","LearnGeom","dplyr","reticulate","gdistance","scales","clhs")
  test_res <- foreach(n = num, .combine = rbind, .packages = pkgs,
                    .export = c("start_sf","trSmall", "calcCost_clhs")) %dopar% { #
                      reticulate::source_python("mTSP.py")
                      templhs <- sampleFun(X,n)
                      dat <- X[templhs$index_samples,]
                      if(nrow(dat) > 1){
                        time <- calcCost_clhs(dat,templhs$final_obj_continuous)
                        data.frame(Num = n, Cost = time)
                      }else{
                        NULL
                      }
                      
                    }
  return(test_res)
}

trans <- as.integer((seq(5,50,length.out = 5))) ##numbers of transects
trans <- rep(trans,each = 10) ##replications at each level

LHSTime <- collectTime(X,timeLHS,num = trans)
LHSTime$Type = "LHS"
CCLHSTime <- collectTime(Xcost,timeCCLHS,num = trans)
CCLHSTime$Type = "CCLHS"

##function to choose route of least costly point set
calcCost_tsp <- function(Xcost,n,nTry){
  pkgs <- c("Rcpp","sf","data.table","raster","dplyr","reticulate","gdistance","scales","clhs")
  tspTime <- foreach(i = 1:nTry,.combine = rbind, .packages = pkgs,
                     .export = c("start_sf","trSmall")) %dopar% {
    reticulate::source_python("mTSP.py")
    xMat <- st_drop_geometry(Xcost)
    temp <- clhs(xMat,size = n,cost = ncol(xMat),iter = 5000,use.cpp = T,simple = F)
    pnts <- Xcost[temp$index_samples,]
    pnts <- pnts[,1]
    colnames(pnts) <- c("name","geometry")
    st_geometry(pnts) <- "geometry"
    startPnts <- st_as_sf(data.frame(name = "Start",geometry = start_sf))
    pnts <- rbind(pnts, startPnts)
    pnts2 <- as(pnts, "Spatial")
    ## create distance matrix between sample points
    test <- costDistance(trSmall,pnts2,pnts2)
    dMat2 <- as.matrix(test)
    dMat2 <- dMat2*60
    dMat2[is.infinite(dMat2)] <- 1000
    
    ##penalty based on quality of points
    objVals <- max(temp$final_obj_continuous) - temp$final_obj_continuous
    maxTime <- 8L ##hours
    ## time per transect
    plotTime <- 50L ##mins
    temp <- dMat2[1:n,1:n]
    maxDist <- sum(temp[upper.tri(temp)])
    minPen <- maxDist
    maxPen <- maxDist * 4
    objVals <- scales::rescale(objVals, to = c(minPen,maxPen))
    objVals <- as.integer(objVals)
    
    ndays <- as.integer(ceiling(n/5)+1)
    pen = objVals
    
    indStart <- as.integer(rep(n,ndays))
    ##run vehicle routing problem from python script
    ## GCS is global span cost coefficient
    vrp <- py_mTSP(dat = dMat2,num_days = ndays, start = indStart, end = indStart, 
                   max_cost = maxTime*60L, plot_time = plotTime, penalty =  pen, arbDepot = F, GSC = 5L)
    time <- vrp[[2]]
    totTime <- sum(as.numeric(unlist(time)))
    num = length(as.numeric(unlist(vrp[[1]])))-2*length(vrp[[1]])
    data.table(It = i, Cost = totTime, Num = num)
  }
  return(min(tspTime$Cost))
}

trans <- as.integer((seq(5,50,length.out = 5))) ##numbers of transects
trans <- rep(trans,each = 5) ##replications at each level

TSPTime <- foreach(n = trans,.combine = rbind) %do% {
  time <- calcCost_tsp(Xcost,n,15)
  data.frame(Num = n, Cost = time)
}
TSPTime$Type <- "TSP"
timeAll <- rbind(LHSTime,CCLHSTime,TSPTime)
timeAll$Cost <- timeAll$Cost/480 ##covert from minutes to 8-hour days

timeAll$Num <- as.factor(timeAll$Num)
timeAll$Type <- as.factor(timeAll$Type)

ggplot(timeAll, aes(x = Num, y = Cost, fill = Type))+
  geom_boxplot()

# Time_plot <- ggplot(timeAll, aes(x = Num, y = Cost, fill = Type)) +
#   geom_boxplot() +
#   labs(x = "Number of Transects", y = "Time Cost to Sample Sufficient Samples") +
#   scale_fill_discrete(
#     guide = guide_legend(reverse = FALSE),
#     name = 'Sample Method',
#     labels = c(
#       'cLHS- cLHS resampled',
#       'cost cLHS - cLHS resampled',
#       'cost cLHS systematic resampled'
#     )
#   ) +
#   theme_few()
# plot(Bfill_transectPlot2)
# ggsave("./Results/KS_transectPlot2.jpeg", device = 'jpeg')
```

## Time to fill variable space

Instead of just calculating the time cost for different numbers of transects, we can use the TSP to estimate the cost of using each sampling method to fill the variable space. Thus, here we use the number of transects needed to fill the variable space as calculated above, and use the TSP to estimate the cost of each method.

```{r num}
nSims <- 20

collectTime2 <- function(X,sampleFun,num, nReps, plotTime = 50L, minNum = 3L){
  pkgs <- c("Rcpp","LaplacesDemon", "foreach","sf",
          "raster","LearnGeom","dplyr","reticulate","gdistance","scales","clhs")
  test_res <- foreach(n = 1:nReps, .combine = rbind, .packages = pkgs,
                    .export = c("start_sf","trSmall", "calcCost_clhs")) %dopar% { #
                      reticulate::source_python("mTSP.py")
                      templhs <- sampleFun(X,num)
                      dat <- X[templhs$index_samples,]
                      if(nrow(dat) > 1){
                        time <- calcCost_clhs(dat,templhs$final_obj_continuous,plotTime,minNum)
                        data.frame(Rep = n, Cost = time)
                      }else{
                        NULL
                      }
                      
                    }
  return(test_res)
}

##single points clhs sampled
allSLDat <- as.data.frame(getValues(ancDatSL_cost))##table of SL data
#allSLDat[,CellID := seq_along(allSLDat$twi)]
allSLDat <- na.omit(allSLDat)
allSLDat <- allSLDat[!is.infinite(allSLDat$layer),]
numNeeded <- numPoints[SType == "CCLHS",as.integer(nPoints)]
numNeeded = 410

pkgs <- c("Rcpp","LaplacesDemon", "foreach","sf",
        "raster","LearnGeom","dplyr","reticulate","gdistance","scales","clhs")
SPoint <- foreach(n = 1:nSims, .combine = rbind, .packages = pkgs,
                  .export = c("start_sf","trSmall", "calcCost_clhs","allSLDat")) %dopar% { #
                    reticulate::source_python("mTSP.py")
                    templhs <- clhs(allSLDat,size = as.integer(numNeeded/4),cost = ncol(allSLDat),use.cpp = T, iter = 10000,simple = F)
                    tempdat <- templhs$sampled_data
                    cellNums <- as.numeric(rownames(tempdat))
                    dat <- raster::xyFromCell(ancDatSL_cost,cell = cellNums, sp = T)
                    dat <- st_as_sf(dat)
                    dat <- cbind(1:nrow(dat),dat)
                    if(nrow(dat) > 1){
                      time <- calcCost_clhs(dat,templhs$final_obj_continuous,plotTime = 2L,minPerDay = 8L)
                      data.frame(Rep = n, Cost = time*4)
                    }else{
                      NULL
                    }
                    
                  }

SPoint$SType <- "Single Point CLHS"

##clhs resampled clhs
numNeeded <- numTrans[SType == "CCLHS",as.integer(nTrans)]
clhsTime <- collectTime2(Xcost,timeCCLHS,numNeeded,nSims)
clhsTime$SType <- "CLHS_Resample"

##clhs regular resampling
numNeeded <- numTrans[SType == "CCLHSReg",as.integer(nTrans)]
clhsregTime <- collectTime2(Xcost,timeCCLHS,numNeeded,nSims)
clhsregTime$SType <- "CLHS_Regular"

##clhs optimised using TSP
nSims <- 5
numNeeded <- numTrans[SType == "CCLHS",as.integer(nTrans)]
tspTime <- foreach(n = 1:(nSims),.combine = rbind) %do% {
  time <- calcCost_tsp(Xcost,numNeeded,15)
  data.frame(Rep = n, Cost = time)
}
tspTime$SType <- "TSP_Optimised"

timeAll <- rbind(SPoint,clhsTime,clhsregTime,tspTime)
timeAll <- as.data.table(timeAll)
timeAll[,Cost := Cost/480]
timeAll[,SType := as.factor(SType)]

ggplot(timeAll, aes(x = SType, y = Cost))+
  geom_boxplot()
```

## Compare SS transect data to covariate space with DataExplorer
```{r}
SL <- fread("./InputData/Deception_Transect_SSxcovar_5m.csv")
LL <- fread("./InputData/Deception_Transect_SSxcovar_LL.csv")
LL <- LL %>% dplyr::select(mapunit1, mapunit2, '25m_MRVBF_', '25m_Landfo', '25m_DAH_3C')

DataExplorer::create_report(LL, y = "mapunit1")
```


