---
title: "PEM Sample Analysis"
author: "Kiri Daust"
date: "20/08/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(clhs)
library(sf)
library(raster)
library(sp)
library(gdistance)
library(foreach)
library(data.table)
library(fasterize)
library(reticulate)
library(here)
library(LearnGeom)
library(tidyverse)
library(goftest)
library(velox)
require(ggthemes)
require(philentropy)
require(ggthemes)
require(tictoc)
#install.packages("philentropy")

```

The below chunk creates some generic statistical functions

```{r source}
source_python("./mTSP.py")

require(doParallel)
cl <- makePSOCKcluster(detectCores()-2)
registerDoParallel(cl)

fillTest <- function(full, small){
  strata <- full[,lapply(.SD,function(x) {quantile(x, probs = seq(0, 1, 0.005), na.rm = TRUE)})]
  
  fillQual <- foreach(var = 1:ncol(full),.combine = cbind) %do% {
    .Call(graphics:::C_BinCount, small[[var]], strata[[var]], TRUE,TRUE)
  }
  
  return(length(fillQual[fillQual == 0])) 
}

fillTest_Transect <- function(full, small){
  strata <- lapply(
    full$rasterbands, 
    function(x) {
      quantile(x, probs = seq(0, 1, 0.01), na.rm = TRUE)
    }
  )
  fillQual <- foreach(var = 1:ncol(small),.combine = cbind) %do% {
    .Call(graphics:::C_BinCount, small[,var], strata[[var]], TRUE,TRUE)
  }
  
  return(length(fillQual[fillQual == 0])) 
}

x=0; y=0
Tri_build <- function(id, x, y){
  tris <- CreateRegularPolygon(3, c(x,y), 145) # number of sides, center pt and length of sides
  tris <- tris[c(1:3,1),]
  lines <- st_linestring(tris)
  
  g = st_sfc(lines)
  out <- st_sf(g,ID = id)
  #st_set_crs(newproj)
  return(out)
} 
```

Load data and setup covariates

```{r load data}
datLocGit <- here("InputData") ## Data
covLoc <- here("Covariates") ## Too big for git data

### landscape levels covariates
covars <- paste(covLoc, c("25m_DAH_3Class.tif","25m_LandformClass_Default_Seive4.tif",
                          "25m_MRVBF_Classified_IS64Low6Up2.tif","becRaster.tif"), sep = "/")# ,"DEM_25m.tif"
layerNamesLL <- c("DAH","LFC","MRVBF","BEC", "cost") ##need to change this if you change the layers

ancDatLL <- raster::stack(covars)
proj4string(ancDatLL) <- "+init=epsg:3005"
becLayer <- raster(paste0(covLoc,"/becRaster.tif"))
ancDatLL <- mask(ancDatLL,becLayer)


# area <- 5
# if(area == "all"){
#   becLayer[!is.na(becLayer)] <- 1
# }else{
#   becLayer[becLayer != area] <- NA
# }
# ancDatLL <- mask(ancDatLL,becLayer)

# LL_corr <- layerStats(ancDatLL, 'pearson', na.rm=T)
# corr_matrixLL <- LL_corr$'pearson correlation coefficient'

## stand level covariates (about 11 million r)
SLcov <- c("twi.tif","valley_depth_2.tif","tca2.tif","swi_area_mod.tif","tpi.tif")
covars <- paste(covLoc, SLcov, sep = "/")
ancDatSL <- raster::stack(covars)
proj4string(ancDatSL) <- "+init=epsg:3005"
layerNamesSl <- names(ancDatSL) ##need to change this if you change the layers
for(i in layerNamesSl){
  ancDatSL[[i]][ancDatSL[[i]] > quantile(ancDatSL[[i]],0.9, na.rm = T)] <- NA
}

ancVals <- as.data.table(getValues(ancDatSL))##table of SL data
ancVals <- na.omit(ancVals)


# for(i in layerNamesSl){
#   temp <- values(ancDatSL[[i]])
#   hist1 <- hist(temp,breaks = 50, plot = F)
#   dens <- hist1$counts/sum(hist1$counts)
#   mx <- which(dens == max(dens))
#   dens[1:mx] <- 1
#   small <- which(dens < 0.01)
#   cutoff <- hist1$breaks[small[1] + 1]
#   ancDatSL[[i]][ancDatSL[[i]] > cutoff] <- NA
# }

# ancDatSL <- mask(ancDatSL,becLayer)

# SL_corr <- layerStats(ancDatSL, 'pearson', na.rm=T)
# corr_matrixSL <- SL_corr$'pearson correlation coefficient'

##combine layers to compare correlation
# ancDatLL2 <- projectRaster(ancDatLL, ancDatSL, method = 'ngb')
# ancDatALL <- stack(ancDatSL, ancDatLL2)
# All_corr <- layerStats(ancDatALL, 'pearson', na.rm=T)
# corr_matrixAll <- All_corr$'pearson correlation coefficient'

## dem for transtion layer
alt <- raster(paste0(covLoc, "/dem.tif"))
proj4string(alt) <- "+init=epsg:3005"

## template raster
allRast <- raster(paste0(covLoc,"/Road_Rast_Small.tif"))    
allRast[allRast == 255] <- NA
allRast <- trim(allRast)

###read in roads
rdsAll <- st_read(paste0(datLocGit,"/road_access_for_cost.gpkg"))
rdsAll <- rdsAll[,"DESCRIPTIO"]
colnames(rdsAll)[1] <- "road_surface"
rdsAll <- as.data.table(rdsAll) %>% st_as_sf()

##Smithers start location
start_sf <- st_read("SmithersStart.gpkg")
start <- as(start_sf, "Spatial")
```

This section creates the transition layer from the slope and clost surface using Tobler's hiking function

```{r create cost surface}
##road speed
rSpd <- fread("road_speed.csv")
rSpd[,Values := speed]
rdsAll <- merge(rdsAll, rSpd, by = "road_surface", all = F)
rdsAll <- rdsAll[,"Values"] %>%
  st_buffer(dist = 35, endCapStyle = "SQUARE", joinStyle = "MITRE") %>%
  st_cast("MULTIPOLYGON")
rdsRast <- fasterize(rdsAll, allRast, field = "Values")

alt2 <- projectRaster(alt,rdsRast,method = 'ngb')
altAll <- merge(rdsRast, alt2)
altSmall <- raster::aggregate(altAll, fact = 3L, fun = min)

## trasition function: if > 500, elevation, so get diff, else speed to select min
trFn <- function(x){
  if(x[1] > 500 & x[2] > 500){
    x[1]-x[2]
  }else{
    min(x[1],x[2])
  }
}

rdIdx <- which(values(altAll) < 100) ##which cells are for roads?
slpIdx <- which(values(altAll) > 500)
adj <- adjacent(altAll, cells = slpIdx, pairs = T, directions = 8)
adj <- adj[!adj[,1] %in% rdIdx ,] ##remove roads inside area
adj <- adj[!adj[,2] %in% rdIdx ,]

tr <- transition(altAll,trFn,directions = 8, symm = F) ##altDiff and speed (km/h)
tr1 <- geoCorrection(tr) ##divided by 25 - slope and conductance (km/h/m)
tr1[adj] <- (3/5)*(6*exp(-3.5*abs(tr1[adj] + 0.08))) ##tobler's hiking function * 3/5 - gives km/h
tr1 <- tr1*1000 ##now roads are correct conductance (h/m), and walking in m/h
tr2 <- geoCorrection(tr1) ##have to geocorrect this part again
tr1[adj] <- tr2[adj] ##tr1 values are now all conductance in h/metre

acost <- accCost(tr1,start)
plot(acost)

##create smaller version for TSP
rdIdx <- which(values(altSmall) < 100) ##which cells are for roads?
slpIdx <- which(values(altSmall) > 500)
adj <- adjacent(altSmall, cells = slpIdx, pairs = T, directions = 8)
adj <- adj[!adj[,1] %in% rdIdx ,] ##remove roads inside area
adj <- adj[!adj[,2] %in% rdIdx ,]

tr <- transition(altSmall,trFn,directions = 8, symm = F) ##altDiff and speed (km/h)
trSmall <- geoCorrection(tr) ##divided by 25 - slope and conductance (km/h/m)
trSmall[adj] <- (3/5)*(6*exp(-3.5*abs(tr1[adj] + 0.08))) ##tobler's hiking function * 3/5 - gives km/h
trSmall <- trSmall*1000 ##now roads are correct conductance (h/m), and walking in m/h
tr2 <- geoCorrection(trSmall) ##have to geocorrect this part again
trSmall[adj] <- tr2[adj] ##tr1 values are now all conductance in h/metre

acost2 <- projectRaster(acost, ancDatSL)
acost2 <- mask(acost2, alt)
#acost2 <- mask(acost2, becLayer)
ancDatSL_cost <- stack(ancDatSL,acost2)

plot(acost2)
writeRaster(acost, "./Results/acost.tif", format = "GTiff", overwrite = TRUE)
writeRaster(acost2, "./Results/acost2.tif", format = "GTiff", overwrite = TRUE)
# png('./outputs/acost2.png', height=nrow(acost2), width=ncol(acost2)) 
# plot(acost2, maxpixels=ncell(acost2))
# dev.off()

```
### Simulation 1: Single point sampling

For all the following simulations we're testing with random samples, LHS samples, CCLHS samples and sometimes TSP sampling (not for single points).

There are different ways of testing space filling: in the below code, we use KS statistics, a simple bin fill function

```{r setup distribtuion and test functions}
ancDatSL <- raster::stack(covars)
proj4string(ancDatSL) <- "+init=epsg:3005"
layerNamesSl <- names(ancDatSL) ##need to change this if you change the layers
for(i in layerNamesSl){
  ancDatSL[[i]][ancDatSL[[i]] > quantile(ancDatSL[[i]],0.9, na.rm = T)] <- NA
}
tempID <- ancDatSL$twi
tempID[] <- 1:(nrow(tempID)*ncol(tempID))
names(tempID) <- "CellID"
ancDatSL <- stack(ancDatSL,tempID)
ancDatVL <- velox(ancDatSL) ##create velox object

###setup full LHS distribution for statistics
ancVals <- as.data.table(getValues(ancDatSL))##table of SL data
ancVals <- na.omit(ancVals)
idVals <- ancVals$CellID
ancVals[,CellID := NULL]

##KL statistic (this one works best)
xx <- KL_stat_clhs(s1)
KL_stat_clhs <- function(sample){
  sampDat <- as.data.table(sample)
  sampDat <- na.omit(sampDat)
  nsamp <- nrow(sampDat)
  
  ancBin <- ancVals[,lapply(.SD, function(x){cut(x,breaks = seq(min(x),max(x),length.out = 9),labels = F,include.lowest = T)})]
  ancBin <- copy(unite(ancBin, LHSVar, sep = "_"))
  ancSmall <- ancBin[,.(Num = .N), by = .(LHSVar)] ##count occurences
  ancSmall <- ancSmall[Num > 150,] ##remove uncommon
  setorder(ancSmall,-Num)
  ancSmall[,ID := seq_along(Num)]
  ancBin[ancSmall, ID := i.ID, on = "LHSVar"]
  ancBin <- na.omit(ancBin)
  
  fullBreaks <- ancVals[,lapply(.SD,function(x){seq(min(x),max(x),length.out = 9)})]
  ##split into same bins as full data set
  for(i in 1:ncol(sampDat)){
    sampDat[[i]] <- cut(sampDat[[i]],breaks = fullBreaks[[i]], labels = F, include.lowest = T)
  }
  sampBin <- copy(unite(sampDat,sampLHS,sep = "_"))
  sampBin[ancSmall, ID := i.ID, on = c(sampLHS = "LHSVar")]
  sampBin <- na.omit(sampBin)
  sampBin <- sampBin[,.(SampNum = .N), by = .(sampLHS)]##count
  #merge to compare
  fullDat <- copy(ancSmall)
  fullDat[sampBin, SampNum := i.SampNum, on = c(LHSVar = "sampLHS")]
  fullDat[is.na(SampNum), SampNum := 0]
  fullDat[,SampNum := SampNum/sum(SampNum)]
  fullDat[,Num := Num/sum(Num)]
  tempMat <- rbind(fullDat$Num,fullDat$SampNum)

  kl2 <- KL(tempMat) ## return KL divergence
  js2 <- JSD(tempMat) ## returns JS divergenxe
  temp <- t(tempMat)# %>% dplyr::select()
  ks2 <- ks.test(temp[,1], temp[,2])
  ks2p <- ks2$p.value## returns KS statistic and p value
  return(kl2)#kl2, ,ks2p js2, js2, ks2$statistic))
}
#sample = s2
##using KS statistic
KS_stat_clhs <- function(sample){
  sampDat <- as.data.table(sample)
  sampDat <- na.omit(sampDat)
  nsamp <- nrow(sampDat)
  
  ancBin <- ancVals[,lapply(.SD, function(x){cut(x,breaks = seq(min(x),max(x),length.out = 15),labels = F,include.lowest = T)})]
  ancBin <- copy(unite(ancBin, LHSVar, sep = "_"))
  ancSmall <- ancBin[,.(Num = .N), by = .(LHSVar)] ##count occurences
  ancSmall <- ancSmall[Num > 150,] ##remove uncommon
  setorder(ancSmall,-Num)
  ancSmall[,ID := seq_along(Num)]
  ancBin[ancSmall, ID := i.ID, on = "LHSVar"]
  ancBin <- na.omit(ancBin)
  
  fullBreaks <- ancVals[,lapply(.SD,function(x){seq(min(x),max(x),length.out = 15)})]
  ##split into same bins as full data set
  for(i in 1:ncol(sampDat)){
    sampDat[[i]] <- cut(sampDat[[i]],breaks = fullBreaks[[i]], labels = F, include.lowest = T)
  }
  sampBin <- copy(unite(sampDat,sampLHS,sep = "_"))
  sampBin[ancSmall, ID := i.ID, on = c(sampLHS = "LHSVar")]
  sampBin <- na.omit(sampBin)

  sampBin <- sampBin[,.(SampNum = .N), by = .(sampLHS)]##count
  #merge to compare
  fullDat <- copy(ancSmall)
  fullDat[sampBin, SampNum := i.SampNum, on = c(LHSVar = "sampLHS")]
  fullDat[is.na(SampNum), SampNum := 0]
  fullDat[,SampNum := SampNum/sum(SampNum)]
  fullDat[,Num := Num/sum(Num)]
  #fullDat[,Diff := abs(Num - SampNum)]
  t1 <- cumsum(fullDat$SampNum)
  t2 <- cumsum(fullDat$Num)
  return(max(abs(t2-t1)))
  #return(mean(fullDat$Diff))
}
#sample = s1
plotUniDists <- function(sample){
  sampDat <- as.data.table(sample)
  sampDat <- na.omit(sampDat)
  nsamp <- nrow(sampDat)
  
  ancBin <- ancVals[,lapply(.SD, function(x){cut(x,breaks = seq(min(x),max(x),length.out = 9),labels = F,include.lowest = T)})]
  ancBin <- copy(unite(ancBin, LHSVar, sep = "_"))
  ancSmall <- ancBin[,.(Num = .N), by = .(LHSVar)] ##count occurences
  ancSmall <- ancSmall[Num > 150,] ##remove uncommon
  setorder(ancSmall,-Num)
  ancSmall[,ID := seq_along(Num)]
  ancBin[ancSmall, ID := i.ID, on = "LHSVar"]
  ancBin <- na.omit(ancBin)
  
  fullBreaks <- ancVals[,lapply(.SD,function(x){seq(min(x),max(x),length.out = 9)})]
  ##split into same bins as full data set
  for(i in 1:ncol(sampDat)){
    sampDat[[i]] <- cut(sampDat[[i]],breaks = fullBreaks[[i]], labels = F, include.lowest = T)
  }
  sampBin <- copy(unite(sampDat,sampLHS,sep = "_"))
  sampBin[ancSmall, ID := i.ID, on = c(sampLHS = "LHSVar")]
  sampBin <- na.omit(sampBin)
  sampBin <- sampBin[,.(SampNum = .N), by = .(sampLHS)]##count
  #merge to compare
  fullDat <- copy(ancSmall)
  fullDat[sampBin, SampNum := i.SampNum, on = c(LHSVar = "sampLHS")]
  fullDat[is.na(SampNum), SampNum := 0]
  fullDat[,SampNum := SampNum/sum(SampNum)]
  fullDat[,Num := Num/sum(Num)]
  fullDat <- melt(fullDat, id.vars = c("LHSVar","ID"))
  require(ggsci)
  print(ggplot(fullDat, aes(x = ID, y = value, col = variable, group = variable))+
    geom_line(size = .5, position=position_dodge(width=5))+
    scale_size_manual(values=c(2,2))+  
      scale_y_continuous(name = "Bin ratio of total map area") + #, limits=c(0, 8),breaks=seq(0,8,1))
    scale_color_aaas( name='Covariate Bin Distribution', labels = c('map area', 'cLHS point sample'))+
      scale_linetype_manual(values=c("solid", "dotted"))+
    expand_limits(x = 0, y = 0) +
  xlab("Multivariate Bins")+
  theme_light())
#ggsave("./Results/Discretized Multivariate Distribution.jpeg", device = 'jpeg', width = 6, height=4, units = "in")

}

```


```{r sample functions single point}
##single points clhs sampled
allSLDat <- as.data.frame(getValues(ancDatSL_cost))##table of SL data
#allSLDat[,CellID := seq_along(allSLDat$twi)]
allSLDat <- na.omit(allSLDat)
allSLDat <- allSLDat[!is.infinite(allSLDat$layer),]


##functions to create sampling setups
createRandom <- function(X,n){
  small <- sample(1:nrow(X),size = n, replace = F)
  return(X[small,])
}

createLHS <- function(X,n){
  temp <- clhs(X,size = n, cost = NULL, use.cpp = T, iter = 5000, simple = F)
  small <- as.matrix(temp$sampled_data)
  return(small)
}

createCCLHS <- function(X,n){
  temp <- clhs(X,size = n,cost = ncol(X),use.cpp = T, iter = 10000,simple = F)
  small <- as.matrix(temp$sampled_data)
  return(small)
}
X=ancVals; sampleFun=createLHS; nums = nums; testFun1 = KL_stat_clhs; testFun2 = KS_stat_clhs; n=1000
#collecting stats with different space filling tests
collectStats <- function(X,sampleFun,nums,testFun1 = KL_stat_clhs, testFun2 = KS_stat_clhs){
  test_res <- foreach(n = nums, .combine = rbind,
                    .packages = c("Rcpp","philentropy","foreach","clhs","data.table","dplyr","tidyr"),
                    .export = c("ancVals")) %dopar% {
                      smallSet <- sampleFun(X,n)
                      res1 <- testFun1(sample = smallSet)
                      res2 <- testFun2(sample = smallSet)
                      #res2 <- testFun2(full = ancVals, small = smallSet)
                    data.frame(Num = n,JD = res1, KS = res2)
                    }
  return(test_res)
}


```

```{r univariate distributions examples}
s1 <- createLHS(ancVals,1000)
plotUniDists(s1)
# s2 <- createCCLHS(ancVals,45)
# plotUniDists(s2)
#s2 <- createRandom(ancVals,400)

#plotUniDists(s2)
```


```{r run and plot simulations}

#nums <- round(seq(10, 5000, length.out = 10))
nums <- c(100,250, 500, 750, 1000, 1250, 1500, 1750, 2000)
nums <- rep(nums, each = 5)
statsRand <- collectStats(ancVals,createRandom,nums)
statsRand$SType <- "Random"
statsLHS <- collectStats(ancVals,createLHS, nums)
statsLHS$SType <- "LHS"
statsCCLHS <- collectStats(allSLDat,createCCLHS, nums)
statsCCLHS$SType <- "CCLHS"

statsAll <- rbind(statsRand,statsLHS,statsCCLHS)%>% filter(Num < 3200) ###filter to remove some categories after the fact for grtaph
statsAll$Num <- round(statsAll$Num, -2) %>% as.factor()
statsAll$SType <- as.factor(statsAll$SType) %>% factor(levels = c("Random", "LHS", "CCLHS"))

###Graphic of number of points needed to match variable space
statsAll <- statsAll 
ggplot(statsAll, aes(x = Num, y = JD, fill = SType))+
  geom_boxplot()+
   scale_y_continuous(name = "Jensen-Shannon Divergence", limits=c(0, 1),breaks=seq(0,1,.1))+
  #labs(x = "Number of Sample Points") +
   scale_fill_discrete(name='Sample Method', labels = c('random', 'cLHS', 'cost cLHS' ))+
  expand_limits(x = 0, y = 0)+
  geom_hline(yintercept = 1,  linetype = "dashed", color = "red")+
  #xlim(0,3000)+
  theme_light()

ggsave("./Results/JS_Divergence_Points_method.jpeg", device = 'jpeg', width = 6, height=4, units = "in")

statsAll <- statsAll 
ggplot(statsAll, aes(x = Num, y = KS, fill = SType))+
  geom_boxplot()+
   scale_y_continuous(name = "Kolmogorov-Smirnov statistic", limits=c(0, 1),breaks=seq(0,1,.1))+
  #labs(x = "Number of Sample Points") +
   scale_fill_discrete(name='Sample Method', labels = c('random', 'cLHS', 'cost cLHS' ))+
  expand_limits(x = 0, y = 0)+
  geom_hline(yintercept = 1,  linetype = "dashed", color = "red")+
  #xlim(0,3000)+
  theme_light()

ggsave("./Results/KS_Divergence_Points_method.jpeg", device = 'jpeg', width = 6, height=4, units = "in")



# 
# ggplot(statsAll, aes(x = Num, y = Fill, fill = SType))+
#   geom_boxplot()+
#   scale_y_continuous(name = "Covariate bins filled", limits=c(0, 8),breaks=seq(0,8,1))+
#   labs(x = "Number of Sample Points") +
#   expand_limits(x = 0, y = 0)+
#   theme_light()
# 
# ggsave("./Results/CovariateBins_method.jpeg", device = 'jpeg', width = 6, height=4, units = "in")
 # statsAll <- as.data.table(statsAll)
# statsAll[, Norm := (KS-min(KS))/(max(KS)-min(KS))]
# statsAll[,Norm := 1-Norm]
# KSplot <- ggplot(statsAll, aes(x = Num, y = Norm, fill = SType))+
#   geom_boxplot() +
#   labs(x = "Minimum Number of Training Points", y= "Kolmogorov-Smirnov statistic")+ 
#   scale_fill_discrete(guide = guide_legend(reverse=TRUE), name='Sample Method', labels = c('cost cLHS', 'cLHS', 'random'))+
#   theme_few()
# plot(KSplot)
# ggsave("./Results/K-S Plot.jpeg", device = 'jpeg')
# 
# binfillplot <- ggplot(statsAll, aes(x = Num, y = fill, fill = SType))+
#   geom_boxplot() +
#   labs(x = "Minimum Number of Training Points", y= "Bins remaining to be filled")+ 
#   scale_fill_discrete(guide = guide_legend(reverse=TRUE), name='Sample Method', labels = c('cost cLHS', 'cLHS', 'random'))+
#   theme_few()
# plot(binfillplot)
# ggsave("./Results/Bin fill Plot.jpeg", device = 'jpeg')
```

```{r num single points}

###this needs to be adjusted to reflect a set KL distance
statsAll <- as.data.table(statsAll)
temp <- statsAll[,.(y = median(KL)), by = .(SType,Num)]
#temp[,`:=`(y = (y-min(y))/(max(y)-min(y)), Num = as.numeric(as.character(Num)))]
temp[,`:=`(Num = as.numeric(as.character(Num)))]
numPoints <- temp[,.(nPoints = predict(loess(Num ~ y,data = .SD), data.frame(y = 1))), by = .(SType)]
numPoints$nPoints <- round(numPoints$nPoints, -1)
write.table(numPoints, file = "./Results/Number of points required to match covariate distribution.csv", sep = ",", quote = FALSE, row.names = F)
knitr::kable(numPoints, digits = -1, caption = "Number of points for KL divergence = 1")

```

### Simulation 2: Transect Sampling

Now instead of choosing single points, we use the cclhs or lhs to pick centre points, and then create a triangular transect around it, which we then sample from (either using a second clhs or just regular spacing). The below chunk simulates 3 transect scenarios: non cost-constrained clhs, using clhs to sample from transects, and cost-constrained clhs using both clhs and regular spacing for transect sampling. Note that this chunk takes a while to run and will use all your CPU resources. The time limiting part is not the clhs function, but rather extracting data from the raster stack once each transect has been created (even using Velox this takes a while).

```{r set up transect simulations}
ancDatLL_cost <- stack(ancDatLL,acost2)
fullSet <- sampleRegular(ancDatLL, size = 100000,sp = T) 
X <- st_as_sf(fullSet) ##This line takes a while, but I'm not sure of an alternative
X <- X[!is.na(X$X25m_DAH_3Class),]
# 
# fullSL <- sampleRegular(ancDatSL, size = 100000,GDAL = T)
# fullSL <- fullSL[complete.cases(fullSL),]

fullSet <- sampleRegular(ancDatLL_cost, size = 100000,sp = T)
Xcost <- st_as_sf(fullSet)
Xcost <- Xcost[!is.na(Xcost$X25m_DAH_3Class),]
Xcost <- Xcost[!is.infinite(Xcost$layer),]
```

```{r sampling functions}
###create LHS transect sample, use clhs to sample transects
createLHS <- function(X,n){ ## X = landscape covariates and n = number of transects
  xMat <- st_drop_geometry(X)
  temp <- clhs(xMat,size = n,cost = NULL,iter = 10000,use.cpp = T,simple = F)
  dat <- X[temp$index_samples,]
  small <- foreach(j = 1:nrow(dat),.combine = c) %do% {
    tri <- Tri_build(id = j, x = st_coordinates(dat[j,])[1],y = st_coordinates(dat[j,])[2])
    tempDat <- ancDatVL$extract(tri,df = T)
    tempDat[,ncol(tempDat)]
  }
  idxs <- which(idVals %in% small)
  nPoints <- as.integer(length(idxs)*0.85) ##this value should be calibrated
  sl_lhs <- clhs(ancVals, size = nPoints,possible.sample = idxs,iter = 10000, use.cpp = T, simple = F)
  temp <- sl_lhs$sampled_data
  return(temp)
}

##create cclhs sample, extract points @ 5m intervals
createCCLHS_30m <- function(X,n){
  xMat <- st_drop_geometry(X)
  temp <- clhs(xMat,size = n,cost = ncol(xMat),iter = 10000,use.cpp = T,simple = F)
  dat <- X[temp$index_samples,]
  small <- foreach(j = 1:nrow(dat),.combine = rbind) %do% {
    tri <- Tri_build(id = j, x = st_coordinates(dat[j,])[1],y = st_coordinates(dat[j,])[2])
    #tPs <- st_sample(tri, size = 30, type = "regular")
    tPs <- tri
    temp <- suppressWarnings(st_sf(data.frame(id = j, geometry = tPs)) %>%
       st_cast("POINT"))
    temp
    
  }
  small <- st_as_sf(small)
  temp <- ancDatVL$extract_points(small)
  temp <- temp[!is.na(temp[,1]),]
  temp <- temp[,-ncol(temp)]
  return(temp)
}

##create cclhs transects, sample using lhs
createCCLHS <- function(X,n){
  xMat <- st_drop_geometry(X)
  temp <- clhs(xMat,size = n,cost = ncol(xMat),iter = 10000,use.cpp = T,simple = F)
  dat <- X[temp$index_samples,]
  small <- foreach(j = 1:nrow(dat),.combine = c) %do% {
    tri <- Tri_build(id = j, x = st_coordinates(dat[j,])[1],y = st_coordinates(dat[j,])[2])
    tempDat <- ancDatVL$extract(tri,df = T)
    tempDat[,ncol(tempDat)]
  }
  idxs <- which(idVals %in% small)
  nPoints <- as.integer(length(idxs)*0.85)
  sl_lhs <- clhs(ancVals, size = nPoints,possible.sample = idxs,iter = 10000,use.cpp = T,simple = F)
  temp <- sl_lhs$sampled_data
  return(temp)
}

##function to run simulations
collectStats <- function(X,sampleFun, nums, testFun1 = ks_stat_clhs_v3,testFun2 = ks_stat_clhs_v4){
  pkgs <- c("Rcpp","philentropy", "foreach","sf",
          "raster","LearnGeom","dplyr","reticulate","gdistance","scales","velox","clhs","data.table","tidyr")
  test_res <- foreach(n = nums, .combine = rbind, .packages = pkgs,
                    .export = c("Tri_build","idVals","ancVals", "ancDatVL")) %dopar% { #
                      #reticulate::source_python("mTSP.py")
                      smallSet <- sampleFun(X,n)
                      smallSet <- na.omit(smallSet)
                      if(nrow(smallSet) > 1){
                        res1 <- testFun1(sample = smallSet)
                        res2 <- testFun2(sample = smallSet)
                        #res3 <- testFun3(full = ancVals, small = smallSet)
                        #res2 <- testFun2(ancDat,smallSet)
                        data.frame(Num = n,KL = res1, Mean = res2)
                      }else{
                        NULL
                      }
                      
                    }
  return(test_res)
}
```

```{r run simulations transect}
nums <- round(seq(6, 100, length.out = 10), -1) ##set sequence of transect number
nums <- rep(nums, each = 5) ##replicates of each number (should be at least 5)

# statsLHS_trans <- collectStats(X,createLHS,nums)
# statsLHS_trans$SType = "LHS"

tic()
statsCCLHS_trans <- collectStats(Xcost,createCCLHS,nums)
statsCCLHS_trans$SType <- "cLHS_resample"
toc()


statsCCLHS_30m <- collectStats(Xcost, createCCLHS_30m, nums)
statsCCLHS_30m$SType <- "All_points"

##plot
statsAll2 <- rbind(statsCCLHS_trans, statsCCLHS_30m) #statsLHS_trans, 
statsAll2$Num <- as.factor(statsAll2$Num)
statsAll2$SType <- as.factor(statsAll2$SType)%>% factor(levels = c("All_points", "cLHS_resample"))
# ggplot(statsAll2, aes(x = Num, y = KL, fill = SType))+
#   geom_boxplot()
ggplot(statsAll2, aes(x = Num, y = KS, fill = SType))+
  geom_boxplot()

###Graphic of number of transects needed to match variable space
ggplot(statsAll2, aes(x = Num, y = KL, fill = SType))+
  geom_boxplot()+
  scale_y_continuous(name = "Kullback-Leibler (KL) Divergence", limits=c(0, 8),breaks=seq(0,8,1))+
  labs(x = "Number of Transects") +
  scale_fill_discrete(name='Sample Method', labels = c('all points', 'cLHS subsampled'))+
  expand_limits(x = 0, y = 0)+
  geom_hline(yintercept = 1, linetype = "dashed", color = "red")+
   geom_smooth(method = "loess", se=FALSE, aes(group=SType), color = "black", size = .5)+
  theme_light()

ggsave("./Results/KL_Divergence_transect_subsampling.jpeg", device = 'jpeg', width = 6, height=4, units = "in")
#fwrite(statsAll,"statsAll_save.csv")

# KS_transectPlot <- ggplot(statsAll, aes(x = Num, y = KS, fill = SType))+
#   geom_boxplot() +
#   labs(x = "Number of Transects", y= "Kolmogorov-Smirnov statistic (0 = identical)")+
#   scale_fill_discrete(guide = guide_legend(reverse=FALSE), name='Sample Method', labels = c('cLHS- cLHS resampled', 'cost cLHS - systematic resampled', 'cost cLHS - cLHS resampled'))+
#   theme_few()
# plot(KS_transectPlot)
#ggsave("./Results/KS_transect_plot.jpeg", device = 'jpeg')
# 
# Bfill_transectPlot <- ggplot(statsAll, aes(x = Num, y = fillQual, fill = SType))+
#   geom_boxplot() +
#   labs(x = "Number of Transects", y= "Bins remaining to be filled")+
#   scale_fill_discrete(guide = guide_legend(reverse=FALSE), name='Sample Method', labels = c('cLHS- cLHS resampled', 'cost cLHS - systematic resampled', 'cost cLHS - cLHS resampled'))+
#   theme_few()
# plot(Bfill_transectPlot)
#ggsave("./Results/KS_transectPlot.jpeg", device = 'jpeg')

# smallSet <- as.matrix(smallSet)
# test <- cramer.test(x = fullSL,y = smallSet, just.statistic = T)
```

## Calculate number of sites to fill

```{r num points}
statsAll2 <- as.data.table(statsAll2)
temp <- statsAll2[,.(y = median(KL)), by = .(SType,Num)]
#temp[,`:=`(y = (y-min(y))/(max(y)-min(y)), Num = as.numeric(as.character(Num)))]
temp[,`:=`(Num = as.numeric(as.character(Num)))]
numTrans <- temp[,.(nTrans = predict(loess(Num ~ y,data = .SD), data.frame(y = 1))), by = .(SType)]

knitr::kable(numTrans, digits = 2, caption = "Number of points for KL divergence = 1")

# calcNumTrans <- function(tempDat){
#   temp <- tempDat[,.(y = median(fillQual)), by = .(Num)]
#   temp[,`:=`(y = (y-min(y))/(max(y)-min(y)), Num = as.numeric(as.character(Num)))]
#   l2 <- loess(Num ~ y, data = temp)
#   numTrans <- predict(l2,data.frame(y = 0.15))
#   return(numTrans)
# }
# 
# statsAll <- as.data.table(statsAll)
# numTrans <- statsAll[,.(Num90 = calcNumTrans(.SD)), by = .(SType)]
```

## Number of TSP iterations needed

Not much benefit after 15-20 iterations

```{r}
calcCost <- function(Xcost,n,nTry){
  pkgs <- c("Rcpp","sf","data.table","raster","dplyr","reticulate","gdistance","scales","clhs")
  tspTime <- foreach(i = 1:nTry,.combine = rbind, .packages = pkgs,
                     .export = c("start_sf","trSmall")) %dopar% {
    reticulate::source_python("mTSP.py")
    xMat <- st_drop_geometry(Xcost)
    temp <- clhs(xMat,size = n,cost = ncol(xMat),iter = 5000,use.cpp = T,simple = F)
    pnts <- Xcost[temp$index_samples,]
    pnts <- pnts[,1]
    colnames(pnts) <- c("name","geometry")
    st_geometry(pnts) <- "geometry"
    startPnts <- st_as_sf(data.frame(name = "Start",geometry = start_sf))
    pnts <- rbind(pnts, startPnts)
    pnts2 <- as(pnts, "Spatial")
    ## create distance matrix between sample points
    test <- costDistance(trSmall,pnts2,pnts2)
    dMat2 <- as.matrix(test)
    dMat2 <- dMat2*60
    dMat2[is.infinite(dMat2)] <- 1000
    
    ##penalty based on quality of points
    objVals <- max(temp$final_obj_continuous) - temp$final_obj_continuous
    maxTime <- 8L ##hours
    ## time per transect
    plotTime <- 50L ##mins
    temp <- dMat2[1:n,1:n]
    maxDist <- sum(temp[upper.tri(temp)])
    minPen <- maxDist
    maxPen <- maxDist * 4
    objVals <- scales::rescale(objVals, to = c(minPen,maxPen))
    objVals <- as.integer(objVals)
    
    ndays <- as.integer(ceiling(n/5)+1)
    pen = objVals
    
    indStart <- as.integer(rep(n,ndays))
    ##run vehicle routing problem from python script
    ## GCS is global span cost coefficient
    vrp <- py_mTSP(dat = dMat2,num_days = ndays, start = indStart, end = indStart, 
                   max_cost = maxTime*60L, plot_time = plotTime, penalty =  pen, arbDepot = F, GSC = 5L)
    time <- vrp[[2]]
    totTime <- sum(as.numeric(unlist(time)))/480
    num = length(as.numeric(unlist(vrp[[1]])))-2*length(vrp[[1]])
    data.table(It = i, Cost = totTime, Num = num)
  }
  return(tspTime)
}

nums <- seq(2,40,by = 5)
tspTest <- foreach(numRep = nums, .combine = rbind) %do% {
  dat <- calcCost(Xcost, n = 30, nTry = numRep)
  dat[,NumRep := numRep]
  dat
}

tspSum <- tspTest[,.(MinTime = min(Cost)), by = .(NumRep)]
knitr::kable(tspSum)
```

## Simulation 3: Sampling Time

The previous chunks investigated how variable space filling changed with different sampling strategies. We now apply the same process, but collect results on the amount of time each plan would take (as determined by the TSP). We use similar sampling plans as in the above chunks: cost-constrained clhs, regular clhs, and TSP (cost-constrained clhs optimised using the TSP). First, we run the simulations as above, with  increasing numbers of transects and multiple replications at each level. We then simulate the cost to fill the variable space to KL = 1 as calculated above, using each of these methods, and present a sample TSP route.

```{r time cost}
##create TSP to calculate time
calcCost_clhs <- function(pnts,objVals,plotTime = 50L, minPerDay = 3L){
  n = nrow(pnts)
  p2 <- st_as_sf(pnts)
  pnts <- pnts[,1]
  colnames(pnts) <- c("name","geometry")
  st_geometry(pnts) <- "geometry"
  startPnts <- st_as_sf(data.frame(name = "Start",geometry = start_sf))
  pnts <- rbind(pnts, startPnts)
  pnts2 <- as(pnts, "Spatial")
  
  ## create distance matrix between sample points
  test <- costDistance(trSmall,pnts2,pnts2)
  dMat2 <- as.matrix(test)
  dMat2 <- dMat2*60
  dMat2[is.infinite(dMat2)] <- 1000
  
  ##penalty based on quality of points
  objVals <- max(objVals) - objVals
  
  maxTime <- 8L ##hours
  ## time per transect
  temp <- dMat2[1:n,1:n]
  maxDist <- sum(temp[upper.tri(temp)])
  minPen <- maxDist
  maxPen <- maxDist * 4
  objVals <- scales::rescale(objVals, to = c(minPen,maxPen))
  objVals <- as.integer(objVals)

  ndays <- as.integer(ceiling(n/minPerDay)+1)
  pen = objVals

  indStart <- as.integer(rep(n,ndays))
  ##run vehicle routing problem from python script
  ## GCS is global span cost coefficient
  vrp <- py_mTSP(dat = dMat2,num_days = ndays, start = indStart, end = indStart, 
                 max_cost = maxTime*60L, plot_time = plotTime, penalty =  pen, arbDepot = F, GSC = 5L)
  time <- vrp[[2]]
  totTime <- sum(as.numeric(unlist(time)))
  return(totTime)
}

## regular clhs
timeLHS <- function(X,n){
  xMat <- st_drop_geometry(X)
  temp <- clhs(xMat,size = n,cost = NULL,iter = 5000,use.cpp = T,simple = F)
  return(temp)
}

## cost_constrained clhs
timeCCLHS <- function(X,n){
  xMat <- st_drop_geometry(X)
  temp <- clhs(xMat,size = n,cost = ncol(xMat),iter = 5000,use.cpp = T,simple = F)
  return(temp)
}

##function to run simulations
collectTime <- function(X,sampleFun, num){
  pkgs <- c("Rcpp","LaplacesDemon", "foreach","sf",
          "raster","LearnGeom","dplyr","reticulate","gdistance","scales","clhs")
  test_res <- foreach(n = num, .combine = rbind, .packages = pkgs,
                    .export = c("start_sf","trSmall", "calcCost_clhs")) %dopar% { #
                      reticulate::source_python("mTSP.py")
                      templhs <- sampleFun(X,n)
                      dat <- X[templhs$index_samples,]
                      if(nrow(dat) > 1){
                        time <- calcCost_clhs(dat,templhs$final_obj_continuous)
                        data.frame(Num = n, Cost = time)
                      }else{
                        NULL
                      }
                      
                    }
  return(test_res)
}

trans <- as.integer(c(5, 10, 15, 20, 25, 30, 35, 40, 45, 50)) ##numbers of transects
trans <- rep(trans,each = 10) ##replications at each level

LHSTime <- collectTime(X,timeLHS,num = trans)
LHSTime$Type = "LHS"
CCLHSTime <- collectTime(Xcost,timeCCLHS,num = trans)
CCLHSTime$Type = "CCLHS"

##function to choose route of least costly point set
calcCost_tsp <- function(Xcost,n,nTry){
  pkgs <- c("Rcpp","sf","data.table","raster","dplyr","reticulate","gdistance","scales","clhs")
  tspTime <- foreach(i = 1:nTry,.combine = rbind, .packages = pkgs,
                     .export = c("start_sf","trSmall")) %dopar% {
    reticulate::source_python("mTSP.py")
    xMat <- st_drop_geometry(Xcost)
    temp <- clhs(xMat,size = n,cost = ncol(xMat),iter = 5000,use.cpp = T,simple = F)
    pnts <- Xcost[temp$index_samples,]
    pnts <- pnts[,1]
    colnames(pnts) <- c("name","geometry")
    st_geometry(pnts) <- "geometry"
    startPnts <- st_as_sf(data.frame(name = "Start",geometry = start_sf))
    pnts <- rbind(pnts, startPnts)
    pnts2 <- as(pnts, "Spatial")
    ## create distance matrix between sample points
    test <- costDistance(trSmall,pnts2,pnts2)
    dMat2 <- as.matrix(test)
    dMat2 <- dMat2*60
    dMat2[is.infinite(dMat2)] <- 1000
    
    ##penalty based on quality of points
    objVals <- max(temp$final_obj_continuous) - temp$final_obj_continuous
    maxTime <- 8L ##hours
    ## time per transect
    plotTime <- 50L ##mins
    temp <- dMat2[1:n,1:n]
    maxDist <- sum(temp[upper.tri(temp)])
    minPen <- maxDist
    maxPen <- maxDist * 4
    objVals <- scales::rescale(objVals, to = c(minPen,maxPen))
    objVals <- as.integer(objVals)
    
    ndays <- as.integer(ceiling(n/5)+1)
    pen = objVals
    
    indStart <- as.integer(rep(n,ndays))
    ##run vehicle routing problem from python script
    ## GCS is global span cost coefficient
    vrp <- py_mTSP(dat = dMat2,num_days = ndays, start = indStart, end = indStart, 
                   max_cost = maxTime*60L, plot_time = plotTime, penalty =  pen, arbDepot = F, GSC = 5L)
    time <- vrp[[2]]
    totTime <- sum(as.numeric(unlist(time)))
    num = length(as.numeric(unlist(vrp[[1]])))-2*length(vrp[[1]])
    data.table(It = i, Cost = totTime, Num = num)
  }
  return(min(tspTime$Cost))
}

# trans <- as.integer(c(5, 10, 15, 20, 25, 30, 35, 40, 45, 50)) ##numbers of transects
# trans <- rep(trans,each = 5) ##replications at each level

TSPTime <- foreach(n = trans,.combine = rbind) %do% {
  time <- calcCost_tsp(Xcost,n,15)
  data.frame(Num = n, Cost = time)
}
TSPTime$Type <- "TSP"
timeAll <- rbind(LHSTime,CCLHSTime,TSPTime)
timeAll$Cost <- timeAll$Cost/480 ##covert from minutes to 8-hour days

timeAll$Num <- as.factor(timeAll$Num)
timeAll$Type <- as.factor(timeAll$Type) %>% factor(levels = c( "LHS", "CCLHS", "TSP"))

ggplot(timeAll, aes(x = Num, y = Cost, fill = Type))+
  geom_boxplot()+
  scale_y_continuous(name = "Number of 8-hour days", limits=c(0, 15),breaks=seq(0,15,1))+
  labs(x = "Number of Sample Sites in Sample Plan") +
  expand_limits(x = 0, y = 0)+
  scale_fill_discrete (name= 'Method', labels = c('cLHS', 'cost-constrained cLHS',  'Travelling Salesman Optimized')) +
  theme_light()
ggsave("./Results/CostsbyTransectMethod.jpeg", device = 'jpeg')

# Time_plot <- ggplot(timeAll, aes(x = Num, y = Cost, fill = Type)) +
#   geom_boxplot() +
#   labs(x = "Number of Transects", y = "Time Cost to Sample Sufficient Samples") +
#   scale_fill_discrete(
#     guide = guide_legend(reverse = FALSE),
#     name = 'Sample Method',
#     labels = c(
#       'cLHS- cLHS resampled',
#       'cost cLHS - cLHS resampled',
#       'cost cLHS systematic resampled'
#     )
#   ) +
#   theme_few()
# plot(Bfill_transectPlot2)
```

## Example TSP Route

```{r tsp_example}
n = 25

xMat <- st_drop_geometry(Xcost)
temp <- clhs(xMat,size = n,cost = ncol(xMat),iter = 10000,use.cpp = T,simple = F)
pnts <- Xcost[temp$index_samples,]
pnts <- pnts[,1]
colnames(pnts) <- c("name","geometry")
st_geometry(pnts) <- "geometry"
startPnts <- st_as_sf(data.frame(name = "Start",geometry = start_sf))
pnts <- rbind(pnts, startPnts)
pnts2 <- as(pnts, "Spatial")
## create distance matrix between sample points
test <- costDistance(trSmall,pnts2,pnts2)
dMat2 <- as.matrix(test)
dMat2 <- dMat2*60
dMat2[is.infinite(dMat2)] <- 1000

##penalty based on quality of points
objVals <- max(temp$final_obj_continuous) - temp$final_obj_continuous
maxTime <- 8L ##hours
## time per transect
plotTime <- 50L ##mins
temp <- dMat2[1:n,1:n]
maxDist <- sum(temp[upper.tri(temp)])
minPen <- maxDist
maxPen <- maxDist * 4
objVals <- scales::rescale(objVals, to = c(minPen,maxPen))
objVals <- as.integer(objVals)

ndays <- as.integer(ceiling(n/5)+1)
pen = objVals

indStart <- as.integer(rep(n,ndays))
##run vehicle routing problem from python script
## GCS is global span cost coefficient
vrp <- py_mTSP(dat = dMat2,num_days = ndays, start = indStart, end = indStart, 
               max_cost = maxTime*60L, plot_time = plotTime, penalty =  pen, arbDepot = F, GSC = 100L)

result <- vrp[[1]]

## create spatial paths
paths <- foreach(j = 0:(length(result)-1), .combine = rbind) %do% {
  if(length(result[[as.character(j)]]) > 2){
    cat("Drop site",j,"...\n")
    p1 <- result[[as.character(j)]]+1
    out <- foreach(i = 1:(length(p1)-1), .combine = rbind) %do% {
      temp1 <- pnts[p1[i],]
      temp2 <- pnts[p1[i+1],]
      temp3 <- shortestPath(trSmall,st_coordinates(temp1),
                            st_coordinates(temp2),output = "SpatialLines") %>% st_as_sf()
      temp3$Segment = i
      temp3
    }
    out$DropSite = j
    out
  }
  
}

paths <- st_transform(paths, 3005)
#st_write(paths, dsn = "RoadTSP.gpkg", layer = "Paths", append = T, driver = "GPKG")  

plot(acost2)
plot(pnts,col = "black", add = T)
plot(paths,col = as.factor(paths$DropSite), add = T)

## label points
p2 <- pnts
p2$PID <- seq_along(p2$name)
p2 <- p2[,"PID"]
p2$DropLoc <- NA
p2$Order <- NA
for(i in 0:(length(result)-1)){
  p1 <- result[[as.character(i)]]+1
  p1 <- p1[-1]
  p2$DropLoc[p1] <- i
  p2$Order[p1] <- 1:length(p1)
}
p2 <- st_transform(p2, 3005)


```

## Time to fill variable space

Instead of just calculating the time cost for different numbers of transects, we can use the TSP to estimate the cost of using each sampling method to fill the variable space. Thus, here we use the number of transects needed to fill the variable space as calculated above, and use the TSP to estimate the cost of each method.

```{r num}
nSims <- 20
nRep <- 3

collectTime2 <- function(X,sampleFun,num, nReps, plotTime = 50L, minNum = 3L){
  pkgs <- c("Rcpp","LaplacesDemon", "foreach","sf",
          "raster","LearnGeom","dplyr","reticulate","gdistance","scales","clhs")
  test_res <- foreach(n = 1:nReps, .combine = rbind, .packages = pkgs,
                    .export = c("start_sf","trSmall", "calcCost_clhs")) %dopar% { #
                      reticulate::source_python("mTSP.py")
                      templhs <- sampleFun(X,num)
                      dat <- X[templhs$index_samples,]
                      if(nrow(dat) > 1){
                        time <- calcCost_clhs(dat,templhs$final_obj_continuous,plotTime,minNum)
                        data.frame(Rep = n, Cost = time)
                      }else{
                        NULL
                      }
                      
                    }
  return(test_res)
}

##single points clhs sampled
allSLDat <- as.data.frame(getValues(ancDatSL_cost))##table of SL data
#allSLDat[,CellID := seq_along(allSLDat$twi)]
allSLDat <- na.omit(allSLDat)
allSLDat <- allSLDat[!is.infinite(allSLDat$layer),]
numNeeded <- numPoints[SType == "CCLHS",as.integer(nPoints)]

pkgs <- c("Rcpp","LaplacesDemon", "foreach","sf",
        "raster","LearnGeom","dplyr","reticulate","gdistance","scales","clhs")

## note this part takes about 10 mins (should be nSims*5)
SPoint <- foreach(n = 1:(nSims*nRep), .combine = rbind, .packages = pkgs,
                  .export = c("start_sf","trSmall", "calcCost_clhs","allSLDat")) %dopar% { #
                    reticulate::source_python("mTSP.py")
                    templhs <- clhs(allSLDat,size = as.integer(numNeeded/4),cost = ncol(allSLDat),use.cpp = T, iter = 6000,simple = F)#6000
                    tempdat <- templhs$sampled_data
                    cellNums <- as.numeric(rownames(tempdat))
                    dat <- raster::xyFromCell(ancDatSL_cost,cell = cellNums, sp = T)
                    dat <- st_as_sf(dat)
                    dat <- cbind(1:nrow(dat),dat)
                    if(nrow(dat) > 1){
                      time <- calcCost_clhs(dat,templhs$final_obj_continuous,plotTime = 2L,minPerDay = 8L)
                      data.frame(Rep = n, Cost = time*4)
                    }else{
                      NULL
                    }
                    
                  }

SPoint <- as.data.table(SPoint)
SPoint[,TSPNum := rep(c(1:nRep),each = nSims)]
TSPpoint <- SPoint[,.(Cost = min(Cost)), by = .(TSPNum)]
TSPpoint[,SType := "Single Point TSP"]
setnames(TSPpoint, old = "TSPNum", new = "Rep")
SPoint[,TSPNum := NULL]
SPoint[,SType := "Single Point CLHS"]

##clhs resampled clhs
# numNeeded <- numTrans[SType == "CCLHS",as.integer(nTrans)]
# clhsTime <- collectTime2(Xcost,timeCCLHS,numNeeded,nSims)
# clhsTime$SType <- "CLHS_Resample"

##clhs regular resampling
numNeeded <- numTrans[SType == "cLHS_resample",as.integer(nTrans)]
clhsregTime <- collectTime2(Xcost,timeCCLHS,numNeeded,nSims)
clhsregTime$SType <- "CLHS_Regular"

##clhs optimised using TSP
nSims <- 5
numNeeded <- numTrans[SType == "cLHS_resample",as.integer(nTrans)]
tspTime <- foreach(n = 1:(nSims),.combine = rbind) %do% {
  time <- calcCost_tsp(Xcost,numNeeded,15)
  data.frame(Rep = n, Cost = time)
}
tspTime$SType <- "Transects_TSP"

timeAll <- rbind(TSPpoint,tspTime)#clhsTime,SPoint,clhsregTime,
timeAll <- as.data.table(timeAll)
timeAll[,Cost := Cost/480]
timeAll[,SType := as.factor(SType)]
timeAll[,SType := reorder(SType,Cost, FUN = function(x){-mean(x)})]

ggplot(timeAll, aes(x = SType, y = Cost))+
  geom_boxplot()+
    scale_y_continuous(name = "Number of 8-hour days", limits=c(0, 30),breaks=seq(0,30,10))+
  labs(x = "Number of Sample Sites in Sample Plan") +
  expand_limits(x = 0, y = 0)+
  scale_fill_discrete (name= 'Method', labels = c('cLHS', 'cost-constrained cLHS',  'Travelling Salesman Optimized')) +
  theme_light()
ggsave("./Results/CostsTransectvsPointsMethod.jpeg", device = 'jpeg', width = 4, height = 4)
```

## Compare SS transect data to covariate space with DataExplorer
```{r}
# SL <- fread("./InputData/Deception_Transect_SSxcovar_5m.csv")
# LL <- fread("./InputData/Deception_Transect_SSxcovar_LL.csv")
# LL <- LL %>% dplyr::select(mapunit1, mapunit2, '25m_MRVBF_', '25m_Landfo', '25m_DAH_3C')
# 
# DataExplorer::create_report(LL, y = "mapunit1")
```


